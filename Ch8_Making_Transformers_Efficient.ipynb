{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bed5f6-166b-434e-ac4c-efee0dfa07e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Chapter 8: Making Transformers Efficient in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570a361e-fe0d-4e0b-8e3c-a6a76607ce04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".pad-left {\n",
       "    padding-left: 20px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".pad-left {\n",
    "    padding-left: 20px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597880b-f340-404f-9502-cc0770d6ee86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "> (W)hen developing a new machine learning model for your business, do you first make it accurate, then worry about making it fast in production? Or do you first make sure it can be fast, then make it accurate? \n",
    "> <p/>\n",
    "> ...\n",
    "> <p/>\n",
    "> While this was a stressful experience for us, it doesnâ€™t have to be for you, because in this article we are going to share the optimizations that made Bert inference fast for us. So you can start with an egg (a known playbook for making certain Bert models fast in production), then focus on the chicken (making your Bert model accurate).\n",
    "\n",
    "* Blogpost@Robolox: [How We Scaled BERT to Serve 1+ Billion Daily Requests on CPUs](https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26)\n",
    "* And the [video from Databricks on YouTube](https://youtu.be/Nw77sEAn_Js)\n",
    "\n",
    "#### Key takeaways\n",
    "\n",
    "1. _Smaller Model_: model distillation\n",
    "1. _Smaller Inputs_: do away with padding inputs and go with dynamically shaped input\n",
    "1. _Smaller Weights_: although this may necessarily trade off accuracy, use quantization \n",
    "1. _Smaller number of requests_: use caching\n",
    "1. _Smaller number of thread per core_: thread tuning with [`torch.set_num_threads`](https://www.theatlantic.com/ideas/archive/2024/01/the-daily-show-jon-stewart/677240/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cda04-461c-47cc-9b74-b1ddc0c22c75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Intent Detection as a Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee31293-5fb4-4c02-afe4-407bc2640b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 06:20:05.753978: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=teacher_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70424d28-eac9-406f-9b9d-2b3d58ebc4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490034222602844}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\"\"\n",
    "\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eed3b-cec2-4a70-80c5-41770a222f7a",
   "metadata": {},
   "source": [
    "### CLINC150\n",
    "\n",
    "A dataset for task-oriented dialog systems, this dataset was used to fine-tune the baseline model in this example. \n",
    "\n",
    "The important thing is that it actually includes queries that are out-of-scope.\n",
    "\n",
    "Please see: [`clinc_oos` at ðŸ¤—](https://huggingface.co/datasets/clinc_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ce07bd-94bc-473e-b750-b658699b6d59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/home/a_naughty_alpaca/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f96a2aef4740ccb727498e2fad1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac08cd-d61b-4856-9935-a98a2039dd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58852802-a4ec-4e2d-92b0-afc81f74c548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0f8fe-188d-4d0c-b128-d9d73a34a949",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e1766-76d5-4255-bdaa-244fb222dce6",
   "metadata": {},
   "source": [
    "## Creating a Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c1b1fb-a497-4033-aed4-281233412b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def compute_size(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c54a7-1145-4e5e-a5d5-7bfcc87640af",
   "metadata": {},
   "source": [
    "#### Implementing `compute_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e411d1-2cd2-4049-a5fc-4e51bd8e8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd019f56-8a6f-4ba3-88bc-74aa4e5b7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
    "    preds, labels = [], []\n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred))\n",
    "        labels.append(label)\n",
    "\n",
    "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "    return accuracy\n",
    "\n",
    "PerformanceBenchmark.compute_accuracy = compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ccd7d-30ec-425c-98e0-6325eac2a68c",
   "metadata": {},
   "source": [
    "#### Implementing `compute_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0294f3-5b7e-403b-8a9c-811229becc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert.encoder.layer.2.attention.self.value.weight',\n",
       " tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\n",
       "           4.6521e-03,  2.9844e-02],\n",
       "         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\n",
       "          -2.6890e-02, -2.1943e-02],\n",
       "         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\n",
       "           3.1152e-02, -9.7786e-03],\n",
       "         ...,\n",
       "         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\n",
       "           1.1093e-02,  2.9703e-03],\n",
       "         [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\n",
       "           6.7487e-03,  1.0511e-03],\n",
       "         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\n",
       "           2.3981e-02, -4.2880e-02]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipe.model.state_dict().items())[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbcb685-e465-4904-96d5-cb4f8a927096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5eecb9-5c6d-458b-beaa-0ec42e9a16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def compute_size(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    # calculate size in megabytes\n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "    # delete tmp file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "    return {\"size_mb\": size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a6a24-9699-4c78-a55c-1e5257e09e57",
   "metadata": {},
   "source": [
    "#### Implementing `time_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9270fb87-87be-4490-b1b7-bcb673fc9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency (ms) - 41.739\n",
      "Latency (ms) - 37.976\n",
      "Latency (ms) - 37.701\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = perf_counter()\n",
    "    _ = pipe(query)\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f\"Latency (ms) - {1000 * latency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844f5595-4454-48bb-b791-3d0509d6135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline method\"\"\"\n",
    "    latencies = []\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "\n",
    "    # now we observed the elapsed time over 100 runs\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    # compute run stats\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return { \"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms }\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb19910e-b81b-4d21-b1a6-5cd3f2330659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 418.15\n",
      "Average latency (ms) - 22.92 +\\- 0.30\n",
      "Accuracy on test set - 0.867\n"
     ]
    }
   ],
   "source": [
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ee87b-2d4b-464b-9adf-60348b18a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04bbda83-15ff-471a-9b6f-f13904a12bd9",
   "metadata": {},
   "source": [
    "## Making Models Smaller via Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82239c28-989f-44d3-b2f4-3a6ec8a3954a",
   "metadata": {},
   "source": [
    "### Creating a Knowledge Distillation Trainer\n",
    "\n",
    "In addition to the _105_ parameters that [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments), we will add two more to support training of a student model with knowledge distillation:\n",
    "\n",
    "* `alpha` ... $\\alpha$ controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see below). Ranges from 0.0 to 1.0; $\\alpha = 1.0$ means that we only use the cross-entropy of the student and ignore any signal from the teacher.\n",
    "* `temperature` ... $T$ softens the probability distributions by scaling the logits before applying softmax:\n",
    "\n",
    "<p class=\"pad-left\">\\(p_{i} = \\frac{exp(z_i(x)/T)}{\\sum_\\limits{j}exp(z_{i}(x)/T)}\\)</p>\n",
    "<p>Ranges from 1.0 to $\\infty$. $T=1$ recovers the original softmax distribution. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7483a47a-87b3-4701-abdf-c2fab1d4ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edc9f0-a726-43fb-aedf-6656242ef85c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "During training, loss is calculated as a weighted average of the usual cross-entropy loss of the student; and the knowledge-distallation loss between the teacher and student. \n",
    "\n",
    "<p class=\"pad-left\">\\(L_{student} = \\alpha L_{CE} + (1 - \\alpha) L_{KD}\\)</p>\n",
    "<p>where</p>\n",
    "\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{CE}\\)</p>\n",
    "<p>is the cross-entropy loss of the ground truth labels.</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{KD} = T^{2}D_{KL}\\)</p><p>is knowledge-distillation loss where \\(T^{2}\\) is a normalization factor to account for the gradients produced by soft labels scales as \\(\\frac{1}{T^{2}}\\).</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(D_{KL}(p, q) = \\sum_\\limits{i} p_i \\  log\\frac{p_i(x)}{q_i(x)}\\)</p>\n",
    "<p>which is the expectation of the log difference between $p_i(x)$ and $q_i(x)$ when the expectation is taken using the probabilities of $p_i(x)$. For our case, $p_i(x)$ is the <i>teacher</i> and $q_i(x)$ is the <i>student</i>. In other words, we measure loss by seeing how far off the student is from the teacher, and that makes perfect sense.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c4cfc5-ef8b-48ed-bcd4-1794ff7fc1b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs_stu = model(**inputs)\n",
    "        # extract cross-entropy loss and logits from student\n",
    "        loss_ce = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        \n",
    "        # extract logits from teacher\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "\n",
    "        # soften probabilities and compute distillation loss\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)\n",
    "        )\n",
    "\n",
    "        # return weighted student loss\n",
    "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31060cd5-17bf-46ca-9dcb-d4f7574f03c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4151b3da-d2a4-4fee-9362-853737d7dc04",
   "metadata": {},
   "source": [
    "## Choosing a Good Student Initialization\n",
    "\n",
    "> A good rule of thumb from the literature is that knowledge distillation works best when teacher and student are of the same _model type_.\n",
    "\n",
    "So if we are using [BERT (`transformersbook/bert-base-uncased-finetuned-clinc`)](https://huggingface.co/transformersbook/bert-base-uncased-finetuned-clinc) for teacher, then [DistilBERT (`distilbert-base-uncased`)](https://huggingface.co/distilbert-base-uncased) for the student is a natural choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf0be30d-9766-4890-9793-907f6d3afb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_text at 0x7f326f00e040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bbd58be0454f06a77e90af094712df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaca35cf2d1429b92c05e134c236c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23238f30712343d1858f690e7a87da6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a40ddc06-303f-46ba-9e9a-699f5d572b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f72f07f48af4e97bb0e5d44b8f8514b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bafb8-7792-431d-9d1e-7c416b3e5aa3",
   "metadata": {},
   "source": [
    "We implement `compute_metrics` for tracking metrics during training. Here, we can reuse `accuracy_score` which we use above in `PerformanceBenchmark.compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25319fbb-cdc9-4ab4-8c51-c2361562089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9700d-f9a8-4434-a4d0-1ceb429f9965",
   "metadata": {},
   "source": [
    "#### Training arguments\n",
    "\n",
    "* [`output_dir`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.output_dir) ... output directory where the model predictions and checkpoints will be written.\n",
    "* [`evaluation_strategy`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy) ... `\"no\"`: No evaluation is done during training; `\"steps\"`: Evaluation is done (and logged) every eval_steps; or `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
    "* [`num_train_epochs`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,) ... number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training); defaults to 3.0.\n",
    "* [`learning_rate`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.learning_rate) ... initial learning rate for [`transformers.AdamW`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/optimizer_schedules#transformers.AdamW) optimizer.\n",
    "* [`weight_decay`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.weight_decay) ... weight decay to apply (if not zero) to all layers except all bias and `LayerNorm` weights in `transformers.AdamW` optimizer.\n",
    "* [`per_device_train_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size) ... batch size per GPU/TPU core/CPU for _training_; defaults to 8.\n",
    "* [`per_device_eval_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size) ... batch size per GPU/TPU core/CPU for _evaluation_; defaults to 8.\n",
    "* `alpha` ... controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see explanation above).\n",
    "* [`push_to_hub`]() ... you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7068223-596f-4b60-9af8-a03a06e8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir=finetuned_ckpt,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    alpha=1,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d0666-36e1-4ee2-af98-7fe442f177ed",
   "metadata": {},
   "source": [
    "#### Student model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f17bc5b-4200-4dc4-9c96-6df9f12f46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n",
    "\n",
    "num_labels = intents.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7454d3d7-6ddb-427b-8246-4ccb9276aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    student_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfb4fb67-194e-46fb-acda-1809ff1e2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6899940a-08df-487c-b3c2-b09ab6e12cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def student_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(\n",
    "        student_ckpt,\n",
    "        config=student_config\n",
    "    ).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "693a7068-eec9-4eef-9190-6ac645947295",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = (AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_ckpt,\n",
    "    num_labels=num_labels\n",
    ").to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12eb147c-5c75-4a90-8c3a-7da459af8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/a_naughty_alpaca/dev/github/transformers-gcp/distilbert-base-uncased-finetuned-clinc is already a clone of https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 05:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.288450</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.786500</td>\n",
       "      <td>1.875109</td>\n",
       "      <td>0.836774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.786500</td>\n",
       "      <td>1.156888</td>\n",
       "      <td>0.896129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.692500</td>\n",
       "      <td>0.857329</td>\n",
       "      <td>0.913226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.918065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1590, training_loss=2.0511699076718504, metrics={'train_runtime': 340.3206, 'train_samples_per_second': 224.053, 'train_steps_per_second': 4.672, 'total_flos': 413896353421488.0, 'train_loss': 2.0511699076718504, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc[\"train\"],\n",
    "    eval_dataset=clinc_enc[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer\n",
    ")\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2760b1d-6252-449f-892f-e8771daf5dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e7d961cca345ef9068b7e9d1cc6fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Jan28_06-22-38_t4-us-west4-b-n1-standard-16/events.out.tfevents.1706422967.t4-us-west4-b-n1-sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc\n",
      "   b89d63e..3fc7b84  main -> main\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/commit/3fc7b84b345c71b5a814b73aed1e1f49996d9e24'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_trainer.push_to_hub(\"training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d99b5-bcaf-46ad-a6bd-4194b58016fb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237333b7-7717-4b96-a4b4-49491ababaef",
   "metadata": {},
   "source": [
    "#### ???\n",
    "\n",
    "So, how it that fine-tuned model of ours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18ec9f59-7e34-4824-8cae-1d81eb711a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/5d7a292d7fb16e9bde665dc8fd3842fd30c3ad1d04184c12e7b1f984bca74af1.fa1cb6f6b168eb29694769428fab54b67197ab45d7daec20c0b182e5dbdde6b1\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-finetuned-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/e1ad7abe1ca6ca7a6d4f1cfc9de256bad2b501ebe7162a2aafff54bb220931dd.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/06f82810f2064792bf87b0151487e96479ade2ffb878fce1b686eefc65f0f8c9.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/47165d38838c8d9d5d1fb8c51331f47ae9876726f3016f84a65a3a72bbcc8221.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/1402d8bf7e99ac0ebec29cb0ceb0226bca0a22660c6bc278dbec17cb2ba182b3.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    }
   ],
   "source": [
    "finetuned_ckpt = \"buruzaemon/distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=finetuned_ckpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9692b5a-1cac-4ea4-9e0c-69cb8fff6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 255.88\n",
      "Average latency (ms) - 12.05 +\\- 0.23\n",
      "Accuracy on test set - 0.863\n"
     ]
    }
   ],
   "source": [
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(\n",
    "    pipe,\n",
    "    clinc[\"test\"],\n",
    "    optim_type=optim_type\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b5118-570c-4f55-a35b-02085434e1a8",
   "metadata": {},
   "source": [
    "### Comparison: base-line model (Teacher) vs. fine-tuned (Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c848824e-20fb-4e96-9453-66f3278df011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        # add a dashed circle around the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx,\n",
    "                marker='$\\u25CC$'\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx\n",
    "            )\n",
    "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
    "    #for handle in legend.legendHandles:\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80, 90)\n",
    "\n",
    "    # use the slowest model to define the x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(perf_metrics, optim_type)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89ec71-9bb7-4c91-90d8-8fb82a464fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

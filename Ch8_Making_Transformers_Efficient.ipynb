{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bed5f6-166b-434e-ac4c-efee0dfa07e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Chapter 8: Making Transformers Efficient in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570a361e-fe0d-4e0b-8e3c-a6a76607ce04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".pad-left {\n",
       "    padding-left: 20px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".pad-left {\n",
    "    padding-left: 20px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597880b-f340-404f-9502-cc0770d6ee86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "> (W)hen developing a new machine learning model for your business, do you first make it accurate, then worry about making it fast in production? Or do you first make sure it can be fast, then make it accurate? \n",
    "> <p/>\n",
    "> ...\n",
    "> <p/>\n",
    "> While this was a stressful experience for us, it doesnâ€™t have to be for you, because in this article we are going to share the optimizations that made Bert inference fast for us. So you can start with an egg (a known playbook for making certain Bert models fast in production), then focus on the chicken (making your Bert model accurate).\n",
    "\n",
    "* Blogpost@Robolox: [How We Scaled BERT to Serve 1+ Billion Daily Requests on CPUs](https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26)\n",
    "* And the [video from Databricks on YouTube](https://youtu.be/Nw77sEAn_Js)\n",
    "\n",
    "#### Key takeaways\n",
    "\n",
    "1. _Smaller Model_: model distillation\n",
    "1. _Smaller Inputs_: do away with padding inputs and go with dynamically shaped input\n",
    "1. _Smaller Weights_: although this may necessarily trade off accuracy, use quantization \n",
    "1. _Smaller number of requests_: use caching\n",
    "1. _Smaller number of thread per core_: thread tuning with [`torch.set_num_threads`](https://www.theatlantic.com/ideas/archive/2024/01/the-daily-show-jon-stewart/677240/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cda04-461c-47cc-9b74-b1ddc0c22c75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Intent Detection as a Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee31293-5fb4-4c02-afe4-407bc2640b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 06:59:55.935015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=teacher_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70424d28-eac9-406f-9b9d-2b3d58ebc4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490034222602844}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\"\"\n",
    "\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eed3b-cec2-4a70-80c5-41770a222f7a",
   "metadata": {},
   "source": [
    "### CLINC150\n",
    "\n",
    "A dataset for task-oriented dialog systems, this dataset was used to fine-tune the baseline model in this example. \n",
    "\n",
    "The important thing is that it actually includes queries that are out-of-scope.\n",
    "\n",
    "Please see: [`clinc_oos` at ðŸ¤—](https://huggingface.co/datasets/clinc_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ce07bd-94bc-473e-b750-b658699b6d59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/home/a_naughty_alpaca/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4110065d83c04bdda8ae84db3516c754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac08cd-d61b-4856-9935-a98a2039dd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58852802-a4ec-4e2d-92b0-afc81f74c548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0f8fe-188d-4d0c-b128-d9d73a34a949",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e1766-76d5-4255-bdaa-244fb222dce6",
   "metadata": {},
   "source": [
    "## Creating a Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c1b1fb-a497-4033-aed4-281233412b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def compute_size(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c54a7-1145-4e5e-a5d5-7bfcc87640af",
   "metadata": {},
   "source": [
    "#### Implementing `compute_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e411d1-2cd2-4049-a5fc-4e51bd8e8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd019f56-8a6f-4ba3-88bc-74aa4e5b7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
    "    preds, labels = [], []\n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred))\n",
    "        labels.append(label)\n",
    "\n",
    "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "    return accuracy\n",
    "\n",
    "PerformanceBenchmark.compute_accuracy = compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ccd7d-30ec-425c-98e0-6325eac2a68c",
   "metadata": {},
   "source": [
    "#### Implementing `compute_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0294f3-5b7e-403b-8a9c-811229becc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert.encoder.layer.2.attention.self.value.weight',\n",
       " tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\n",
       "           4.6521e-03,  2.9844e-02],\n",
       "         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\n",
       "          -2.6890e-02, -2.1943e-02],\n",
       "         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\n",
       "           3.1152e-02, -9.7786e-03],\n",
       "         ...,\n",
       "         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\n",
       "           1.1093e-02,  2.9703e-03],\n",
       "         [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\n",
       "           6.7487e-03,  1.0511e-03],\n",
       "         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\n",
       "           2.3981e-02, -4.2880e-02]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipe.model.state_dict().items())[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbcb685-e465-4904-96d5-cb4f8a927096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5eecb9-5c6d-458b-beaa-0ec42e9a16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def compute_size(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    # calculate size in megabytes\n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "    # delete tmp file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "    return {\"size_mb\": size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a6a24-9699-4c78-a55c-1e5257e09e57",
   "metadata": {},
   "source": [
    "#### Implementing `time_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9270fb87-87be-4490-b1b7-bcb673fc9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency (ms) - 40.518\n",
      "Latency (ms) - 38.497\n",
      "Latency (ms) - 37.216\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = perf_counter()\n",
    "    _ = pipe(query)\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f\"Latency (ms) - {1000 * latency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844f5595-4454-48bb-b791-3d0509d6135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline method\"\"\"\n",
    "    latencies = []\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "\n",
    "    # now we observed the elapsed time over 100 runs\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    # compute run stats\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return { \"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms }\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb19910e-b81b-4d21-b1a6-5cd3f2330659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 418.15\n",
      "Average latency (ms) - 22.76 +\\- 0.36\n",
      "Accuracy on test set - 0.867\n"
     ]
    }
   ],
   "source": [
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbda83-15ff-471a-9b6f-f13904a12bd9",
   "metadata": {},
   "source": [
    "## Making Models Smaller via Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82239c28-989f-44d3-b2f4-3a6ec8a3954a",
   "metadata": {},
   "source": [
    "### Creating a Knowledge Distillation Trainer\n",
    "\n",
    "In addition to the _105_ parameters that [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments), we will add two more to support training of a student model with knowledge distillation:\n",
    "\n",
    "* `alpha` ... $\\alpha$ controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see below). Ranges from 0.0 to 1.0; $\\alpha = 1.0$ means that we only use the cross-entropy of the student and ignore any signal from the teacher.\n",
    "* `temperature` ... $T$ softens the probability distributions by scaling the logits before applying softmax:\n",
    "\n",
    "<p class=\"pad-left\">\\(p_{i} = \\frac{exp(z_i(x)/T)}{\\sum_\\limits{j}exp(z_{i}(x)/T)}\\)</p>\n",
    "<p>Ranges from 1.0 to $\\infty$. $T=1$ recovers the original softmax distribution. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7483a47a-87b3-4701-abdf-c2fab1d4ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edc9f0-a726-43fb-aedf-6656242ef85c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "During training, loss is calculated as a weighted average of the usual cross-entropy loss of the student; and the knowledge-distillation loss between the teacher and student. \n",
    "\n",
    "<p class=\"pad-left\">\\(L_{student} = \\alpha L_{CE} + (1 - \\alpha) L_{KD}\\)</p>\n",
    "<p>where</p>\n",
    "\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{CE}\\)</p>\n",
    "<p>is the cross-entropy loss of the ground truth labels.</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{KD} = T^{2}D_{KL}\\)</p><p>is knowledge-distillation loss where \\(T^{2}\\) is a normalization factor to account for the gradients produced by soft labels scales as \\(\\frac{1}{T^{2}}\\).</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(D_{KL}(p, q) = \\sum_\\limits{i} p_i \\  log\\frac{p_i(x)}{q_i(x)}\\)</p>\n",
    "<p>which is the expectation of the log difference between $p_i(x)$ and $q_i(x)$ when the expectation is taken using the probabilities of $p_i(x)$. For our case, $p_i(x)$ is the <i>teacher</i> and $q_i(x)$ is the <i>student</i>. In other words, we measure loss by seeing how far off the student is from the teacher, and that makes perfect sense.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c4cfc5-ef8b-48ed-bcd4-1794ff7fc1b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs_stu = model(**inputs)\n",
    "        # extract cross-entropy loss and logits from student\n",
    "        loss_ce = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        \n",
    "        # extract logits from teacher\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "\n",
    "        # soften probabilities and compute distillation loss\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)\n",
    "        )\n",
    "\n",
    "        # return weighted student loss\n",
    "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151b3da-d2a4-4fee-9362-853737d7dc04",
   "metadata": {},
   "source": [
    "## Choosing a Good Student Initialization\n",
    "\n",
    "> A good rule of thumb from the literature is that knowledge distillation works best when teacher and student are of the same _model type_.\n",
    "\n",
    "So if we are using [BERT (`transformersbook/bert-base-uncased-finetuned-clinc`)](https://huggingface.co/transformersbook/bert-base-uncased-finetuned-clinc) for teacher, then [DistilBERT (`distilbert-base-uncased`)](https://huggingface.co/distilbert-base-uncased) for the student is a natural choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8baac9e7-1c82-455e-8cd1-867537761400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf0be30d-9766-4890-9793-907f6d3afb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_text at 0x7f67b7cb2af0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20db3efef8f34208b59ab62d1f7e6908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346b1d98aa79499588c1aa1ebd518d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383831c4dfc349e28da6add8d7614000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40ddc06-303f-46ba-9e9a-699f5d572b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60794438b5d45cc8bbf5d7096300613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bafb8-7792-431d-9d1e-7c416b3e5aa3",
   "metadata": {},
   "source": [
    "We implement `compute_metrics` for tracking metrics during training. Here, we can reuse `accuracy_score` which we use above in `PerformanceBenchmark.compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25319fbb-cdc9-4ab4-8c51-c2361562089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9700d-f9a8-4434-a4d0-1ceb429f9965",
   "metadata": {},
   "source": [
    "#### Training arguments\n",
    "\n",
    "* [`output_dir`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.output_dir) ... output directory where the model predictions and checkpoints will be written.\n",
    "* [`evaluation_strategy`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy) ... `\"no\"`: No evaluation is done during training; `\"steps\"`: Evaluation is done (and logged) every eval_steps; or `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
    "* [`num_train_epochs`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,) ... number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training); defaults to 3.0.\n",
    "* [`learning_rate`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.learning_rate) ... initial learning rate for [`transformers.AdamW`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/optimizer_schedules#transformers.AdamW) optimizer.\n",
    "* [`weight_decay`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.weight_decay) ... weight decay to apply (if not zero) to all layers except all bias and `LayerNorm` weights in `transformers.AdamW` optimizer.\n",
    "* [`per_device_train_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size) ... batch size per GPU/TPU core/CPU for _training_; defaults to 8.\n",
    "* [`per_device_eval_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size) ... batch size per GPU/TPU core/CPU for _evaluation_; defaults to 8.\n",
    "* `alpha` ... controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see explanation above).\n",
    "* `temperature` ... controls the softening of the probability distributions by scaling the logits before applying softmax (see explanation above).\n",
    "* [`push_to_hub`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.push_to_hub) ... push the model to the Hub every time the model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa22bd3-7ecd-4c15-91f3-16ffd4f4d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 8675309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7068223-596f-4b60-9af8-a03a06e8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    seed=SEED,\n",
    "    output_dir=finetuned_ckpt,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # here are the 2 hyperparams for knowledge-distillation\n",
    "    alpha=1,\n",
    "    temperature=2.0,\n",
    "    # to avoid those deprecation warnings\n",
    "    optim=\"adamw_torch\",\n",
    "    #\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d0666-36e1-4ee2-af98-7fe442f177ed",
   "metadata": {},
   "source": [
    "#### Student model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f17bc5b-4200-4dc4-9c96-6df9f12f46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n",
    "\n",
    "num_labels = intents.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7454d3d7-6ddb-427b-8246-4ccb9276aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    student_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb4fb67-194e-46fb-acda-1809ff1e2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6899940a-08df-487c-b3c2-b09ab6e12cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def student_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(\n",
    "        student_ckpt,\n",
    "        config=student_config\n",
    "    ).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693a7068-eec9-4eef-9190-6ac645947295",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = (AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_ckpt,\n",
    "    num_labels=num_labels\n",
    ").to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12eb147c-5c75-4a90-8c3a-7da459af8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/a_naughty_alpaca/dev/github/transformers-gcp/distilbert-base-uncased-finetuned-clinc is already a clone of https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 06:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.306145</td>\n",
       "      <td>0.668065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>1.912227</td>\n",
       "      <td>0.827097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>1.195120</td>\n",
       "      <td>0.883226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.732300</td>\n",
       "      <td>0.890680</td>\n",
       "      <td>0.903871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>0.807984</td>\n",
       "      <td>0.908387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "Several commits (61) will be pushed upstream.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 8s, sys: 6.79 s, total: 5min 14s\n",
      "Wall time: 7min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1590, training_loss=2.0806282571276777, metrics={'train_runtime': 418.4619, 'train_samples_per_second': 182.215, 'train_steps_per_second': 3.8, 'total_flos': 413328758053176.0, 'train_loss': 2.0806282571276777, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "distilbert_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc[\"train\"],\n",
    "    eval_dataset=clinc_enc[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer\n",
    ")\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2760b1d-6252-449f-892f-e8771daf5dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5f2f468e774902b8d5ff0dfa60e9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672338e907ea495c97a5395df3a8d622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Jan31_07-03-11_t4-us-west4-b-n1-standard-16/events.out.tfevents.1706684635.t4-us-west4-b-n1-sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc\n",
      "   ff7637e..b2445e3  main -> main\n",
      "\n",
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc\n",
      "   b2445e3..84721a8  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't forget to add the knowledge-distillation hyperparameters to your model card!\n"
     ]
    }
   ],
   "source": [
    "distilbert_trainer.push_to_hub(\"Fine-tuned student model training completed\")\n",
    "\n",
    "print(\"don't forget to add the knowledge-distillation hyperparameters to your model card!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d99b5-bcaf-46ad-a6bd-4194b58016fb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237333b7-7717-4b96-a4b4-49491ababaef",
   "metadata": {},
   "source": [
    "#### ???\n",
    "\n",
    "So, how is that fine-tuned model of ours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ec9f59-7e34-4824-8cae-1d81eb711a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/a_naughty_alpaca/.cache/huggingface/transformers/tmpnfi32epv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92b099452b54da4892e188646187021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/pytorch_model.bin in cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/5d7a292d7fb16e9bde665dc8fd3842fd30c3ad1d04184c12e7b1f984bca74af1.5b96114a0c4b9c999327ec4f9afd47e27cac15d8eee35088a0e3a85979a5929d\n",
      "creating metadata file for /home/a_naughty_alpaca/.cache/huggingface/transformers/5d7a292d7fb16e9bde665dc8fd3842fd30c3ad1d04184c12e7b1f984bca74af1.5b96114a0c4b9c999327ec4f9afd47e27cac15d8eee35088a0e3a85979a5929d\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/5d7a292d7fb16e9bde665dc8fd3842fd30c3ad1d04184c12e7b1f984bca74af1.5b96114a0c4b9c999327ec4f9afd47e27cac15d8eee35088a0e3a85979a5929d\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-finetuned-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/e1ad7abe1ca6ca7a6d4f1cfc9de256bad2b501ebe7162a2aafff54bb220931dd.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/06f82810f2064792bf87b0151487e96479ade2ffb878fce1b686eefc65f0f8c9.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/47165d38838c8d9d5d1fb8c51331f47ae9876726f3016f84a65a3a72bbcc8221.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/1402d8bf7e99ac0ebec29cb0ceb0226bca0a22660c6bc278dbec17cb2ba182b3.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    }
   ],
   "source": [
    "finetuned_ckpt = \"buruzaemon/distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=finetuned_ckpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9692b5a-1cac-4ea4-9e0c-69cb8fff6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 255.88\n",
      "Average latency (ms) - 12.22 +\\- 0.31\n",
      "Accuracy on test set - 0.855\n"
     ]
    }
   ],
   "source": [
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(\n",
    "    pipe,\n",
    "    clinc[\"test\"],\n",
    "    optim_type=optim_type\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b5118-570c-4f55-a35b-02085434e1a8",
   "metadata": {},
   "source": [
    "### Comparison: base-line model (Teacher) vs. fine-tuned (Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c848824e-20fb-4e96-9453-66f3278df011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        # add a dashed circle around the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx,\n",
    "                marker='$\\u25CC$'\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx\n",
    "            )\n",
    "   \n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "    #for handle in legend.legendHandles:\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80, 90)\n",
    "\n",
    "    # use the slowest model to define the x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(perf_metrics, optim_type)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4a82a-2bb6-42c5-b7f0-9bb16253cd9f",
   "metadata": {},
   "source": [
    "#### Optuna\n",
    "\n",
    "The default backend for hyperparameter search in HF is Optuna. Pretty neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f07d5065-eb88-4c70-abd0-f1669de62c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -2, 2)\n",
    "    y = trial.suggest_float(\"y\", -2, 2)\n",
    "    return (1 - x)**2 + 100*(y - x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a89ec71-9bb7-4c91-90d8-8fb82a464fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-31 07:13:15,649] A new study created in memory with name: no-name-3f505bca-e6c9-4153-b4cc-e8ea0bd406b6\n",
      "[I 2024-01-31 07:13:15,651] Trial 0 finished with value: 290.008313577678 and parameters: {'x': -1.6047973694046762, 'y': 0.8924505217870848}. Best is trial 0 with value: 290.008313577678.\n",
      "[I 2024-01-31 07:13:15,653] Trial 1 finished with value: 9.101647339938243 and parameters: {'x': 0.25266313626113934, 'y': 0.35612507735287613}. Best is trial 1 with value: 9.101647339938243.\n",
      "[I 2024-01-31 07:13:15,654] Trial 2 finished with value: 75.18570707918255 and parameters: {'x': -1.40559412497737, 'y': 1.142635278563151}. Best is trial 1 with value: 9.101647339938243.\n",
      "[I 2024-01-31 07:13:15,655] Trial 3 finished with value: 132.90032853069678 and parameters: {'x': 1.6050366915733902, 'y': 1.424907535430282}. Best is trial 1 with value: 9.101647339938243.\n",
      "[I 2024-01-31 07:13:15,656] Trial 4 finished with value: 3.8902612578924307 and parameters: {'x': -0.5552531441912154, 'y': 0.42960934821051566}. Best is trial 4 with value: 3.8902612578924307.\n",
      "[I 2024-01-31 07:13:15,657] Trial 5 finished with value: 849.6905495655251 and parameters: {'x': -1.341745628619989, 'y': -1.1052423334041546}. Best is trial 4 with value: 3.8902612578924307.\n",
      "[I 2024-01-31 07:13:15,658] Trial 6 finished with value: 1201.012210110765 and parameters: {'x': -1.305300608870359, 'y': -1.754076667969393}. Best is trial 4 with value: 3.8902612578924307.\n",
      "[I 2024-01-31 07:13:15,659] Trial 7 finished with value: 314.37612025994247 and parameters: {'x': 1.8261011701229033, 'y': 1.5635055197814949}. Best is trial 4 with value: 3.8902612578924307.\n",
      "[I 2024-01-31 07:13:15,660] Trial 8 finished with value: 979.9776942612178 and parameters: {'x': 1.4982457565277993, 'y': -0.8853226642769192}. Best is trial 4 with value: 3.8902612578924307.\n",
      "[I 2024-01-31 07:13:15,661] Trial 9 finished with value: 2.47590661684129 and parameters: {'x': 0.6684583925032648, 'y': 0.6006542671181445}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,695] Trial 10 finished with value: 53.442409408335415 and parameters: {'x': 0.6275574471613965, 'y': -0.33626545238412975}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,704] Trial 11 finished with value: 4.87298505558278 and parameters: {'x': -0.6471079488603055, 'y': 0.2717786168364511}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,715] Trial 12 finished with value: 40.68962488873092 and parameters: {'x': -0.33210526213746683, 'y': 0.7341137546097101}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,725] Trial 13 finished with value: 169.44584625835617 and parameters: {'x': 0.7907415557111932, 'y': 1.9268176630005276}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,735] Trial 14 finished with value: 42.95500844245856 and parameters: {'x': -0.5883799558465728, 'y': -0.2896711003247401}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,746] Trial 15 finished with value: 18.03684722780713 and parameters: {'x': 0.9427848592160399, 'y': 0.4641837370598981}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,756] Trial 16 finished with value: 2.8757882612916257 and parameters: {'x': 0.07894808108864448, 'y': -0.13615581042879654}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,766] Trial 17 finished with value: 4.47103517897376 and parameters: {'x': 0.1602173760654501, 'y': -0.16838709348943626}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,775] Trial 18 finished with value: 346.8681895907504 and parameters: {'x': 1.082284296695212, 'y': -0.6910822937994252}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,784] Trial 19 finished with value: 275.90900797324446 and parameters: {'x': 0.5122962077575204, 'y': -1.3978873599074957}. Best is trial 9 with value: 2.47590661684129.\n",
      "[I 2024-01-31 07:13:15,794] Trial 20 finished with value: 1.450233187299226 and parameters: {'x': -0.1981452643044156, 'y': 0.0271449815155691}. Best is trial 20 with value: 1.450233187299226.\n",
      "[I 2024-01-31 07:13:15,804] Trial 21 finished with value: 1.2585337742076106 and parameters: {'x': -0.06408254797759683, 'y': 0.03963995237481526}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,814] Trial 22 finished with value: 1.8100012027861778 and parameters: {'x': -0.18483287481368582, 'y': 0.0978948369340481}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,823] Trial 23 finished with value: 36.767499094463446 and parameters: {'x': -0.26017017779607476, 'y': -0.525434317611817}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,833] Trial 24 finished with value: 45.16441661852056 and parameters: {'x': -0.8533262304107813, 'y': 0.08218114716192834}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,842] Trial 25 finished with value: 71.39739692613801 and parameters: {'x': -0.9491016761195429, 'y': 0.07861153265210592}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,852] Trial 26 finished with value: 92.41587723226365 and parameters: {'x': -0.19975831509019432, 'y': 0.9937191801736385}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,862] Trial 27 finished with value: 51.9642030825907 and parameters: {'x': 0.37720963249964323, 'y': -0.5758795537564735}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,871] Trial 28 finished with value: 2.0072016822865066 and parameters: {'x': -0.08960767557692302, 'y': 0.09858100130890915}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,881] Trial 29 finished with value: 7.081295148949216 and parameters: {'x': -0.9936970995896954, 'y': 0.8111822016552537}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,890] Trial 30 finished with value: 1915.2854777560742 and parameters: {'x': -1.830923266776963, 'y': -1.0149517816957379}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,900] Trial 31 finished with value: 1.8268460140255538 and parameters: {'x': -0.07551035639862498, 'y': 0.08756288448234345}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,910] Trial 32 finished with value: 9.87634194094516 and parameters: {'x': 0.010970069819561945, 'y': 0.29841820930575513}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,920] Trial 33 finished with value: 29.962836366080886 and parameters: {'x': -0.44068086143298196, 'y': -0.3338844154439441}. Best is trial 21 with value: 1.2585337742076106.\n",
      "[I 2024-01-31 07:13:15,930] Trial 34 finished with value: 1.1091459833654096 and parameters: {'x': 0.2773081063889229, 'y': 0.0002928403435393731}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,941] Trial 35 finished with value: 120.67870860821097 and parameters: {'x': 0.2742444260234463, 'y': 1.1713486253395042}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,951] Trial 36 finished with value: 12.72022884596823 and parameters: {'x': 0.4441499504126347, 'y': 0.5495653545355612}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,960] Trial 37 finished with value: 58.07051417476066 and parameters: {'x': -0.7869968620683969, 'y': -0.12142711337857487}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,970] Trial 38 finished with value: 310.9774276648584 and parameters: {'x': -1.1619307081151142, 'y': -0.4000698322342545}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,980] Trial 39 finished with value: 154.61369745000104 and parameters: {'x': 1.2388394496558615, 'y': 0.29151502098507254}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,989] Trial 40 finished with value: 42.20887282883845 and parameters: {'x': 0.15586644566051855, 'y': -0.6198819575059948}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:15,999] Trial 41 finished with value: 1.244575324694633 and parameters: {'x': -0.11478533241885076, 'y': 0.01745233866812769}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:16,009] Trial 42 finished with value: 8.482596367052023 and parameters: {'x': -0.4022140082783571, 'y': -0.09349614311227253}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:16,019] Trial 43 finished with value: 10.835235043665907 and parameters: {'x': -0.16703742580705788, 'y': 0.33568809474008426}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:16,028] Trial 44 finished with value: 44.26392232487683 and parameters: {'x': 0.2423256356604319, 'y': 0.7197047080520922}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:16,038] Trial 45 finished with value: 3.130232827173301 and parameters: {'x': -0.489930771121509, 'y': 0.14462046705539983}. Best is trial 34 with value: 1.1091459833654096.\n",
      "[I 2024-01-31 07:13:16,048] Trial 46 finished with value: 0.2017625047233524 and parameters: {'x': 0.7315438592968273, 'y': 0.5711694442240705}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,058] Trial 47 finished with value: 22.53601341933629 and parameters: {'x': 0.7438200634752926, 'y': 1.0272976662919242}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,069] Trial 48 finished with value: 221.960125699801 and parameters: {'x': 1.426043186325878, 'y': 0.5443758384554446}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,079] Trial 49 finished with value: 550.0475094387605 and parameters: {'x': 0.5970060455683631, 'y': -1.9885466923733568}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,090] Trial 50 finished with value: 448.3212569260194 and parameters: {'x': 1.925478835655171, 'y': 1.5921324946783546}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,100] Trial 51 finished with value: 9.773459234841381 and parameters: {'x': 0.850958732941426, 'y': 0.41186091495067284}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,110] Trial 52 finished with value: 2.3946487493243476 and parameters: {'x': 0.3340700604585365, 'y': -0.02808205662471802}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,120] Trial 53 finished with value: 62.71469008313861 and parameters: {'x': -0.6950137656165709, 'y': -0.29052951056243315}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,131] Trial 54 finished with value: 5.397151084115247 and parameters: {'x': 0.03327333532148702, 'y': 0.21235556193628363}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,141] Trial 55 finished with value: 75.99804167300593 and parameters: {'x': -0.30910543107905813, 'y': -0.7663371461767101}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,152] Trial 56 finished with value: 202.13768523344595 and parameters: {'x': 0.9724656493447031, 'y': -0.476059227427325}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,163] Trial 57 finished with value: 13.075887651453108 and parameters: {'x': 0.5695307874685579, 'y': 0.6833999110339847}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,174] Trial 58 finished with value: 9.343781649523423 and parameters: {'x': 0.19192101970166597, 'y': -0.25796778099773476}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,184] Trial 59 finished with value: 20.709920008067613 and parameters: {'x': -0.12779343739312019, 'y': 0.4572166576464325}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,194] Trial 60 finished with value: 21.518425398651267 and parameters: {'x': -0.6150034953553432, 'y': -0.056629173179300704}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,205] Trial 61 finished with value: 1.0389255432005635 and parameters: {'x': -0.01775650766316328, 'y': 0.005880573709941441}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,216] Trial 62 finished with value: 5.939746050243689 and parameters: {'x': 0.05661010833476027, 'y': 0.22792144968180536}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,227] Trial 63 finished with value: 2.1235744855949337 and parameters: {'x': -0.3012523226018591, 'y': 0.02515441931921992}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,237] Trial 64 finished with value: 5.61868083827742 and parameters: {'x': -0.08061508100065073, 'y': -0.20447400044422478}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,247] Trial 65 finished with value: 34.02413376769363 and parameters: {'x': 0.36304784111774646, 'y': -0.4480102500151676}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,257] Trial 66 finished with value: 3.415781578589998 and parameters: {'x': -0.2183667994745359, 'y': 0.18665757891160356}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,267] Trial 67 finished with value: 49.80248221005265 and parameters: {'x': 0.4566128453267667, 'y': 0.9121089104125069}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,277] Trial 68 finished with value: 19.954250834250207 and parameters: {'x': -0.4268492420436053, 'y': 0.6055010204683325}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,288] Trial 69 finished with value: 0.9150464598966083 and parameters: {'x': 0.044947022613069, 'y': -0.0033837176755603868}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,298] Trial 70 finished with value: 4.644898326729978 and parameters: {'x': 0.06680219016919917, 'y': -0.18980635770449444}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,309] Trial 71 finished with value: 1.1846286276040225 and parameters: {'x': 0.1523648619115653, 'y': -0.045059636717875766}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,320] Trial 72 finished with value: 0.720121047314514 and parameters: {'x': 0.15369596442545055, 'y': 0.017385040816211297}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,330] Trial 73 finished with value: 37.16453801710145 and parameters: {'x': 0.7211881329212169, 'y': -0.0888770196964978}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,341] Trial 74 finished with value: 9.029732406172261 and parameters: {'x': 0.2032384495539041, 'y': 0.33104546491084297}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,351] Trial 75 finished with value: 15.227427633296658 and parameters: {'x': -0.05234239074589209, 'y': -0.37302620028631556}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,362] Trial 76 finished with value: 6.032801008478577 and parameters: {'x': 0.4935326061908547, 'y': 0.003235260796806679}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,372] Trial 77 finished with value: 2.7248566830554104 and parameters: {'x': 0.11563432595835466, 'y': 0.15275400923885424}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,383] Trial 78 finished with value: 175.00140133710224 and parameters: {'x': 0.32539304987760237, 'y': -1.2152791066498243}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,393] Trial 79 finished with value: 0.26427998298094246 and parameters: {'x': 1.1625067116599095, 'y': 1.302649777433202}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,403] Trial 80 finished with value: 2.5264013813508326 and parameters: {'x': 1.260194975773973, 'y': 1.7448937990114421}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,414] Trial 81 finished with value: 278.1851397884543 and parameters: {'x': 1.6802951435462143, 'y': 1.1568914294726673}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,425] Trial 82 finished with value: 136.99942153920122 and parameters: {'x': 0.8501720585525734, 'y': 1.8931641501561063}. Best is trial 46 with value: 0.2017625047233524.\n",
      "[I 2024-01-31 07:13:16,435] Trial 83 finished with value: 0.05116685616713867 and parameters: {'x': 1.1415887324248584, 'y': 1.285584117739146}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,445] Trial 84 finished with value: 2.127914611243875 and parameters: {'x': 1.145946148859344, 'y': 1.4583343802936193}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,456] Trial 85 finished with value: 9.272240999258354 and parameters: {'x': 1.010085486914259, 'y': 1.3247745674242397}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,466] Trial 86 finished with value: 4.697370525102196 and parameters: {'x': 1.062871914068881, 'y': 1.3463396757737165}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,477] Trial 87 finished with value: 13.681251975536753 and parameters: {'x': 1.3774131181003884, 'y': 1.5293156604687868}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,487] Trial 88 finished with value: 10.977281938379237 and parameters: {'x': 0.8555602886886089, 'y': 1.0629882279110259}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,499] Trial 89 finished with value: 137.28447667975135 and parameters: {'x': 1.5767476941202447, 'y': 1.315869052473838}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,510] Trial 90 finished with value: 15.580340506892725 and parameters: {'x': 0.6839247175674323, 'y': 0.8612048832262453}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,520] Trial 91 finished with value: 97.17336275696032 and parameters: {'x': 1.1776259452525135, 'y': 0.40119740945645455}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,531] Trial 92 finished with value: 1.292482711092331 and parameters: {'x': -0.017147163431912638, 'y': 0.05107732552151394}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,542] Trial 93 finished with value: 2.706485962845502 and parameters: {'x': 0.25716433821313356, 'y': -0.08065482497650983}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,552] Trial 94 finished with value: 3.3946014657146746 and parameters: {'x': 0.10780296204981803, 'y': -0.14957982134284517}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,563] Trial 95 finished with value: 390.4769684474118 and parameters: {'x': 1.290948728604306, 'y': -0.3092861841527922}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,573] Trial 96 finished with value: 0.2695964680547781 and parameters: {'x': 0.5468736608151421, 'y': 0.273718682876963}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,584] Trial 97 finished with value: 4.942740200302567 and parameters: {'x': 0.5404021622432068, 'y': 0.5095548414730393}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,595] Trial 98 finished with value: 1.7356550008426166 and parameters: {'x': 0.39321975773157086, 'y': 0.27156076787959427}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,606] Trial 99 finished with value: 7.305905173057863 and parameters: {'x': 0.6213775339184219, 'y': 0.6537394513687655}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,618] Trial 100 finished with value: 51.07315113319058 and parameters: {'x': 0.916672962564381, 'y': 0.1256830809029839}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,629] Trial 101 finished with value: 7.38955288167764 and parameters: {'x': 0.1589582001483305, 'y': -0.2332318374636182}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,640] Trial 102 finished with value: 1.981786573396871 and parameters: {'x': 0.29825946244958373, 'y': 0.21099750369630887}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,651] Trial 103 finished with value: 1.3510985592458542 and parameters: {'x': -0.16236492797628718, 'y': 0.026110706241144258}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,663] Trial 104 finished with value: 3.076871470317758 and parameters: {'x': 0.4311218116104727, 'y': 0.35179519040592266}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,674] Trial 105 finished with value: 1.1371030189665912 and parameters: {'x': -0.007978341492872187, 'y': -0.03473328300806288}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,684] Trial 106 finished with value: 157.9362793508812 and parameters: {'x': 0.0016138773739336754, 'y': 1.2527575866338914}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,695] Trial 107 finished with value: 3.462694742573545 and parameters: {'x': 0.16953151999646732, 'y': -0.13778284119787831}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,706] Trial 108 finished with value: 247.7642190960597 and parameters: {'x': -0.3549694291374113, 'y': 1.694213348332435}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,717] Trial 109 finished with value: 626.5587854764809 and parameters: {'x': -1.5702225356138677, 'y': -0.024286209276616733}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,728] Trial 110 finished with value: 20.369717814826142 and parameters: {'x': 0.773092707948668, 'y': 0.14691485155550935}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,739] Trial 111 finished with value: 1.5755772334581786 and parameters: {'x': -0.12105890676411911, 'y': 0.07111800423718695}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,750] Trial 112 finished with value: 5.48513314362574 and parameters: {'x': 0.24341245676831239, 'y': 0.28089592868221114}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,760] Trial 113 finished with value: 8.246797157326338 and parameters: {'x': -0.23420461331668999, 'y': -0.20444602265151607}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,771] Trial 114 finished with value: 18.128813773426067 and parameters: {'x': -0.0355687770942843, 'y': -0.41172894294141515}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,782] Trial 115 finished with value: 1.9037552229988224 and parameters: {'x': 0.0743311607174881, 'y': -0.09679263961357759}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,793] Trial 116 finished with value: 2.418484840188713 and parameters: {'x': -0.5023910042949971, 'y': 0.21223378923157793}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,803] Trial 117 finished with value: 2.597606617669271 and parameters: {'x': -0.2686624156030785, 'y': -0.027223841039576195}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,814] Trial 118 finished with value: 3.5175964186998523 and parameters: {'x': 0.4963396812068571, 'y': 0.42701637651918534}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,826] Trial 119 finished with value: 9.719117603881607 and parameters: {'x': -0.10456608571490975, 'y': -0.28059725907061683}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,837] Trial 120 finished with value: 0.42150751663164765 and parameters: {'x': 0.3509774918287301, 'y': 0.12151996549641317}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,848] Trial 121 finished with value: 0.4712416601612788 and parameters: {'x': 0.34446753520182877, 'y': 0.1390340570703603}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,859] Trial 122 finished with value: 1.2750335543664875 and parameters: {'x': 0.39276012546491634, 'y': 0.059061078701989025}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,869] Trial 123 finished with value: 2.6367325628118214 and parameters: {'x': 0.31369149661017154, 'y': -0.048761268210581}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,881] Trial 124 finished with value: 5.248652600401735 and parameters: {'x': 0.5898145303993326, 'y': 0.12248374281957415}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,892] Trial 125 finished with value: 5.293085295757837 and parameters: {'x': 0.21912211674595866, 'y': 0.2644241836052071}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,904] Trial 126 finished with value: 3.9401599934350955 and parameters: {'x': 0.11628250886739039, 'y': -0.16421985863904975}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,915] Trial 127 finished with value: 7.3046802057226525 and parameters: {'x': 0.6733540535614753, 'y': 0.18511511822566756}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,926] Trial 128 finished with value: 15.278583658190465 and parameters: {'x': 0.024303619691120992, 'y': 0.3790956227225033}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,937] Trial 129 finished with value: 39.128317325767526 and parameters: {'x': 0.3827274223898963, 'y': 0.768953507311232}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,948] Trial 130 finished with value: 382.2956700250516 and parameters: {'x': 1.3971341008188205, 'y': -0.0028512194333191188}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,960] Trial 131 finished with value: 0.5397095346270055 and parameters: {'x': 0.27466218457791575, 'y': 0.0870988988825486}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,971] Trial 132 finished with value: 0.5618308490202378 and parameters: {'x': 0.28635663402620515, 'y': 0.05907764484203246}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,981] Trial 133 finished with value: 0.6350425528395336 and parameters: {'x': 0.2513377141607597, 'y': 0.09047400433868194}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:16,993] Trial 134 finished with value: 2.268759003162368 and parameters: {'x': 0.4960336718568787, 'y': 0.10410656500603399}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,004] Trial 135 finished with value: 6.151067913321981 and parameters: {'x': 0.2718134548349439, 'y': 0.3109650771778072}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,015] Trial 136 finished with value: 21.927235282489036 and parameters: {'x': 0.2876446699873258, 'y': 0.5455545938490476}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,026] Trial 137 finished with value: 0.5573754127024441 and parameters: {'x': 0.3574334129648971, 'y': 0.16576964231344016}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,036] Trial 138 finished with value: 8.2000371954277 and parameters: {'x': 0.43965696381192065, 'y': 0.4741194144966209}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,047] Trial 139 finished with value: 1.193902258334674 and parameters: {'x': 0.5371257444367553, 'y': 0.1895268112895866}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,059] Trial 140 finished with value: 167.5541677524603 and parameters: {'x': 0.35779506712367337, 'y': 1.4208504028300655}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,070] Trial 141 finished with value: 0.8740736906943246 and parameters: {'x': 0.19103612290282898, 'y': 0.08336175409610985}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,081] Trial 142 finished with value: 1.1398792895220913 and parameters: {'x': 0.2064530627678141, 'y': 0.11404853113653074}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,092] Trial 143 finished with value: 2.5873074034081265 and parameters: {'x': 0.33789829931935034, 'y': 0.26076750950132116}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,102] Trial 144 finished with value: 2.1985867132304966 and parameters: {'x': 0.11189116417913028, 'y': -0.10621744705333008}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,114] Trial 145 finished with value: 47.78100009668991 and parameters: {'x': 0.6271280561371476, 'y': 1.0835212020274305}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,125] Trial 146 finished with value: 102.37347275901334 and parameters: {'x': 0.44873053963762294, 'y': 1.211653978617236}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,135] Trial 147 finished with value: 6.3752279668861425 and parameters: {'x': 1.0898943163909727, 'y': 0.9355375583574284}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,147] Trial 148 finished with value: 0.5821057093332319 and parameters: {'x': 0.24241226934829918, 'y': 0.04972681743867714}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,158] Trial 149 finished with value: 1.1199384547647515 and parameters: {'x': 0.19087181719602497, 'y': 0.10464129033723577}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,169] Trial 150 finished with value: 0.5889056528257289 and parameters: {'x': 0.24016174903606416, 'y': 0.04692988211562153}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,180] Trial 151 finished with value: 0.630336774084783 and parameters: {'x': 0.2523941687723829, 'y': 0.03697786643984438}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,192] Trial 152 finished with value: 1.4922975212872287 and parameters: {'x': 0.2571423920421716, 'y': 0.16309953164891294}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,203] Trial 153 finished with value: 1.0557834631955063 and parameters: {'x': 0.35240180448721614, 'y': 0.04441234657524853}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,215] Trial 154 finished with value: 0.610109140072684 and parameters: {'x': 0.41869908343059403, 'y': 0.2274815574832466}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,227] Trial 155 finished with value: 0.4165817048986642 and parameters: {'x': 0.5263807435108101, 'y': 0.23322848252565803}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,238] Trial 156 finished with value: 0.3237891519126532 and parameters: {'x': 0.5150834559956221, 'y': 0.23553764011084688}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,250] Trial 157 finished with value: 0.16749010025887337 and parameters: {'x': 0.5935169305755106, 'y': 0.3475067033585271}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,262] Trial 158 finished with value: 5.588115193901344 and parameters: {'x': 0.7500840054626611, 'y': 0.3275588506026805}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,273] Trial 159 finished with value: 0.8675800877302526 and parameters: {'x': 0.5770971404172457, 'y': 0.4160310642365751}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,285] Trial 160 finished with value: 0.876832232563898 and parameters: {'x': 0.6655154165406355, 'y': 0.3554492183297218}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,297] Trial 161 finished with value: 0.2824306836859811 and parameters: {'x': 0.489477084499491, 'y': 0.25435163568644703}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,308] Trial 162 finished with value: 0.6503433194138263 and parameters: {'x': 0.5638883753507568, 'y': 0.2501357449712163}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,320] Trial 163 finished with value: 15.340394764571316 and parameters: {'x': 0.47829914649970395, 'y': 0.6169481639084603}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,331] Trial 164 finished with value: 29.083365976125968 and parameters: {'x': 0.8666644452622101, 'y': 0.21198215826136513}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,342] Trial 165 finished with value: 4.147258949724218 and parameters: {'x': 0.4230544686213079, 'y': 0.37427978903571824}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,353] Trial 166 finished with value: 6.1119651057476565 and parameters: {'x': 0.49044484621323853, 'y': 0.4824518070877044}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,364] Trial 167 finished with value: 8.502611276741554 and parameters: {'x': 0.7572228639900398, 'y': 0.28280651910855853}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,376] Trial 168 finished with value: 1.2462522412298858 and parameters: {'x': 0.5411202161174331, 'y': 0.19104264597344106}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,388] Trial 169 finished with value: 8.923698807742298 and parameters: {'x': 0.6851223025942006, 'y': 0.17233111385980765}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,400] Trial 170 finished with value: 4.633006885717596 and parameters: {'x': 0.329583429449748, 'y': 0.31316248315175466}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,411] Trial 171 finished with value: 1.0103855303294393 and parameters: {'x': 0.41928845154302413, 'y': 0.0937564999024443}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,422] Trial 172 finished with value: 1.5660923294423303 and parameters: {'x': 0.6060410832323335, 'y': 0.24850495851256288}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,433] Trial 173 finished with value: 0.403882594566998 and parameters: {'x': 0.40311874878999265, 'y': 0.14068378006423896}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,445] Trial 174 finished with value: 8.267393446370992 and parameters: {'x': 0.39294658435230934, 'y': 0.4354564728222079}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,456] Trial 175 finished with value: 0.38893490190909963 and parameters: {'x': 0.4558628712800228, 'y': 0.1773397107917007}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,467] Trial 176 finished with value: 0.9907952767607989 and parameters: {'x': 0.5088089093619865, 'y': 0.17231130256525123}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,479] Trial 177 finished with value: 0.5978588083143727 and parameters: {'x': 0.6321410744940938, 'y': 0.3315921462351447}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,490] Trial 178 finished with value: 1.7691233197087026 and parameters: {'x': 0.818691925787094, 'y': 0.5384896031824854}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,502] Trial 179 finished with value: 43.59527195131144 and parameters: {'x': 1.0071998397256499, 'y': 0.35418475201264854}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,514] Trial 180 finished with value: 1.163546382984056 and parameters: {'x': 0.6160983442468467, 'y': 0.2787721152629111}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,526] Trial 181 finished with value: 0.33772772603414364 and parameters: {'x': 0.451167578423353, 'y': 0.22265995686186876}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,537] Trial 182 finished with value: 1.4513266184143718 and parameters: {'x': 0.5139031775141032, 'y': 0.15386778188227443}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,548] Trial 183 finished with value: 0.8912754935487682 and parameters: {'x': 0.7020239157660492, 'y': 0.40325600951271806}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,560] Trial 184 finished with value: 4.168751991028381 and parameters: {'x': 0.3691867623270392, 'y': 0.33048503002600177}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,571] Trial 185 finished with value: 26.86247614297907 and parameters: {'x': 1.2090142864170699, 'y': 1.9795841494379856}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,583] Trial 186 finished with value: 3.507649348391467 and parameters: {'x': 0.5651723148904939, 'y': 0.13725020271160726}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,594] Trial 187 finished with value: 0.5654479389430183 and parameters: {'x': 0.439126568548474, 'y': 0.24291896111514355}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,605] Trial 188 finished with value: 0.3250527036154957 and parameters: {'x': 0.46612402308997675, 'y': 0.2372788897983082}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,617] Trial 189 finished with value: 0.39761697812100055 and parameters: {'x': 0.4538104223284448, 'y': 0.23745483773126908}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,629] Trial 190 finished with value: 0.27399746091248917 and parameters: {'x': 0.47994002115575407, 'y': 0.23628807720751677}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,641] Trial 191 finished with value: 0.39286555905626835 and parameters: {'x': 0.5319994492895448, 'y': 0.2413291646098371}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,653] Trial 192 finished with value: 4.890031928522275 and parameters: {'x': 0.5064780042294604, 'y': 0.4720766427130215}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,664] Trial 193 finished with value: 217.04131816727005 and parameters: {'x': 1.3069896410211537, 'y': 0.23530958378985525}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,675] Trial 194 finished with value: 0.8662642627646024 and parameters: {'x': 0.46728305971207945, 'y': 0.14203328203619242}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,686] Trial 195 finished with value: 1.4430474356313199 and parameters: {'x': 0.3348176882177121, 'y': 0.2121318765268841}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,698] Trial 196 finished with value: 0.24017871125276866 and parameters: {'x': 0.5605921589812866, 'y': 0.29256115859075316}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,710] Trial 197 finished with value: 0.545497174762889 and parameters: {'x': 0.5814928404947184, 'y': 0.39899022414118956}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,721] Trial 198 finished with value: 1.6679119877761344 and parameters: {'x': 0.716766325666963, 'y': 0.3877503675067336}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,733] Trial 199 finished with value: 0.981066291294152 and parameters: {'x': 0.5987327287707763, 'y': 0.4490375405730145}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,744] Trial 200 finished with value: 0.8631399502354747 and parameters: {'x': 0.656877514021471, 'y': 0.5178250193722279}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,755] Trial 201 finished with value: 0.9389945121030446 and parameters: {'x': 0.5539089880754838, 'y': 0.39283826403802835}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,767] Trial 202 finished with value: 342.2084914911139 and parameters: {'x': 0.4479812518384937, 'y': -1.6483767957925095}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,778] Trial 203 finished with value: 0.30405587699146713 and parameters: {'x': 0.5268711209861413, 'y': 0.30591365496743805}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,790] Trial 204 finished with value: 0.21713169352038864 and parameters: {'x': 0.5345160189456242, 'y': 0.28357112334878143}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,801] Trial 205 finished with value: 0.27800614953017594 and parameters: {'x': 0.5066215027619586, 'y': 0.27526206935083086}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,813] Trial 206 finished with value: 0.36690589859037503 and parameters: {'x': 0.5200376853893708, 'y': 0.307390794345289}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,824] Trial 207 finished with value: 7.878036072524221 and parameters: {'x': 0.756981641676354, 'y': 0.29339684798968746}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,835] Trial 208 finished with value: 0.6051470767340585 and parameters: {'x': 0.49798069875025236, 'y': 0.3074089883887647}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,847] Trial 209 finished with value: 7.974015565122373 and parameters: {'x': 0.9358349805594501, 'y': 0.5934770257301052}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,859] Trial 210 finished with value: 1347.4483275977532 and parameters: {'x': -1.9841469254592012, 'y': 0.2782283637519303}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,870] Trial 211 finished with value: 2.8164441900776063 and parameters: {'x': 0.6465624511907326, 'y': 0.25398429156834534}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,882] Trial 212 finished with value: 0.65050088777158 and parameters: {'x': 0.5307300969458391, 'y': 0.21607819766054503}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,894] Trial 213 finished with value: 1.1498691526211882 and parameters: {'x': 0.4957179562445869, 'y': 0.34037078759645095}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,908] Trial 214 finished with value: 141.2777989196856 and parameters: {'x': 0.45013790236421847, 'y': 1.389954963698571}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,920] Trial 215 finished with value: 2.048020448284357 and parameters: {'x': 0.5730386680585048, 'y': 0.1917817901221655}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,931] Trial 216 finished with value: 2.6872145080440077 and parameters: {'x': 0.3993854294585755, 'y': 0.3120366415176892}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,943] Trial 217 finished with value: 1.1527695386853505 and parameters: {'x': 0.6075837827881949, 'y': 0.4690969870196035}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,955] Trial 218 finished with value: 91.72600024144961 and parameters: {'x': 1.0948951398950253, 'y': 0.2411054637232688}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,967] Trial 219 finished with value: 297.0206009475743 and parameters: {'x': 1.7314499215406967, 'y': 1.2760431617729904}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,980] Trial 220 finished with value: 101.16742063805809 and parameters: {'x': 0.7065916851409925, 'y': 1.5046639330066365}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:17,992] Trial 221 finished with value: 0.37692122231660613 and parameters: {'x': 0.3996124647752775, 'y': 0.1468620161952796}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,003] Trial 222 finished with value: 0.3248083665145834 and parameters: {'x': 0.43237875000074794, 'y': 0.18183818184864434}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,015] Trial 223 finished with value: 0.4040049567495802 and parameters: {'x': 0.434095282446403, 'y': 0.15949794586953334}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,027] Trial 224 finished with value: 0.5296971646488269 and parameters: {'x': 0.5207194572966954, 'y': 0.32591985197622786}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,039] Trial 225 finished with value: 0.39430852940466476 and parameters: {'x': 0.4506463632099835, 'y': 0.2334990990173569}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,051] Trial 226 finished with value: 0.3366326866740773 and parameters: {'x': 0.4277362755097488, 'y': 0.17339437016027603}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,062] Trial 227 finished with value: 2.3771554095537457 and parameters: {'x': 0.4334279445321829, 'y': 0.3312526530607016}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,074] Trial 228 finished with value: 0.4378131818206174 and parameters: {'x': 0.6723085262064036, 'y': 0.39451558471437576}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,086] Trial 229 finished with value: 1.6222732394588908 and parameters: {'x': 0.5689722592878151, 'y': 0.20387584089598718}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,097] Trial 230 finished with value: 1.97856506575546 and parameters: {'x': 0.38617776714803154, 'y': 0.2756950050356426}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,109] Trial 231 finished with value: 0.5993378455470078 and parameters: {'x': 0.4329915128709006, 'y': 0.1347711938484959}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,121] Trial 232 finished with value: 0.9179907819135062 and parameters: {'x': 0.4893430139533521, 'y': 0.1583874651348599}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,132] Trial 233 finished with value: 0.3106435795361929 and parameters: {'x': 0.45455644834527986, 'y': 0.19516080058040564}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,144] Trial 234 finished with value: 0.6951708515927798 and parameters: {'x': 0.5748423741719386, 'y': 0.25872124509322736}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,156] Trial 235 finished with value: 1.068573991517117 and parameters: {'x': 0.5008404540723707, 'y': 0.3413626351928725}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,168] Trial 236 finished with value: 2.9925933562440057 and parameters: {'x': 0.6282915950362106, 'y': 0.22579985583022907}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,180] Trial 237 finished with value: 0.7467228001055501 and parameters: {'x': 0.3936623128233785, 'y': 0.09340075397772488}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,192] Trial 238 finished with value: 308.39937166798677 and parameters: {'x': 1.4733922984059609, 'y': 0.41539270779993254}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,204] Trial 239 finished with value: 0.3432882720722109 and parameters: {'x': 0.46415406770691947, 'y': 0.1917414440711745}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,216] Trial 240 finished with value: 13.543896109200611 and parameters: {'x': 0.801784099007935, 'y': 0.27537159793088817}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,228] Trial 241 finished with value: 0.48230824783108917 and parameters: {'x': 0.4621370858648428, 'y': 0.1696375854638012}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,239] Trial 242 finished with value: 0.6443397248326614 and parameters: {'x': 0.5259013804724529, 'y': 0.21179802143963924}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,251] Trial 243 finished with value: 98.7179681738252 and parameters: {'x': 0.37262794409538236, 'y': 1.1304380533521825}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,263] Trial 244 finished with value: 0.7763548468307717 and parameters: {'x': 0.6262398254832815, 'y': 0.3123854562126105}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,276] Trial 245 finished with value: 1.1713024159461507 and parameters: {'x': 0.4661138656447332, 'y': 0.12312032720780927}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,288] Trial 246 finished with value: 7.2092861596513345 and parameters: {'x': 0.317783884124213, 'y': 0.3606761680528598}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,300] Trial 247 finished with value: 0.9893381918096316 and parameters: {'x': 0.5533536915127628, 'y': 0.21732706857947362}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,313] Trial 248 finished with value: 1.773963354059768 and parameters: {'x': 0.407799519638129, 'y': 0.04699990567352205}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,325] Trial 249 finished with value: 0.2389144499566911 and parameters: {'x': 0.5493378011192402, 'y': 0.2828463671852678}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,337] Trial 250 finished with value: 2.1281495283242515 and parameters: {'x': 0.6386112379003452, 'y': 0.26648968574704623}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,349] Trial 251 finished with value: 2.6463497160596336 and parameters: {'x': 0.531716840348733, 'y': 0.4385130611142064}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,361] Trial 252 finished with value: 1.1071047528393316 and parameters: {'x': 0.6813618892802821, 'y': 0.363975686161996}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,374] Trial 253 finished with value: 0.4085428485397258 and parameters: {'x': 0.5785076788148728, 'y': 0.28662042459643894}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,386] Trial 254 finished with value: 0.4063329324279451 and parameters: {'x': 0.485267722166167, 'y': 0.1978837496123229}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,399] Trial 255 finished with value: 217.35098288639077 and parameters: {'x': 0.479062687876212, 'y': 1.7028632248581919}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,411] Trial 256 finished with value: 1.7611962016257932 and parameters: {'x': 0.5817288024922678, 'y': 0.4643546344860706}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,424] Trial 257 finished with value: 2.8809701329255564 and parameters: {'x': 0.37961342670390985, 'y': 0.3020965633229441}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,436] Trial 258 finished with value: 15.488349332674554 and parameters: {'x': 0.7138702705116116, 'y': 0.11709988288160178}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,448] Trial 259 finished with value: 516.4686368060981 and parameters: {'x': 1.1537173830947811, 'y': -0.9414788459083292}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,461] Trial 260 finished with value: 8.172619015232057 and parameters: {'x': 0.3153261232504822, 'y': 0.37698849845622817}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,473] Trial 261 finished with value: 0.5790046036712864 and parameters: {'x': 0.5286637204638249, 'y': 0.2197486810135193}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,485] Trial 262 finished with value: 2.0330643797139847 and parameters: {'x': 0.6256670014910386, 'y': 0.5290433232353255}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,497] Trial 263 finished with value: 5.901536680473468 and parameters: {'x': 0.5017049411913481, 'y': 0.013942444417217537}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,509] Trial 264 finished with value: 0.425510981586011 and parameters: {'x': 0.40289637011653334, 'y': 0.1885891932387098}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,522] Trial 265 finished with value: 2.0279951591087064 and parameters: {'x': 0.7461350193200261, 'y': 0.6968441144391633}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,534] Trial 266 finished with value: 1.177972071104458 and parameters: {'x': 0.4560320003404581, 'y': 0.30188381428803734}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,546] Trial 267 finished with value: 7.287306427226873 and parameters: {'x': 0.5871103035680283, 'y': 0.07792466190267952}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,559] Trial 268 finished with value: 3.221631884857032 and parameters: {'x': 0.6579371570737502, 'y': 0.2566818447605345}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,572] Trial 269 finished with value: 8.069048907874945 and parameters: {'x': 0.3270979409092323, 'y': 0.3829686366260784}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,584] Trial 270 finished with value: 0.6293152782053166 and parameters: {'x': 0.45344304956490195, 'y': 0.14811357508995207}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,597] Trial 271 finished with value: 233.8970026661382 and parameters: {'x': 1.3253536348360198, 'y': 0.2275392108546616}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,609] Trial 272 finished with value: 0.4765540356703449 and parameters: {'x': 0.5543021968743129, 'y': 0.35996785859879404}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,622] Trial 273 finished with value: 46.82149493716815 and parameters: {'x': 0.8802000798014458, 'y': 0.09059472114808291}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,633] Trial 274 finished with value: 3.581456208390547 and parameters: {'x': 0.34335102131114204, 'y': 0.29537987642927443}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,645] Trial 275 finished with value: 0.5065333464064222 and parameters: {'x': 0.46285498685870274, 'y': 0.1675433497969041}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,657] Trial 276 finished with value: 24.00262885691476 and parameters: {'x': -1.1933326790167906, 'y': 0.9859570394399457}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,670] Trial 277 finished with value: 655.7605071810968 and parameters: {'x': 1.9499107835562128, 'y': 1.2431323751894603}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,682] Trial 278 finished with value: 10.463887973273565 and parameters: {'x': 0.5468526257491256, 'y': -0.02124184698123327}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,695] Trial 279 finished with value: 2.8213918481267672 and parameters: {'x': 0.8023176843105824, 'y': 0.8105163508941221}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,708] Trial 280 finished with value: 0.2004345999246199 and parameters: {'x': 0.625373854212797, 'y': 0.4156056888208355}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,720] Trial 281 finished with value: 0.22362604507735145 and parameters: {'x': 0.6553942013580961, 'y': 0.46192564301445155}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,733] Trial 282 finished with value: 1.1652164745504743 and parameters: {'x': 0.702508724369169, 'y': 0.5972834062860053}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,745] Trial 283 finished with value: 0.5967658990601039 and parameters: {'x': 0.6598862154950935, 'y': 0.5048103618511443}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,757] Trial 284 finished with value: 0.42518224663540494 and parameters: {'x': 0.6329088185208189, 'y': 0.4544647880900766}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,769] Trial 285 finished with value: 36.8778445358959 and parameters: {'x': 1.0099649003349915, 'y': 0.4127586063338769}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,781] Trial 286 finished with value: 0.14585687463911423 and parameters: {'x': 0.7619117372121353, 'y': 0.5506480060953184}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,793] Trial 287 finished with value: 0.31236721585892585 and parameters: {'x': 0.8054129441392293, 'y': 0.5962969677711699}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,805] Trial 288 finished with value: 0.4260388462103223 and parameters: {'x': 0.8437704150378402, 'y': 0.6485741365314699}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,818] Trial 289 finished with value: 5.3820399968200805 and parameters: {'x': 0.9135924972387783, 'y': 0.6028199817592608}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,831] Trial 290 finished with value: 3.7135055767744283 and parameters: {'x': 0.7279422862539926, 'y': 0.7206744509435423}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,844] Trial 291 finished with value: 0.2368454950609436 and parameters: {'x': 0.7418906402826736, 'y': 0.5091433831652891}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,856] Trial 292 finished with value: 2.285429935531419 and parameters: {'x': 0.8315010857365743, 'y': 0.5411596370404936}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,868] Trial 293 finished with value: 0.27973228016537766 and parameters: {'x': 0.755566694709318, 'y': 0.6177835502813567}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,881] Trial 294 finished with value: 0.2004648691145595 and parameters: {'x': 0.7621780606718547, 'y': 0.542980505331554}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,893] Trial 295 finished with value: 0.20599825103876837 and parameters: {'x': 0.7943888750996806, 'y': 0.6715162992756714}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,905] Trial 296 finished with value: 0.42312708301224095 and parameters: {'x': 0.7941595190069806, 'y': 0.6923948340427004}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,918] Trial 297 finished with value: 0.2847641631768795 and parameters: {'x': 0.8004915258931427, 'y': 0.5912931953807362}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,931] Trial 298 finished with value: 3.145055504453849 and parameters: {'x': 0.8776287409404436, 'y': 0.593311863848791}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,944] Trial 299 finished with value: 5.12240254557119 and parameters: {'x': 0.9880157408214929, 'y': 0.7498510240156384}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,957] Trial 300 finished with value: 0.3732287800326471 and parameters: {'x': 0.7786635441120126, 'y': 0.6632588988537373}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,969] Trial 301 finished with value: 10.742074935154022 and parameters: {'x': 0.9323510918628999, 'y': 0.541597332908469}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,981] Trial 302 finished with value: 0.08333167623992832 and parameters: {'x': 0.7771041833679756, 'y': 0.5855472122989476}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:18,994] Trial 303 finished with value: 0.17156843286361867 and parameters: {'x': 0.7585429338340213, 'y': 0.6090425333475307}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,006] Trial 304 finished with value: 3.969394367556671 and parameters: {'x': 0.8220643539818506, 'y': 0.8742270282683918}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,018] Trial 305 finished with value: 1.8765332176093947 and parameters: {'x': 0.8739753269402211, 'y': 0.6274271904227947}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,031] Trial 306 finished with value: 0.0698968742166896 and parameters: {'x': 0.746488075343933, 'y': 0.549742061494037}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,044] Trial 307 finished with value: 2.5539973930925566 and parameters: {'x': 0.7784584335004315, 'y': 0.7642668198982404}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,057] Trial 308 finished with value: 0.11381266882657204 and parameters: {'x': 0.7434504507827524, 'y': 0.5746263334084035}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,069] Trial 309 finished with value: 0.4503747636277554 and parameters: {'x': 0.7738603008326967, 'y': 0.5356746720191943}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,081] Trial 310 finished with value: 1.434831863971737 and parameters: {'x': 0.7367105649041865, 'y': 0.6595975178354077}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,094] Trial 311 finished with value: 15.9150255741191 and parameters: {'x': 0.9542444854328811, 'y': 0.5116723725657745}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,107] Trial 312 finished with value: 0.08612450236223214 and parameters: {'x': 0.7288979440195423, 'y': 0.5425297292499198}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,120] Trial 313 finished with value: 26.54201135338047 and parameters: {'x': 1.0630978433200766, 'y': 0.6150262696868631}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,134] Trial 314 finished with value: 0.07550484469397628 and parameters: {'x': 0.7522486481382143, 'y': 0.5539935377470526}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,147] Trial 315 finished with value: 0.129937653054833 and parameters: {'x': 0.7293656960287467, 'y': 0.5557849730008683}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,160] Trial 316 finished with value: 3.8248131454831564 and parameters: {'x': 0.7152707769052802, 'y': 0.7050998139961869}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,173] Trial 317 finished with value: 6.840221562967816 and parameters: {'x': 0.8675280100818279, 'y': 0.4914023839915501}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,185] Trial 318 finished with value: 0.33320880730058355 and parameters: {'x': 0.7030609367319078, 'y': 0.5437957918576}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,198] Trial 319 finished with value: 5.766299367153538 and parameters: {'x': 0.741692675040591, 'y': 0.7888458865815097}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,211] Trial 320 finished with value: 0.178320089097523 and parameters: {'x': 0.6894986197448123, 'y': 0.5040280919043999}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,224] Trial 321 finished with value: 9.279183638619342 and parameters: {'x': 0.8857371636691158, 'y': 0.48012717524222404}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,237] Trial 322 finished with value: 2.228443103918655 and parameters: {'x': 0.715468017761749, 'y': 0.6584374697719519}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,249] Trial 323 finished with value: 8.568879736087252 and parameters: {'x': 0.92323692472458, 'y': 0.5597405972390782}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,262] Trial 324 finished with value: 1.0460809679237566 and parameters: {'x': 0.7522885265531736, 'y': 0.4667049684860242}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,275] Trial 325 finished with value: 0.9978197045441187 and parameters: {'x': 0.6891027985844456, 'y': 0.5697922532741032}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,288] Trial 326 finished with value: 0.1003042785429821 and parameters: {'x': 0.8191988773276174, 'y': 0.6970897297927865}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,300] Trial 327 finished with value: 1.3506274392086246 and parameters: {'x': 0.8473843743995509, 'y': 0.83327034307637}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,313] Trial 328 finished with value: 2.958057460168714 and parameters: {'x': 0.9410240077074478, 'y': 0.7136372860435596}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,326] Trial 329 finished with value: 84.99565294532819 and parameters: {'x': 1.1775411497778796, 'y': 0.4648432552940671}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,339] Trial 330 finished with value: 1.887058875484903 and parameters: {'x': 0.6409722397645683, 'y': 0.5434409604475805}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,352] Trial 331 finished with value: 0.3355949374967503 and parameters: {'x': 0.7920200130011601, 'y': 0.6813641078975312}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,365] Trial 332 finished with value: 25.353419735093375 and parameters: {'x': 1.01120670109449, 'y': 0.5190184451415273}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,378] Trial 333 finished with value: 0.13354277691923117 and parameters: {'x': 0.7198635344273787, 'y': 0.4947372904425954}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,391] Trial 334 finished with value: 0.12086554988416426 and parameters: {'x': 0.7063675894664806, 'y': 0.4803418543427972}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,404] Trial 335 finished with value: 6.181033170193329 and parameters: {'x': 0.8517073697210356, 'y': 0.4772312615391822}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,417] Trial 336 finished with value: 3.749410131512944 and parameters: {'x': 0.6827379113099047, 'y': 0.6571481987126936}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,430] Trial 337 finished with value: 0.08295480269447678 and parameters: {'x': 0.7867715799864448, 'y': 0.599647586424706}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,444] Trial 338 finished with value: 1.5199328479365208 and parameters: {'x': 0.79025219762157, 'y': 0.745986753683252}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,457] Trial 339 finished with value: 2.6541957009371413 and parameters: {'x': 0.849704293275188, 'y': 0.5597751032783134}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,471] Trial 340 finished with value: 1.763406027587102 and parameters: {'x': 0.704839921419057, 'y': 0.6262708003199023}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,484] Trial 341 finished with value: 23.530051057945737 and parameters: {'x': 0.9767636082921527, 'y': 0.46899487219845326}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,498] Trial 342 finished with value: 36.193509332429606 and parameters: {'x': 1.090689569561155, 'y': 0.5880616793147819}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,512] Trial 343 finished with value: 105.19773454245941 and parameters: {'x': 0.7753192462754201, 'y': 1.6265332800638124}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,525] Trial 344 finished with value: 7.701235918563236 and parameters: {'x': 0.6757993457938944, 'y': 0.7323155294105872}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,538] Trial 345 finished with value: 6.143779750530162 and parameters: {'x': 0.8829035857660756, 'y': 0.5319289968113584}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,552] Trial 346 finished with value: 1.5438393676357236 and parameters: {'x': 0.7496714570994465, 'y': 0.44030376084527767}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,564] Trial 347 finished with value: 0.1675995564302747 and parameters: {'x': 0.8192420785504189, 'y': 0.634425290899246}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,578] Trial 348 finished with value: 2.044601480728346 and parameters: {'x': 0.8140011589734467, 'y': 0.8043725640929468}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,591] Trial 349 finished with value: 2.663855904044087 and parameters: {'x': 0.908816003161559, 'y': 0.6629882083069513}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,604] Trial 350 finished with value: 1.5843142827790817 and parameters: {'x': 0.6809806381738899, 'y': 0.5854942669505203}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,617] Trial 351 finished with value: 2.8769890102015103 and parameters: {'x': 0.8189368840375029, 'y': 0.5020099056072567}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,630] Trial 352 finished with value: 15.592508447944931 and parameters: {'x': 0.7255102344024291, 'y': 0.9202834142996743}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,642] Trial 353 finished with value: 4.99930607219489 and parameters: {'x': 0.6599899779059855, 'y': 0.6565777041682411}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,655] Trial 354 finished with value: 0.05899134684830055 and parameters: {'x': 0.7616997876796102, 'y': 0.5848816231603616}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,668] Trial 355 finished with value: 1.8477011954630231 and parameters: {'x': 0.9273000563282909, 'y': 0.7241497722572041}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,681] Trial 356 finished with value: 1.2647766946780026 and parameters: {'x': 0.8278682789068663, 'y': 0.5742287032865074}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,694] Trial 357 finished with value: 0.2792692870894537 and parameters: {'x': 0.7637803838700151, 'y': 0.6306330440023945}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,707] Trial 358 finished with value: 1.5562162232325099 and parameters: {'x': 0.7569595648655363, 'y': 0.45062980107798495}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,721] Trial 359 finished with value: 1.044456844962982 and parameters: {'x': 0.6422670984960622, 'y': 0.5082401974985582}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,734] Trial 360 finished with value: 300.7227265841535 and parameters: {'x': 1.5931192782844423, 'y': 0.8049077577640239}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,746] Trial 361 finished with value: 1.7621467789247751 and parameters: {'x': 0.8626394198400222, 'y': 0.6121134830880686}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,759] Trial 362 finished with value: 3.6718415321185374 and parameters: {'x': 0.7162124371328144, 'y': 0.7024676731981221}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,773] Trial 363 finished with value: 27.77804072729061 and parameters: {'x': 0.9791464070482139, 'y': 0.43168304074635766}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,786] Trial 364 finished with value: 134.12230841149454 and parameters: {'x': 0.8173040498947993, 'y': 1.8259536582356335}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,799] Trial 365 finished with value: 6.354306995108003 and parameters: {'x': 0.8954423598988888, 'y': 0.5499564494460392}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,813] Trial 366 finished with value: 1.5056489481965019 and parameters: {'x': 0.6445458415316382, 'y': 0.5328830002181218}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,826] Trial 367 finished with value: 1.5324720064050423 and parameters: {'x': 0.7435282513930775, 'y': 0.4317271077106822}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,839] Trial 368 finished with value: 5.623754125016495 and parameters: {'x': 0.6480269578370877, 'y': 0.6544569353163634}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,853] Trial 369 finished with value: 2.3847510473321294 and parameters: {'x': 0.7834432732714527, 'y': 0.7666837951164995}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,865] Trial 370 finished with value: 0.9180576169452832 and parameters: {'x': 0.709210987247528, 'y': 0.5942764109360881}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,879] Trial 371 finished with value: 31.18558161828037 and parameters: {'x': 1.0253432007591454, 'y': 0.4928939083942828}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,892] Trial 372 finished with value: 0.11704607887757326 and parameters: {'x': 0.8502301953093366, 'y': 0.6921318199212176}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,906] Trial 373 finished with value: 3.251230140033022 and parameters: {'x': 0.92653829347931, 'y': 0.6783112399973542}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,920] Trial 374 finished with value: 0.39208142071734364 and parameters: {'x': 0.8357237386996477, 'y': 0.7588572367531978}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,933] Trial 375 finished with value: 0.7608563483691915 and parameters: {'x': 0.8895024443314612, 'y': 0.877738967210195}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,946] Trial 376 finished with value: 0.23771489832735246 and parameters: {'x': 0.8013196717541761, 'y': 0.5975889504287555}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,959] Trial 377 finished with value: 0.07660054205336912 and parameters: {'x': 0.7253925055654749, 'y': 0.5296457593581169}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,973] Trial 378 finished with value: 8.133376757161129 and parameters: {'x': 0.6432921670696833, 'y': 0.6967759790085998}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,986] Trial 379 finished with value: 2.43040066335471 and parameters: {'x': 0.8535137770264625, 'y': 0.573278085746252}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:19,999] Trial 380 finished with value: 21.694908504570574 and parameters: {'x': 0.9430686926179218, 'y': 0.4236354180824356}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,013] Trial 381 finished with value: 1.8461620816369064 and parameters: {'x': 0.7241536012036673, 'y': 0.657442447618366}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,026] Trial 382 finished with value: 1.2533602344971857 and parameters: {'x': 0.789419970086103, 'y': 0.5132286125579932}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,039] Trial 383 finished with value: 19.269812065690992 and parameters: {'x': 0.6471099153859794, 'y': 0.8563044468285981}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,053] Trial 384 finished with value: 1.0114927985012212 and parameters: {'x': 0.7166250418261488, 'y': 0.6100497112603435}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,065] Trial 385 finished with value: 0.14799644337651666 and parameters: {'x': 0.8347108268054879, 'y': 0.7314806060037934}. Best is trial 83 with value: 0.05116685616713867.\n",
      "[I 2024-01-31 07:13:20,079] Trial 386 finished with value: 0.013437472247862335 and parameters: {'x': 0.8858068241935203, 'y': 0.7826602633419106}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,092] Trial 387 finished with value: 0.8663992507821283 and parameters: {'x': 0.92895032378063, 'y': 0.7701396961762178}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,105] Trial 388 finished with value: 4.055386564854323 and parameters: {'x': 1.1009781279943855, 'y': 1.0110262616962293}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,119] Trial 389 finished with value: 12.505959430042468 and parameters: {'x': 1.029073144751464, 'y': 0.705365828591948}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,133] Trial 390 finished with value: 0.0261742856038426 and parameters: {'x': 0.8616139940628376, 'y': 0.7339979832871183}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,146] Trial 391 finished with value: 0.21530064279367356 and parameters: {'x': 1.0014505216654672, 'y': 0.9565028737693503}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,159] Trial 392 finished with value: 0.06780872492112111 and parameters: {'x': 0.8787803400896894, 'y': 0.7953014735051025}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,173] Trial 393 finished with value: 0.6581484353547024 and parameters: {'x': 0.8923460686568575, 'y': 0.8766904029206071}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,187] Trial 394 finished with value: 1.0500154456564044 and parameters: {'x': 0.9527472102968281, 'y': 0.8053659933401227}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,200] Trial 395 finished with value: 0.5980275738561082 and parameters: {'x': 0.8631023646807012, 'y': 0.8210565723025571}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,214] Trial 396 finished with value: 66.327128113803 and parameters: {'x': 1.2458121961952284, 'y': 0.7380043900587043}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,227] Trial 397 finished with value: 3.6534513716641754 and parameters: {'x': 0.8412771563770247, 'y': 0.8982271328468252}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,240] Trial 398 finished with value: 13.074316560708342 and parameters: {'x': 1.0497107831071955, 'y': 0.7403426569676673}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,254] Trial 399 finished with value: 0.5250004082389078 and parameters: {'x': 0.936824981888601, 'y': 0.8054600709888365}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,268] Trial 400 finished with value: 173.1746022317274 and parameters: {'x': -1.5453174129898883, 'y': 1.0968980047419281}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,281] Trial 401 finished with value: 0.3370155216529413 and parameters: {'x': 0.8295528372829197, 'y': 0.6326634699087182}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,295] Trial 402 finished with value: 1.2131633544132243 and parameters: {'x': 0.7784478898101365, 'y': 0.7138735643778065}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,308] Trial 403 finished with value: 14.734406526690428 and parameters: {'x': 0.974925891895568, 'y': 0.5666344602449581}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,321] Trial 404 finished with value: 2.1777803353916445 and parameters: {'x': 0.8884779023326366, 'y': 0.6422419327460686}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,334] Trial 405 finished with value: 11.917666245595626 and parameters: {'x': -0.9194813300689473, 'y': 0.5585093786037829}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,348] Trial 406 finished with value: 3.3228130857702167 and parameters: {'x': 0.7686157846226722, 'y': 0.771581574419856}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,363] Trial 407 finished with value: 1.0515291044195438 and parameters: {'x': 0.8869420119963725, 'y': 0.6847471944742671}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,378] Trial 408 finished with value: 3.2942036027190036 and parameters: {'x': 0.8196502615763069, 'y': 0.8524277023108854}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,392] Trial 409 finished with value: 37.460125724007604 and parameters: {'x': 1.1126504985718633, 'y': 0.6260480333986711}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,406] Trial 410 finished with value: 0.07429641848897417 and parameters: {'x': 0.7274271981778271, 'y': 0.5290806009079783}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,420] Trial 411 finished with value: 0.09341583181127451 and parameters: {'x': 0.6979150065624221, 'y': 0.4824372508020026}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,434] Trial 412 finished with value: 0.09153975785509251 and parameters: {'x': 0.7174937961964101, 'y': 0.5256278612650955}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,448] Trial 413 finished with value: 7.368551127536979 and parameters: {'x': 0.9881133232562325, 'y': 0.704919789216832}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,462] Trial 414 finished with value: 0.26387790350946533 and parameters: {'x': 0.7197586826107346, 'y': 0.5611040080376817}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,476] Trial 415 finished with value: 6.63171203428592 and parameters: {'x': 0.8511157634578891, 'y': 0.9814884066553902}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,489] Trial 416 finished with value: 4.66808612528996 and parameters: {'x': -0.700579792662593, 'y': 0.6240829914858785}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,503] Trial 417 finished with value: 1.9709829264645722 and parameters: {'x': 0.7902044596855042, 'y': 0.7632383883344902}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,517] Trial 418 finished with value: 9.829954632320353 and parameters: {'x': 0.903927597526999, 'y': 0.5037047459995689}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,531] Trial 419 finished with value: 590.7394902940368 and parameters: {'x': 0.7308798421371981, 'y': -1.8961789595735237}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,544] Trial 420 finished with value: 0.6528400929448268 and parameters: {'x': 0.8388033373804811, 'y': 0.6244168176863616}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,558] Trial 421 finished with value: 0.31455689431750333 and parameters: {'x': 0.6997710036081664, 'y': 0.44230652808557513}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,572] Trial 422 finished with value: 15.026307357633547 and parameters: {'x': 1.0392209981167098, 'y': 0.6923623128718919}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,585] Trial 423 finished with value: 12.562271788710152 and parameters: {'x': 0.9561789773325949, 'y': 0.5598723746266931}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,599] Trial 424 finished with value: 5.852760888931901 and parameters: {'x': 0.8211415698665773, 'y': 0.9155362090519322}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,613] Trial 425 finished with value: 16.870939251583984 and parameters: {'x': 0.6301494877017122, 'y': 0.8061623292130957}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,626] Trial 426 finished with value: 1.3164193851722628 and parameters: {'x': 0.7088683624300703, 'y': 0.6134746127228192}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,640] Trial 427 finished with value: 189.98256340814243 and parameters: {'x': 0.7753553614305452, 'y': -0.7769826109638065}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,654] Trial 428 finished with value: 4.537169588111334 and parameters: {'x': 0.8391246317530985, 'y': 0.4917322026152332}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,667] Trial 429 finished with value: 1.190063966689739 and parameters: {'x': 0.9303865131091155, 'y': 0.7567513491335478}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,681] Trial 430 finished with value: 1.8147141598765457 and parameters: {'x': 0.751959795935712, 'y': 0.4330354451832752}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,695] Trial 431 finished with value: 9.541460059927557 and parameters: {'x': 0.6214035064723973, 'y': 0.6927059287922599}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,709] Trial 432 finished with value: 0.6054641289471645 and parameters: {'x': 0.7033373301242549, 'y': 0.5666177720676703}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,723] Trial 433 finished with value: 10.141925507117971 and parameters: {'x': 0.9000442773256915, 'y': 0.49177270529802314}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,736] Trial 434 finished with value: 0.213486487319812 and parameters: {'x': 0.7889315832707043, 'y': 0.6635149421597029}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,750] Trial 435 finished with value: 18.267558249917546 and parameters: {'x': 1.0019017708902704, 'y': 0.5764015591909988}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,764] Trial 436 finished with value: 0.1861534352489618 and parameters: {'x': 0.6152338008357008, 'y': 0.39803400444500137}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,778] Trial 437 finished with value: 1.9822776090836962 and parameters: {'x': 0.8553506753977252, 'y': 0.8716731331945013}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,792] Trial 438 finished with value: 253.88302829711506 and parameters: {'x': 1.4513483483539424, 'y': 0.5136806856838012}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,805] Trial 439 finished with value: 3.394485748364531 and parameters: {'x': 0.6882553175038941, 'y': 0.6552801006282143}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,820] Trial 440 finished with value: 343.11296441320803 and parameters: {'x': 0.7778926269792014, 'y': -1.247080762887752}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,834] Trial 441 finished with value: 111.84842108472024 and parameters: {'x': 1.3455678387399947, 'y': 0.7535333956151753}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,848] Trial 442 finished with value: 3.9984964055822667 and parameters: {'x': 0.8977716119665047, 'y': 0.6062929468178431}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,862] Trial 443 finished with value: 0.4206892790508971 and parameters: {'x': 0.6887594328857707, 'y': 0.4174844961191217}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,875] Trial 444 finished with value: 0.861130511418372 and parameters: {'x': 0.7795227072502591, 'y': 0.5175157445956289}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,889] Trial 445 finished with value: 12.952296988143987 and parameters: {'x': 1.0778311054913232, 'y': 0.8019110634575921}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,903] Trial 446 finished with value: 10.28629346128145 and parameters: {'x': 0.6138946618043247, 'y': 0.6952566074093283}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,917] Trial 447 finished with value: 2.1224520373962075 and parameters: {'x': 0.873484565042808, 'y': 0.6178392842479086}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,931] Trial 448 finished with value: 71.96130182025234 and parameters: {'x': 1.1770268345858859, 'y': 0.5372768274147928}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,945] Trial 449 finished with value: 25.662103175211637 and parameters: {'x': 0.9802921636915022, 'y': 0.4543987950022961}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,959] Trial 450 finished with value: 3.4496634985132695 and parameters: {'x': 0.7371023678280773, 'y': 0.7271825758670476}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,973] Trial 451 finished with value: 0.7145799618738811 and parameters: {'x': 0.8124800884205944, 'y': 0.5776971845353279}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:20,986] Trial 452 finished with value: 3.8743093248346314 and parameters: {'x': 0.6720544683396965, 'y': 0.6457386617564345}. Best is trial 386 with value: 0.013437472247862335.\n",
      "[I 2024-01-31 07:13:21,000] Trial 453 finished with value: 0.008824052781396044 and parameters: {'x': 0.9119436475606855, 'y': 0.828369929765194}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,014] Trial 454 finished with value: 0.06863903301167118 and parameters: {'x': 0.9726586347614995, 'y': 0.9200088256944027}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,028] Trial 455 finished with value: 7.308392241917593 and parameters: {'x': 1.1552455237969206, 'y': 1.0646979618985797}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,042] Trial 456 finished with value: 1.4473697092619313 and parameters: {'x': 1.0219051064551876, 'y': 0.9240033110223542}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,055] Trial 457 finished with value: 16.029583989695926 and parameters: {'x': 1.0996059235767741, 'y': 0.8088874795093911}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,069] Trial 458 finished with value: 10.125405530719766 and parameters: {'x': 0.9376202379208719, 'y': 1.1972749848237487}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,083] Trial 459 finished with value: 0.6543387954439435 and parameters: {'x': 1.0502677680161288, 'y': 1.0223275130750455}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,097] Trial 460 finished with value: 2.0915373476306156 and parameters: {'x': 0.9878472569199452, 'y': 0.8312258256998123}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,111] Trial 461 finished with value: 1.6721177306669226 and parameters: {'x': 0.9180396184161242, 'y': 0.9718471275179503}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,126] Trial 462 finished with value: 47.58222211360845 and parameters: {'x': 1.255998940925302, 'y': 0.8882098549057817}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,139] Trial 463 finished with value: 0.7693817716140955 and parameters: {'x': 0.8770593113848596, 'y': 0.8560816026697037}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,154] Trial 464 finished with value: 3.193087863061528 and parameters: {'x': 0.9545336652427179, 'y': 0.7325002359445533}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,168] Trial 465 finished with value: 2.944188680843455 and parameters: {'x': 0.8731997796115404, 'y': 0.9335950776528393}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,181] Trial 466 finished with value: 0.8436464394645465 and parameters: {'x': 0.831118325548443, 'y': 0.781041967565161}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,195] Trial 467 finished with value: 0.559338180598757 and parameters: {'x': 0.7354791467155505, 'y': 0.47097481138588737}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,209] Trial 468 finished with value: 33.37986877440767 and parameters: {'x': 0.9738826924046514, 'y': 1.5261947327486878}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,223] Trial 469 finished with value: 113.22089777253821 and parameters: {'x': 0.615078743914111, 'y': 1.4416784965878324}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,237] Trial 470 finished with value: 6.448669645005757 and parameters: {'x': 0.80670417014966, 'y': 0.39756604222731073}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,251] Trial 471 finished with value: 72.92050744505296 and parameters: {'x': 0.7073401457448278, 'y': 1.353763486652081}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,265] Trial 472 finished with value: 17.923520235275106 and parameters: {'x': 1.0732688495413591, 'y': 0.7286076426882993}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,279] Trial 473 finished with value: 0.040921596299834044 and parameters: {'x': 0.9104529397133658, 'y': 0.8470637175671237}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,293] Trial 474 finished with value: 0.014993523875444057 and parameters: {'x': 0.9273519371731599, 'y': 0.8501247483091234}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,307] Trial 475 finished with value: 3.055541013092939 and parameters: {'x': 1.1495340413756632, 'y': 1.1472682245129038}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,321] Trial 476 finished with value: 0.012837687986425908 and parameters: {'x': 0.9976736965990654, 'y': 0.984024841123823}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,335] Trial 477 finished with value: 0.07444150731004383 and parameters: {'x': 1.0513764239391012, 'y': 1.0785964951351592}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,348] Trial 478 finished with value: 11.914415355889334 and parameters: {'x': 1.1078196544787455, 'y': 0.8822601799475576}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,362] Trial 479 finished with value: 17.10250586020617 and parameters: {'x': 1.1817904466768716, 'y': 0.9834766527975358}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,376] Trial 480 finished with value: 0.9878635919203717 and parameters: {'x': 1.05262777238724, 'y': 1.2072771243865148}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,390] Trial 481 finished with value: 0.037315587010749385 and parameters: {'x': 1.0244199125447695, 'y': 1.0685984261032204}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,404] Trial 482 finished with value: 0.08937717613626237 and parameters: {'x': 1.0494489548094157, 'y': 1.0718588795755535}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,419] Trial 483 finished with value: 13.444296643827327 and parameters: {'x': 1.2028412919614526, 'y': 1.0807240180464728}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,433] Trial 484 finished with value: 1.8561383928107582 and parameters: {'x': 1.070689994552195, 'y': 1.0103204101507168}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,447] Trial 485 finished with value: 1.4473200418449435 and parameters: {'x': 1.1468836301541474, 'y': 1.1959374866203674}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,461] Trial 486 finished with value: 0.18836274349537677 and parameters: {'x': 1.0278930124199728, 'y': 1.0998750970763371}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,476] Trial 487 finished with value: 36.557979623865826 and parameters: {'x': 1.2784231803058694, 'y': 1.0303752638073582}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,491] Trial 488 finished with value: 0.43391713053863046 and parameters: {'x': 1.0224741055509832, 'y': 1.1112873337005023}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,506] Trial 489 finished with value: 3.6078957998667236 and parameters: {'x': 1.1144586721112182, 'y': 1.0524186853192523}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,521] Trial 490 finished with value: 0.036807158266399645 and parameters: {'x': 0.9661310356242117, 'y': 0.9522930471757228}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,536] Trial 491 finished with value: 0.23258179236265303 and parameters: {'x': 1.007534162496075, 'y': 0.9669042395002301}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,550] Trial 492 finished with value: 29.02106710493867 and parameters: {'x': 1.2147612542148962, 'y': 0.9373611068374571}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,564] Trial 493 finished with value: 0.27913351007058085 and parameters: {'x': 1.0937313377894369, 'y': 1.144253244658692}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,579] Trial 494 finished with value: 0.23566135366236585 and parameters: {'x': 0.9774851504602381, 'y': 1.0039699441042556}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,593] Trial 495 finished with value: 0.13407500230270156 and parameters: {'x': 1.036123890693875, 'y': 1.0371150897726544}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,607] Trial 496 finished with value: 0.03301406005899907 and parameters: {'x': 1.1307957089322744, 'y': 1.2660868210902805}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,621] Trial 497 finished with value: 2.898005048533674 and parameters: {'x': 1.18322525226808, 'y': 1.23077562229759}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,635] Trial 498 finished with value: 0.017174726797208665 and parameters: {'x': 1.1197185041444386, 'y': 1.2484382936171483}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,649] Trial 499 finished with value: 0.019707642126566616 and parameters: {'x': 1.111110823729685, 'y': 1.2259870371013382}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,663] Trial 500 finished with value: 1.6378956886795695 and parameters: {'x': 1.2038481215559105, 'y': 1.3229038858151771}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,678] Trial 501 finished with value: 33.02171926783452 and parameters: {'x': 1.3639412347603894, 'y': 1.2868440524857496}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,692] Trial 502 finished with value: 7.384950392358291 and parameters: {'x': 1.2412464191234625, 'y': 1.2700129626718706}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,706] Trial 503 finished with value: 0.7813024892874093 and parameters: {'x': 1.1232765899639208, 'y': 1.1742228520564768}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,721] Trial 504 finished with value: 9.201550743611298 and parameters: {'x': 1.308600990305613, 'y': 1.4106698178704027}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,736] Trial 505 finished with value: 1.4612281856158722 and parameters: {'x': 1.119939580361095, 'y': 1.1339798897817797}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,750] Trial 506 finished with value: 3.7037114299035587 and parameters: {'x': 1.0738840308410913, 'y': 1.3455353249251507}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,765] Trial 507 finished with value: 0.18576601757075345 and parameters: {'x': 1.1567861607726408, 'y': 1.298006479930574}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,779] Trial 508 finished with value: 12.879595205063861 and parameters: {'x': 1.2568287259265003, 'y': 1.2216570767417976}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,794] Trial 509 finished with value: 0.36100958917654097 and parameters: {'x': 1.0635506878119911, 'y': 1.1908871072507998}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,807] Trial 510 finished with value: 0.15852244865983545 and parameters: {'x': 0.9914579786689396, 'y': 0.9431832099905414}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,822] Trial 511 finished with value: 0.6830650876601163 and parameters: {'x': 1.099166673632795, 'y': 1.1261167177743225}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,836] Trial 512 finished with value: 15.150967943122144 and parameters: {'x': 0.9835626359650175, 'y': 1.3566344311724872}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,850] Trial 513 finished with value: 1.2041389843046155 and parameters: {'x': 1.178082563007784, 'y': 1.2795999223417662}. Best is trial 453 with value: 0.008824052781396044.\n",
      "[I 2024-01-31 07:13:21,864] Trial 514 finished with value: 0.008036130294594055 and parameters: {'x': 1.0342799172759196, 'y': 1.0780180732398168}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,878] Trial 515 finished with value: 0.43779112214051813 and parameters: {'x': 1.0443296074140154, 'y': 1.0246072093200749}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,893] Trial 516 finished with value: 4.160686945358666 and parameters: {'x': 1.1200504526284623, 'y': 1.0508889796845533}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,908] Trial 517 finished with value: 0.9490935959495511 and parameters: {'x': 0.9872011628387253, 'y': 1.0719791630531614}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,922] Trial 518 finished with value: 3.2213442991419705 and parameters: {'x': 0.9542070282346341, 'y': 1.0899336628705276}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,936] Trial 519 finished with value: 16.204449822277144 and parameters: {'x': 1.2454708989795056, 'y': 1.1493993796701105}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,951] Trial 520 finished with value: 1.8256737976555897 and parameters: {'x': 1.0361602876237628, 'y': 0.9385590396579646}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,965] Trial 521 finished with value: 0.22372482550360268 and parameters: {'x': 1.1158037026481664, 'y': 1.1991578627259987}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,980] Trial 522 finished with value: 0.2929644489139856 and parameters: {'x': 0.9371967301254847, 'y': 0.9320983097461445}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:21,993] Trial 523 finished with value: 11.000826770823668 and parameters: {'x': 1.0515378224045728, 'y': 1.437366690950539}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,008] Trial 524 finished with value: 9.965327541519466 and parameters: {'x': 1.1815833691456792, 'y': 1.0809828681498592}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,022] Trial 525 finished with value: 11.34131292487168 and parameters: {'x': 0.9535339687246732, 'y': 1.2459636303263084}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,036] Trial 526 finished with value: 46.719367617269704 and parameters: {'x': 1.294849546793883, 'y': 0.9937559205140805}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,050] Trial 527 finished with value: 6.518106821721419 and parameters: {'x': 1.065039751970979, 'y': 0.879086699148234}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,064] Trial 528 finished with value: 5.8081795242031395 and parameters: {'x': 0.9596622406465865, 'y': 1.1619195058793976}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,078] Trial 529 finished with value: 10.82493317505696 and parameters: {'x': 1.1420375768661999, 'y': 0.9755439021447199}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,093] Trial 530 finished with value: 3.075803425455812 and parameters: {'x': 1.0354271221157478, 'y': 0.8967654247066543}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,106] Trial 531 finished with value: 4.196553557593857 and parameters: {'x': 0.923675628204614, 'y': 1.0578893464465864}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,121] Trial 532 finished with value: 1.6220246761598949 and parameters: {'x': 1.200845439348221, 'y': 1.3162646792279875}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,135] Trial 533 finished with value: 1.0889967209254126 and parameters: {'x': 1.0055265727062686, 'y': 1.1154372306639722}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,149] Trial 534 finished with value: 12.572728887505093 and parameters: {'x': 1.1039071951781452, 'y': 0.8641829338422219}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,164] Trial 535 finished with value: 44.62146059306891 and parameters: {'x': 1.389922954848966, 'y': 1.2650318582748128}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,179] Trial 536 finished with value: 1.3193536450292693 and parameters: {'x': 0.9282774964578392, 'y': 0.976338088422465}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,193] Trial 537 finished with value: 1.168700821919033 and parameters: {'x': 1.0292555822097569, 'y': 1.1674339278437857}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,208] Trial 538 finished with value: 2.814095307856826 and parameters: {'x': 0.928879104712653, 'y': 1.0304182147331704}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,223] Trial 539 finished with value: 24.84086182828687 and parameters: {'x': 1.161353905025921, 'y': 0.8505980669341269}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,239] Trial 540 finished with value: 41.855613780284834 and parameters: {'x': 1.237837010109585, 'y': 0.8857186361472054}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,255] Trial 541 finished with value: 1.3752585924263863 and parameters: {'x': 0.9863651251629735, 'y': 1.0901796532894976}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,270] Trial 542 finished with value: 0.8915176747999529 and parameters: {'x': 1.0692245984765376, 'y': 1.2374073521207212}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,285] Trial 543 finished with value: 23.109072956871483 and parameters: {'x': 0.9578197282037757, 'y': 1.3981190975312474}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,300] Trial 544 finished with value: 2.1656224185108117 and parameters: {'x': 0.8969818924656817, 'y': 0.9513760279456175}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,314] Trial 545 finished with value: 21.16291427073562 and parameters: {'x': 1.1357352214583785, 'y': 0.8300631087255275}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,329] Trial 546 finished with value: 2131.892566792382 and parameters: {'x': 1.7630927042001086, 'y': -1.50811570534032}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,343] Trial 547 finished with value: 1.1606808997899896 and parameters: {'x': 1.058295606856392, 'y': 1.0124125250688467}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,358] Trial 548 finished with value: 9.249591345417755 and parameters: {'x': 0.938883038156861, 'y': 1.185571352307264}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,373] Trial 549 finished with value: 0.796193010811741 and parameters: {'x': 1.0127864952642833, 'y': 1.1149569712218865}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,388] Trial 550 finished with value: 1.3731760143825151 and parameters: {'x': 0.9014422081794657, 'y': 0.9293654463708507}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,404] Trial 551 finished with value: 13.169931261658782 and parameters: {'x': 1.3000500587735435, 'y': 1.3284686945020276}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,419] Trial 552 finished with value: 313.1616555810128 and parameters: {'x': 1.120195628380155, 'y': -0.5147583426264775}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,434] Trial 553 finished with value: 0.16087700054316273 and parameters: {'x': 0.8920055455715405, 'y': 0.8343021470258133}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,449] Trial 554 finished with value: 20.403096827624324 and parameters: {'x': 1.2217131269271273, 'y': 1.0414295524944406}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,463] Trial 555 finished with value: 0.9939482658486568 and parameters: {'x': 1.0055293056059382, 'y': 0.9113937636434308}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,478] Trial 556 finished with value: 0.1295925573491864 and parameters: {'x': 1.0811259754776246, 'y': 1.1337604317977215}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,492] Trial 557 finished with value: 0.2541259604812274 and parameters: {'x': 0.8863104102157987, 'y': 0.8346583248064812}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,508] Trial 558 finished with value: 22.629439011838258 and parameters: {'x': 0.9924278873396425, 'y': 1.4606166082451193}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,522] Trial 559 finished with value: 11.138942916987194 and parameters: {'x': 1.1424370780628916, 'y': 0.9717160109281472}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,537] Trial 560 finished with value: 16.908147833189197 and parameters: {'x': 0.9083677496160846, 'y': 1.2362250402900807}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,552] Trial 561 finished with value: 602.9277623748806 and parameters: {'x': 1.8757731375162985, 'y': 1.0646284031293713}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,566] Trial 562 finished with value: 5.094574039049556 and parameters: {'x': 1.0133154813314222, 'y': 1.2525159680969875}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,582] Trial 563 finished with value: 15.173827190971183 and parameters: {'x': 1.0927364230820078, 'y': 0.8046473243125594}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,597] Trial 564 finished with value: 9.981723925760823 and parameters: {'x': 1.2147102520138615, 'y': 1.160312756711605}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,612] Trial 565 finished with value: 0.12118661215278777 and parameters: {'x': 0.9516450944806375, 'y': 0.9401027843594055}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,626] Trial 566 finished with value: 104.73371956172222 and parameters: {'x': -1.4182902078019022, 'y': 1.0171347643043993}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,641] Trial 567 finished with value: 0.8533257304477989 and parameters: {'x': 0.8552336954849237, 'y': 0.8226589025312245}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,655] Trial 568 finished with value: 8.190717875919734 and parameters: {'x': 1.044150091661197, 'y': 1.3764096600252556}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,670] Trial 569 finished with value: 0.6573352399414752 and parameters: {'x': 0.9012136768398005, 'y': 0.8926582295013641}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,685] Trial 570 finished with value: 3.982514019056084 and parameters: {'x': 1.1403734065844637, 'y': 1.101383444572238}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,700] Trial 571 finished with value: 0.06457767548053577 and parameters: {'x': 0.9878779181801507, 'y': 1.0012859903656617}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,715] Trial 572 finished with value: 0.015365641425421517 and parameters: {'x': 0.9478881599267237, 'y': 0.9097391845191358}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,731] Trial 573 finished with value: 0.18700401948665404 and parameters: {'x': 0.9484959486348133, 'y': 0.9425807209229817}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,745] Trial 574 finished with value: 2.1473677122850976 and parameters: {'x': 1.0025043492818115, 'y': 0.8584761890961371}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,760] Trial 575 finished with value: 193.83416039313957 and parameters: {'x': 1.5240034819690589, 'y': 0.9313296966504365}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,775] Trial 576 finished with value: 5.323213365800134 and parameters: {'x': 0.8694613937440998, 'y': 0.9863144346796939}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,790] Trial 577 finished with value: 1.8730600506632993 and parameters: {'x': 0.9677231901962503, 'y': 0.7996664546036959}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,805] Trial 578 finished with value: 18.797209199716704 and parameters: {'x': 1.081971910290897, 'y': 1.6041432004207357}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,821] Trial 579 finished with value: 0.2194007012253066 and parameters: {'x': 0.921189420878176, 'y': 0.8947624077949291}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,837] Trial 580 finished with value: 0.16597007201288777 and parameters: {'x': 0.8618646578989886, 'y': 0.7811367500699575}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,852] Trial 581 finished with value: 0.058855180561767396 and parameters: {'x': 1.0223684957998793, 'y': 1.0210805966060834}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,867] Trial 582 finished with value: 16.805484016616475 and parameters: {'x': 1.1964205227441091, 'y': 1.0219479777716651}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,882] Trial 583 finished with value: 40.46872714963928 and parameters: {'x': 1.3335973648835635, 'y': 1.1432068734977332}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,897] Trial 584 finished with value: 0.2308555279816807 and parameters: {'x': 1.0536437162245949, 'y': 1.0624180515449435}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,912] Trial 585 finished with value: 9.143408382691417 and parameters: {'x': 1.1356007640750425, 'y': 0.9875126014879807}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,928] Trial 586 finished with value: 2.7338175093315167 and parameters: {'x': 1.0102872680931005, 'y': 1.1860197629282407}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,942] Trial 587 finished with value: 12.222888609388686 and parameters: {'x': 0.9777501132559353, 'y': 1.305600683700439}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,957] Trial 588 finished with value: 50.22118266886115 and parameters: {'x': 1.264251841076876, 'y': 0.8901565133685784}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,973] Trial 589 finished with value: 2.596136299092502 and parameters: {'x': 1.0688540315999866, 'y': 0.9814708236768869}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:22,988] Trial 590 finished with value: 4.753640233708729 and parameters: {'x': 1.1462025988587667, 'y': 1.0962426989713545}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:23,004] Trial 591 finished with value: 33.33291633362045 and parameters: {'x': -0.5514221620747497, 'y': 0.8601779508658843}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:23,019] Trial 592 finished with value: 13.162751046511053 and parameters: {'x': 0.932982627501558, 'y': 1.2331997391619625}. Best is trial 514 with value: 0.008036130294594055.\n",
      "[I 2024-01-31 07:13:23,035] Trial 593 finished with value: 0.0011202322795740847 and parameters: {'x': 0.9917703579551643, 'y': 0.9868526766822685}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,050] Trial 594 finished with value: 510.1106592006111 and parameters: {'x': -1.8188765617403022, 'y': 1.067409016932958}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,067] Trial 595 finished with value: 3.7435962376174334 and parameters: {'x': 1.0823568397776646, 'y': 0.9781879328165369}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,083] Trial 596 finished with value: 0.5589518898388649 and parameters: {'x': 0.9974753248155125, 'y': 0.9201943647643456}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,098] Trial 597 finished with value: 0.4751243320870397 and parameters: {'x': 1.1113288386687101, 'y': 1.1670275109779202}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,112] Trial 598 finished with value: 14.804362676961777 and parameters: {'x': 1.183604497394046, 'y': 1.0165935459133388}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,128] Trial 599 finished with value: 0.5386251194159682 and parameters: {'x': 0.9991550679038329, 'y': 0.9249198144679779}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,143] Trial 600 finished with value: 0.6003035167674081 and parameters: {'x': 0.9541253379249723, 'y': 0.8330118328883616}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,159] Trial 601 finished with value: 0.7389953840763696 and parameters: {'x': 1.0512911104574028, 'y': 1.0194013093929655}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,174] Trial 602 finished with value: 10.348801153902201 and parameters: {'x': 0.8976250831942617, 'y': 1.1272633782238575}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,190] Trial 603 finished with value: 3.415142118390593 and parameters: {'x': 1.233355953270234, 'y': 1.3378451379615959}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,205] Trial 604 finished with value: 17.252634154759345 and parameters: {'x': 1.0343582729887155, 'y': 1.48524572973354}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,221] Trial 605 finished with value: 17.779588722156436 and parameters: {'x': 1.1660197228100504, 'y': 0.9382704597504372}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,235] Trial 606 finished with value: 11.489138001766499 and parameters: {'x': 0.94102875914544, 'y': 1.2244401327977519}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,250] Trial 607 finished with value: 4.243842718507805 and parameters: {'x': 1.1215907465892188, 'y': 1.0523190569606795}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,266] Trial 608 finished with value: 0.3018141383451003 and parameters: {'x': 0.8769385650820197, 'y': 0.8225628215763259}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,281] Trial 609 finished with value: 0.9010937096251445 and parameters: {'x': 1.0303751564486472, 'y': 0.9667956180228345}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,297] Trial 610 finished with value: 5.877557960974828 and parameters: {'x': 0.930535489688025, 'y': 1.1082335139180097}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,313] Trial 611 finished with value: 10.085973839633935 and parameters: {'x': 1.0914405477812552, 'y': 0.8737899143685687}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,329] Trial 612 finished with value: 138.74974284770093 and parameters: {'x': 1.4057257528721379, 'y': 0.7988430378307712}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,345] Trial 613 finished with value: 0.3319218137821031 and parameters: {'x': 0.9793797175883007, 'y': 1.0167603741829718}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,360] Trial 614 finished with value: 20.781694602774714 and parameters: {'x': 1.2810114337979834, 'y': 1.1859877970791182}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,377] Trial 615 finished with value: 35.43743366591375 and parameters: {'x': 0.8379767243509715, 'y': 1.2972779451071355}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,393] Trial 616 finished with value: 1.657227088009393 and parameters: {'x': 0.8778698183322267, 'y': 0.8988081122815845}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,409] Trial 617 finished with value: 13.288938264437643 and parameters: {'x': 1.0175069803753127, 'y': 1.3998562115215913}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,425] Trial 618 finished with value: 7.140346128242244 and parameters: {'x': 1.1659701778597489, 'y': 1.0927881236743195}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,440] Trial 619 finished with value: 2.4445718312205584 and parameters: {'x': 0.9677880607644983, 'y': 0.7802956502799461}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,457] Trial 620 finished with value: 4.388633202341155 and parameters: {'x': 1.0821160033865775, 'y': 0.9616453965190471}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,474] Trial 621 finished with value: 0.13119446047911104 and parameters: {'x': 0.9151055120723393, 'y': 0.872629940891902}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,490] Trial 622 finished with value: 3.9343221160543926 and parameters: {'x': 1.0281432100871644, 'y': 1.2554097508636535}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,506] Trial 623 finished with value: 20.859392973244987 and parameters: {'x': 1.2204900858551115, 'y': 1.033407743208422}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,521] Trial 624 finished with value: 16.364458168479747 and parameters: {'x': 0.8605388200580095, 'y': 1.1448166692484296}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,537] Trial 625 finished with value: 19.969070353170817 and parameters: {'x': 1.0964473015598732, 'y': 0.7554331204332088}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,553] Trial 626 finished with value: 0.07459673480797802 and parameters: {'x': 0.9664468804235566, 'y': 0.9611250929622169}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,568] Trial 627 finished with value: 0.007159246352736781 and parameters: {'x': 0.980109057830506, 'y': 0.9688378741732844}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,584] Trial 628 finished with value: 0.20494415690797974 and parameters: {'x': 1.0087863309336393, 'y': 0.9723876302811524}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,600] Trial 629 finished with value: 14.597646400374362 and parameters: {'x': 1.1357897643554256, 'y': 0.9081911040342396}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,616] Trial 630 finished with value: 0.7666868315106917 and parameters: {'x': 0.9744383130637673, 'y': 1.0370533618074473}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,632] Trial 631 finished with value: 3.150444799483434 and parameters: {'x': 1.0673905801626042, 'y': 0.9619557058643968}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,647] Trial 632 finished with value: 1.8894314743272567 and parameters: {'x': 0.975566496877208, 'y': 1.0891648645246084}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,663] Trial 633 finished with value: 11.608581714935262 and parameters: {'x': 1.165884240896139, 'y': 1.0189764374824524}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,679] Trial 634 finished with value: 0.005064987905258006 and parameters: {'x': 0.9292149035564288, 'y': 0.8627023801719668}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,694] Trial 635 finished with value: 0.010819252362212213 and parameters: {'x': 0.9196677822873869, 'y': 0.8523963914756258}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,710] Trial 636 finished with value: 0.11353788572910711 and parameters: {'x': 0.8928682223242078, 'y': 0.829160597804456}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,726] Trial 637 finished with value: 0.7130719171852578 and parameters: {'x': 0.8827461687103715, 'y': 0.8628663600291395}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,742] Trial 638 finished with value: 0.09892272854275896 and parameters: {'x': 0.9203890534817397, 'y': 0.8166882550478333}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,758] Trial 639 finished with value: 1.3324310408851767 and parameters: {'x': 0.8617185525018848, 'y': 0.8571585706262234}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,775] Trial 640 finished with value: 1.8857893977235722 and parameters: {'x': 0.9502296947465354, 'y': 0.7657026462469303}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,791] Trial 641 finished with value: 3.834407712754822 and parameters: {'x': 1.0452863239397008, 'y': 0.896859435366836}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,807] Trial 642 finished with value: 3.5080502879362574 and parameters: {'x': 0.8490353437569755, 'y': 0.9075495245173173}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,823] Trial 643 finished with value: 3.455694410644332 and parameters: {'x': 0.9929562865744499, 'y': 0.8000685401217029}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,839] Trial 644 finished with value: 19.63812616735817 and parameters: {'x': 1.0968039945334551, 'y': 0.7599354926650557}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,855] Trial 645 finished with value: 0.03324298322141993 and parameters: {'x': 0.9460175770159962, 'y': 0.9123644451494348}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,872] Trial 646 finished with value: 0.009040115205338716 and parameters: {'x': 0.9517696116981479, 'y': 0.9140592602688906}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,888] Trial 647 finished with value: 0.6364522316065888 and parameters: {'x': 1.0013900365641721, 'y': 0.9230041700995907}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,904] Trial 648 finished with value: 74.10139198100369 and parameters: {'x': -0.34443583327573435, 'y': 0.968894146829419}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,920] Trial 649 finished with value: 0.04166702077042351 and parameters: {'x': 0.9416262041677484, 'y': 0.8670998672888619}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,936] Trial 650 finished with value: 0.00650363733432613 and parameters: {'x': 0.9244796323773166, 'y': 0.8518336131117275}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,952] Trial 651 finished with value: 9.12016645042067 and parameters: {'x': 1.0780429348735925, 'y': 0.860281293702066}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,968] Trial 652 finished with value: 1.6349586495848183 and parameters: {'x': 0.9365997161131506, 'y': 1.0049272524528385}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:23,984] Trial 653 finished with value: 3.796891166286951 and parameters: {'x': 1.0332572940618212, 'y': 0.8727928882554068}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,000] Trial 654 finished with value: 12.641111491036574 and parameters: {'x': 1.1568224686295556, 'y': 0.9830408393022281}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,016] Trial 655 finished with value: 0.0996553114971922 and parameters: {'x': 0.934765854306133, 'y': 0.9046740632940266}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,031] Trial 656 finished with value: 46.02731797089521 and parameters: {'x': 1.2257632232589522, 'y': 0.8244368615420187}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,047] Trial 657 finished with value: 1.557136747119666 and parameters: {'x': 1.0837599947806729, 'y': 1.0500318705075942}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,062] Trial 658 finished with value: 0.19258648643701748 and parameters: {'x': 0.9976478669215348, 'y': 0.9514172198519438}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,079] Trial 659 finished with value: 40.17418095940028 and parameters: {'x': 1.3451264820556832, 'y': 1.1764745143627773}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,095] Trial 660 finished with value: 0.2183862367750515 and parameters: {'x': 0.9046128088010144, 'y': 0.8640722868644787}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,111] Trial 661 finished with value: 4.266115587228941 and parameters: {'x': 1.117317230317934, 'y': 1.0421854692198613}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,127] Trial 662 finished with value: 4.933685889197237 and parameters: {'x': -1.0163370023838976, 'y': 0.9397705754851994}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,143] Trial 663 finished with value: 2.740993917786117 and parameters: {'x': 0.974150124664884, 'y': 0.7834291739239025}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,160] Trial 664 finished with value: 306.9129888047396 and parameters: {'x': 1.0511650185962151, 'y': -0.6469378617860418}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,176] Trial 665 finished with value: 7.4731004581133105 and parameters: {'x': 0.9191472651779252, 'y': 1.1180818239657324}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,193] Trial 666 finished with value: 45.77836367886785 and parameters: {'x': 1.2392382198244878, 'y': 0.8595373586305339}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,210] Trial 667 finished with value: 0.5803142669256737 and parameters: {'x': 1.0291100380229365, 'y': 0.9829447488756413}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,226] Trial 668 finished with value: 0.2602664499925573 and parameters: {'x': 1.1501063218639702, 'y': 1.3715025808921943}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,242] Trial 669 finished with value: 28.337477331050383 and parameters: {'x': 0.8471967882361139, 'y': 1.2498526169327715}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,258] Trial 670 finished with value: 0.0024023798445435315 and parameters: {'x': 0.9510293099000567, 'y': 0.9042505601302772}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,275] Trial 671 finished with value: 0.4975259241344491 and parameters: {'x': 0.9314275177052589, 'y': 0.7973558148547673}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,291] Trial 672 finished with value: 3.281207187879406 and parameters: {'x': 0.8416855607031183, 'y': 0.888882462723164}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,307] Trial 673 finished with value: 11.742229352034387 and parameters: {'x': 1.0822532357775165, 'y': 0.8287014348420361}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,323] Trial 674 finished with value: 1.0937473806904103 and parameters: {'x': 0.9216059682454097, 'y': 0.7450694138970331}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,339] Trial 675 finished with value: 2.827780161456208 and parameters: {'x': 1.0394747262402568, 'y': 0.9123939977280854}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,355] Trial 676 finished with value: 13.802007347043334 and parameters: {'x': 0.8436077689481095, 'y': 1.082855274480886}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,371] Trial 677 finished with value: 2.313491709686636 and parameters: {'x': 0.9616741078274147, 'y': 0.7727637162839427}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,387] Trial 678 finished with value: 17.540423089909304 and parameters: {'x': 1.1601692623023223, 'y': 0.9274862190656635}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,403] Trial 679 finished with value: 1.9561818052853328 and parameters: {'x': 1.0213189252814294, 'y': 1.1829396677669834}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,419] Trial 680 finished with value: 0.3256364102020377 and parameters: {'x': 0.8998312113491927, 'y': 0.8658747300790288}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,435] Trial 681 finished with value: 40.96206241344998 and parameters: {'x': 1.2882790563948987, 'y': 1.0202963843934996}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,451] Trial 682 finished with value: 6.679726845312864 and parameters: {'x': 1.104812218985004, 'y': 0.9623709785777681}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,467] Trial 683 finished with value: 71.74039690080508 and parameters: {'x': 0.8290228918800335, 'y': 1.5341033982793075}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,483] Trial 684 finished with value: 11.623749627139793 and parameters: {'x': 0.98216786884802, 'y': 1.3055852647322186}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,499] Trial 685 finished with value: 38.764480161574305 and parameters: {'x': 1.1876538850722675, 'y': 0.7881933311528948}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,515] Trial 686 finished with value: 5.893198568482318 and parameters: {'x': 0.932434685682496, 'y': 1.1120995105289015}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,530] Trial 687 finished with value: 7.159154987621708 and parameters: {'x': 1.0518732747317292, 'y': 0.8389217014894381}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,546] Trial 688 finished with value: 2.711208054466907 and parameters: {'x': 0.8742932880483686, 'y': 0.9285656665512868}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,562] Trial 689 finished with value: 3.1738011976058162 and parameters: {'x': 1.1006278817063708, 'y': 1.0335145018462304}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,579] Trial 690 finished with value: 408.0963120655445 and parameters: {'x': 0.9943504763430553, 'y': -1.031406432691162}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,595] Trial 691 finished with value: 0.23421538401137917 and parameters: {'x': 0.8378537399373136, 'y': 0.7475975709068983}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,612] Trial 692 finished with value: 10.169707479806945 and parameters: {'x': 0.9213018524076813, 'y': 1.1675997707633274}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,628] Trial 693 finished with value: 36.149368457208865 and parameters: {'x': 1.2184275114122398, 'y': 0.8837190487789383}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,644] Trial 694 finished with value: 0.16876986263350163 and parameters: {'x': 1.0141216788939502, 'y': 0.9873854488542396}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,660] Trial 695 finished with value: 21.010132301884685 and parameters: {'x': 1.1351433667885253, 'y': 0.8303816234084052}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,676] Trial 696 finished with value: 14.045027279349638 and parameters: {'x': 0.9452181113449712, 'y': 1.2681641949023732}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,692] Trial 697 finished with value: 0.44339525264543406 and parameters: {'x': 1.0697877516253826, 'y': 1.0782246192252862}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,708] Trial 698 finished with value: 6.386725121506736 and parameters: {'x': 0.8242933623757438, 'y': 0.9315677045821222}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,724] Trial 699 finished with value: 0.28682354746796124 and parameters: {'x': 0.9060846650112448, 'y': 0.7682633870044651}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,740] Trial 700 finished with value: 0.04351227947576624 and parameters: {'x': 1.0104735589679534, 'y': 1.0418901002091714}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,756] Trial 701 finished with value: 31.880629063152963 and parameters: {'x': 1.2993989277448166, 'y': 1.1246025856378972}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,773] Trial 702 finished with value: 0.5422301232936054 and parameters: {'x': 1.1299190216573345, 'y': 1.2042358827146176}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,789] Trial 703 finished with value: 0.7121241114439222 and parameters: {'x': 1.0794206924963008, 'y': 1.0811361475530026}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,806] Trial 704 finished with value: 17.94558253782703 and parameters: {'x': 1.200339421779845, 'y': 1.0176664478662008}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,822] Trial 705 finished with value: 13.401624613951796 and parameters: {'x': 1.0057194393936586, 'y': 1.3775534382068713}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,838] Trial 706 finished with value: 0.7455844441949091 and parameters: {'x': 1.0425590305241788, 'y': 1.0006870470745468}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,853] Trial 707 finished with value: 3.3761841421385106 and parameters: {'x': 0.9873951296572407, 'y': 1.1586887746867274}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,870] Trial 708 finished with value: 17.420387268791277 and parameters: {'x': 1.163503798249966, 'y': 0.9366840927159983}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,887] Trial 709 finished with value: 0.695004252728962 and parameters: {'x': 1.0785160838036676, 'y': 1.080200589048275}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,903] Trial 710 finished with value: 0.7907406362530846 and parameters: {'x': 0.9608280159290961, 'y': 1.0120277539506577}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,920] Trial 711 finished with value: 20.609714002035197 and parameters: {'x': 0.9018601051066989, 'y': 1.2672247874394023}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,937] Trial 712 finished with value: 3.1283727020356924 and parameters: {'x': 1.0222263620471008, 'y': 0.8680886370365211}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,954] Trial 713 finished with value: 0.8179041503849485 and parameters: {'x': 1.125886032974518, 'y': 1.178061732152199}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,972] Trial 714 finished with value: 35.64729194328756 and parameters: {'x': 1.2409232448161887, 'y': 0.9433232532452678}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:24,989] Trial 715 finished with value: 2.6025129664464752 and parameters: {'x': 0.9529978404664584, 'y': 1.0694594579291912}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,007] Trial 716 finished with value: 217.93101921380077 and parameters: {'x': 1.0556548616418988, 'y': -0.3618310111824177}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,025] Trial 717 finished with value: 1.6057235928226368 and parameters: {'x': 0.8684507063187025, 'y': 0.880239100365676}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,042] Trial 718 finished with value: 0.048320021766924376 and parameters: {'x': 0.9933160212130784, 'y': 0.9647050666736348}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,058] Trial 719 finished with value: 57.69917703034533 and parameters: {'x': 0.8252000909104658, 'y': 1.4403537811759146}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,075] Trial 720 finished with value: 0.009961553466886196 and parameters: {'x': 0.9638523220506345, 'y': 0.938314469604285}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,092] Trial 721 finished with value: 1.0534315544843458 and parameters: {'x': 0.9143900182867959, 'y': 0.9383882571578689}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,109] Trial 722 finished with value: 2.644645259724757 and parameters: {'x': 0.8295908604059469, 'y': 0.8499493476939352}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,125] Trial 723 finished with value: 0.21207711537338522 and parameters: {'x': 0.9537643189772492, 'y': 0.9554855181336349}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,142] Trial 724 finished with value: 105.16202082339623 and parameters: {'x': 0.8968499615323051, 'y': 1.8297733271737546}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,159] Trial 725 finished with value: 1.3527665372407978 and parameters: {'x': 0.9772654942406701, 'y': 0.838761575354708}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,176] Trial 726 finished with value: 2.291964210072143 and parameters: {'x': 0.8653656582237992, 'y': 0.8996502223820998}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,193] Trial 727 finished with value: 0.2015142640199048 and parameters: {'x': 0.9745226396486096, 'y': 0.9945123590925317}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,211] Trial 728 finished with value: 435.02725261644497 and parameters: {'x': 1.6882990699902751, 'y': 0.7657590720847658}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,230] Trial 729 finished with value: 2.727651168230642 and parameters: {'x': 1.0329366929942998, 'y': 0.901835035211586}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,248] Trial 730 finished with value: 12.585273837987085 and parameters: {'x': 0.8248496648384067, 'y': 1.0347016257692863}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,264] Trial 731 finished with value: 0.010077137578182955 and parameters: {'x': 0.9004188295605134, 'y': 0.8120218543822306}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,282] Trial 732 finished with value: 0.33813308962366057 and parameters: {'x': 0.9011112758558624, 'y': 0.7546993404291406}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,299] Trial 733 finished with value: 1.2263938708789737 and parameters: {'x': 0.83482467406982, 'y': 0.8064361625107321}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,315] Trial 734 finished with value: 1.3502886493508264 and parameters: {'x': 0.9165267569753082, 'y': 0.7241195763744551}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,332] Trial 735 finished with value: 65.25343219124201 and parameters: {'x': -0.19330770597444435, 'y': 0.8363012137811529}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,348] Trial 736 finished with value: 4.154177508036628 and parameters: {'x': 0.7909389250915072, 'y': 0.8283273479609197}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,365] Trial 737 finished with value: 0.2382339968039493 and parameters: {'x': 0.917749050701698, 'y': 0.890374522308613}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,383] Trial 738 finished with value: 22.18643756936663 and parameters: {'x': 1.0945841284298385, 'y': 0.7271845742978553}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,400] Trial 739 finished with value: 7.022749140454617 and parameters: {'x': 0.8159091469479377, 'y': 0.9300722514356644}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,417] Trial 740 finished with value: 1.6049501924133853 and parameters: {'x': 0.9547240948002819, 'y': 0.7848923988677847}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,433] Trial 741 finished with value: 27.57417541851588 and parameters: {'x': -0.7688999461322974, 'y': 1.0856276809329142}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,450] Trial 742 finished with value: 4.02746905819364 and parameters: {'x': 1.0297969493888002, 'y': 0.8598183273177203}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,465] Trial 743 finished with value: 4.675872339288631 and parameters: {'x': 0.8635752888325923, 'y': 0.9615691523448506}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,481] Trial 744 finished with value: 4.232614557277581 and parameters: {'x': 1.1049720184313387, 'y': 1.0154979471215748}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,497] Trial 745 finished with value: 7.005291453476541 and parameters: {'x': 0.9915283369255409, 'y': 0.7184546874385342}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,514] Trial 746 finished with value: 0.021384497346509237 and parameters: {'x': 0.9109169367936816, 'y': 0.818172810475795}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,531] Trial 747 finished with value: 1.1607686428626711 and parameters: {'x': 0.8179493803436777, 'y': 0.7752309347665126}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,547] Trial 748 finished with value: 0.16206405052147785 and parameters: {'x': 0.8901746388828823, 'y': 0.8311410362481948}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,563] Trial 749 finished with value: 0.4120790789005029 and parameters: {'x': 0.8077695448282548, 'y': 0.7137392114635869}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,580] Trial 750 finished with value: 0.05616426437799178 and parameters: {'x': 0.920609892926117, 'y': 0.8251928917320406}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,597] Trial 751 finished with value: 1.1466534895180196 and parameters: {'x': 0.8867874915284305, 'y': 0.8928738102559832}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,614] Trial 752 finished with value: 0.9086019551553755 and parameters: {'x': 0.9507314133703997, 'y': 0.8086970188511601}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,630] Trial 753 finished with value: 0.2498164836582616 and parameters: {'x': 0.8157179386787861, 'y': 0.7118561262873307}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,647] Trial 754 finished with value: 2.764597142530134 and parameters: {'x': 1.045091838076685, 'y': 0.9260073272885918}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,663] Trial 755 finished with value: 0.10158878159054531 and parameters: {'x': 0.9533482938451494, 'y': 0.8773432373926486}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,680] Trial 756 finished with value: 1.4197171975332252 and parameters: {'x': 0.8006900875043956, 'y': 0.758577709160358}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,696] Trial 757 finished with value: 9.96859272174076 and parameters: {'x': 1.1237033943291153, 'y': 0.9472209637768416}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,713] Trial 758 finished with value: 0.42541413217090196 and parameters: {'x': 0.8816123568796992, 'y': 0.8413807015693335}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,730] Trial 759 finished with value: 3.3270613147441117 and parameters: {'x': 1.040016039141116, 'y': 0.8992749225637361}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,747] Trial 760 finished with value: 2.928676190722606 and parameters: {'x': 0.9642242086356866, 'y': 0.758631969108638}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,763] Trial 761 finished with value: 3.7959386979526872 and parameters: {'x': 0.8910422800165596, 'y': 0.9884831273343881}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,780] Trial 762 finished with value: 25.557107227885428 and parameters: {'x': 1.1633274428486506, 'y': 0.8480542667046894}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,797] Trial 763 finished with value: 554.8429230018603 and parameters: {'x': -1.2381404557088813, 'y': -0.811861406280691}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,814] Trial 764 finished with value: 0.207035563712282 and parameters: {'x': 1.0708216944873725, 'y': 1.1017124829021878}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,832] Trial 765 finished with value: 10.593522452051225 and parameters: {'x': 0.8026954025145138, 'y': 0.9691982456434723}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,849] Trial 766 finished with value: 2.0353063251805965 and parameters: {'x': 0.9737965441714449, 'y': 0.8056396132511605}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,865] Trial 767 finished with value: 1.4200659855030395 and parameters: {'x': 0.8883419421818615, 'y': 0.9077936610910468}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,883] Trial 768 finished with value: 10.545189957449216 and parameters: {'x': 1.015050210281145, 'y': 0.7055968345650356}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,900] Trial 769 finished with value: 9.310761667232264 and parameters: {'x': 1.1406263965315127, 'y': 0.9962173909113277}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,920] Trial 770 finished with value: 680.1141867332958 and parameters: {'x': 0.948884296976356, 'y': -1.7075134774802834}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,937] Trial 771 finished with value: 7.069224983781111 and parameters: {'x': 1.0731237251263905, 'y': 0.8858149609028009}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,955] Trial 772 finished with value: 11.38194817605649 and parameters: {'x': 0.8700133657887726, 'y': 1.0940441793329103}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,974] Trial 773 finished with value: 2.38482725002884 and parameters: {'x': 0.7912193066604497, 'y': 0.7790390324063193}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:25,993] Trial 774 finished with value: 4.040301966317167 and parameters: {'x': 0.9901193182971144, 'y': 1.1813388599321228}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,011] Trial 775 finished with value: 17.8127393584757 and parameters: {'x': 1.198283190659629, 'y': 1.0142972265378352}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,028] Trial 776 finished with value: 5.664741941628493 and parameters: {'x': 1.0797385371201, 'y': 0.9279617348244815}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,045] Trial 777 finished with value: 643.9588411707405 and parameters: {'x': -1.7980972988240138, 'y': 0.7109931305891641}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,061] Trial 778 finished with value: 1.068249932019268 and parameters: {'x': 0.9433098642582186, 'y': 0.7866329108356314}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,078] Trial 779 finished with value: 1.4591954465925954 and parameters: {'x': 0.8706651094913664, 'y': 0.8781605193932422}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,095] Trial 780 finished with value: 0.4270635957816202 and parameters: {'x': 1.0254108519572362, 'y': 1.11676809558258}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,112] Trial 781 finished with value: 14.56629146192695 and parameters: {'x': 0.8062105178290583, 'y': 1.031141204168348}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,128] Trial 782 finished with value: 0.011033953979711242 and parameters: {'x': 0.9203856878712112, 'y': 0.8402574314337101}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,145] Trial 783 finished with value: 0.1175150969903546 and parameters: {'x': 0.8800801042498776, 'y': 0.8066555217463459}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,163] Trial 784 finished with value: 189.77009577525794 and parameters: {'x': 1.4481932527177208, 'y': 0.7204223171610409}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,180] Trial 785 finished with value: 12.291530781297553 and parameters: {'x': 0.7846932865693663, 'y': 0.9656746014870983}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,197] Trial 786 finished with value: 0.03099503263041003 and parameters: {'x': 1.10003051543405, 'y': 1.2245546911370393}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,215] Trial 787 finished with value: 9.052964336618432 and parameters: {'x': 1.2717590768174858, 'y': 1.317719496030114}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,232] Trial 788 finished with value: 3.9676504736440097 and parameters: {'x': 1.1877534482740366, 'y': 1.212455477398354}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,249] Trial 789 finished with value: 0.021176484735393915 and parameters: {'x': 1.1245723245551416, 'y': 1.2721850276787598}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,267] Trial 790 finished with value: 0.05728042412509987 and parameters: {'x': 1.1888613189469577, 'y': 1.398690274382754}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,284] Trial 791 finished with value: 25.701541290380128 and parameters: {'x': 1.365242054544611, 'y': 1.3582363887871638}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,301] Trial 792 finished with value: 9.9765181165267 and parameters: {'x': 1.2636064241936606, 'y': 1.281946850263289}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,319] Trial 793 finished with value: 0.5631580533602246 and parameters: {'x': 1.130574911137364, 'y': 1.3520987667614066}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,337] Trial 794 finished with value: 0.08063517808415319 and parameters: {'x': 1.198031499000429, 'y': 1.455631058158983}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,354] Trial 795 finished with value: 28.728244472070013 and parameters: {'x': 1.3500896086153653, 'y': 1.2878991544058853}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,371] Trial 796 finished with value: 0.20130259762415972 and parameters: {'x': 1.100808016274952, 'y': 1.25549788858802}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,389] Trial 797 finished with value: 0.3318788876955964 and parameters: {'x': 1.1272977167831246, 'y': 1.2146152529259948}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,406] Trial 798 finished with value: 0.018079964229194917 and parameters: {'x': 1.0785594626165806, 'y': 1.1742030645428772}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,424] Trial 799 finished with value: 19.050716291128076 and parameters: {'x': 1.2866151024091443, 'y': 1.2198492255480131}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,441] Trial 800 finished with value: 1.3517051888934168 and parameters: {'x': 1.2056031835682335, 'y': 1.3390485979802391}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,459] Trial 801 finished with value: 0.4692907013198719 and parameters: {'x': 1.1304857827644643, 'y': 1.2107475167960324}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,477] Trial 802 finished with value: 1.2330417097393265 and parameters: {'x': 1.2607130504920627, 'y': 1.481458968091823}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,494] Trial 803 finished with value: 0.6544610602571362 and parameters: {'x': 1.0997511883149342, 'y': 1.2897341037339358}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,512] Trial 804 finished with value: 0.02191285287318715 and parameters: {'x': 1.0663273197024394, 'y': 1.1502878257726026}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,529] Trial 805 finished with value: 8.749718804623969 and parameters: {'x': 1.2063824524356996, 'y': 1.1602802406364017}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,546] Trial 806 finished with value: 26.76540892864611 and parameters: {'x': 1.3052317347070077, 'y': 1.1871781166772755}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,563] Trial 807 finished with value: 3.2929051560264093 and parameters: {'x': 1.0620725940576687, 'y': 1.3093556362823577}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,580] Trial 808 finished with value: 4.852122684402441 and parameters: {'x': 1.1709801951127639, 'y': 1.1515838603857067}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,597] Trial 809 finished with value: 0.10875472304252488 and parameters: {'x': 1.0999685559219072, 'y': 1.2413570886485066}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,614] Trial 810 finished with value: 8.398557858549097 and parameters: {'x': 1.059189816923948, 'y': 1.4116252713477315}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,631] Trial 811 finished with value: 788.9093172188136 and parameters: {'x': 1.9891471892043566, 'y': 1.1496958437655118}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,647] Trial 812 finished with value: 4.1841114015899 and parameters: {'x': 1.2162075132338177, 'y': 1.2757555601295327}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,663] Trial 813 finished with value: 2.8102633569681714 and parameters: {'x': 1.1438989116922402, 'y': 1.1414850668306742}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,680] Trial 814 finished with value: 25.481135466878353 and parameters: {'x': 1.0450738176497663, 'y': 1.5969475858164666}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,697] Trial 815 finished with value: 9.978045301654632 and parameters: {'x': -1.0673870839300168, 'y': 1.3781447484872218}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,714] Trial 816 finished with value: 2.493490108977109 and parameters: {'x': 1.0364532046045645, 'y': 1.2321010514126118}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,731] Trial 817 finished with value: 3.9130926346233714 and parameters: {'x': 1.1529345386846799, 'y': 1.132034733881147}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,746] Trial 818 finished with value: 6.789295758113663 and parameters: {'x': 1.0181949858713684, 'y': 1.2972774473400213}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,763] Trial 819 finished with value: 10.943394789639893 and parameters: {'x': 1.2356285644587819, 'y': 1.196810163465928}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,779] Trial 820 finished with value: 1.6660697172938268 and parameters: {'x': 1.1147578775233289, 'y': 1.1141199534341557}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,795] Trial 821 finished with value: 3.5198890060924555 and parameters: {'x': 1.0117992383411412, 'y': 1.211347660705125}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,812] Trial 822 finished with value: 0.9966190002367743 and parameters: {'x': 1.0801556891515125, 'y': 1.067227817403553}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,829] Trial 823 finished with value: 11.917747087248669 and parameters: {'x': 0.9939687174849264, 'y': 1.3331941860397745}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,846] Trial 824 finished with value: 36.4782154357576 and parameters: {'x': 1.3120208797522392, 'y': 1.118233319019207}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,863] Trial 825 finished with value: 162.42172235029298 and parameters: {'x': -0.40702960569274715, 'y': 1.432330078097452}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,880] Trial 826 finished with value: 0.26129533498697655 and parameters: {'x': 1.1414426613313873, 'y': 1.2537701410050284}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,897] Trial 827 finished with value: 87.6053261800182 and parameters: {'x': 1.4144181952161317, 'y': 1.0655195615028168}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,914] Trial 828 finished with value: 0.020194942353004834 and parameters: {'x': 1.0645065682835864, 'y': 1.1458367159035956}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,931] Trial 829 finished with value: 3.8752990419736046 and parameters: {'x': 0.967201093761748, 'y': 1.1323084227683171}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,949] Trial 830 finished with value: 0.1116197446588814 and parameters: {'x': 1.0435557396949362, 'y': 1.0558841735991524}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,968] Trial 831 finished with value: 5.281020174691191 and parameters: {'x': 0.9612473285518044, 'y': 1.1537684529076369}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:26,985] Trial 832 finished with value: 0.9449415262343133 and parameters: {'x': 1.0658694279482315, 'y': 1.0390929610834188}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,002] Trial 833 finished with value: 3.69561586218011 and parameters: {'x': 0.9240276787839264, 'y': 1.0459168195064126}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,019] Trial 834 finished with value: 3.63709637138627 and parameters: {'x': 1.0142040882338703, 'y': 1.2193163723154328}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,037] Trial 835 finished with value: 9.510850395568328 and parameters: {'x': 0.9130892171130789, 'y': 1.1420060968822459}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,053] Trial 836 finished with value: 7.324087282156589 and parameters: {'x': 0.8563517802452886, 'y': 1.0035873765435006}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,070] Trial 837 finished with value: 1.0241255364360355 and parameters: {'x': 1.0889031128700266, 'y': 1.0849021634822282}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,086] Trial 838 finished with value: 6.44276878401372 and parameters: {'x': 0.9819457761811476, 'y': 0.7103978306401642}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,103] Trial 839 finished with value: 32.561503073250215 and parameters: {'x': 1.2286089383333247, 'y': 0.9393111673989679}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,120] Trial 840 finished with value: 0.0958216906424493 and parameters: {'x': 0.878203544968515, 'y': 0.7996997365788552}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,137] Trial 841 finished with value: 3.313642047349547 and parameters: {'x': 1.1627136274561498, 'y': 1.170597537667818}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,154] Trial 842 finished with value: 0.546704080565346 and parameters: {'x': 1.0313703201282003, 'y': 0.9898518745933887}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,171] Trial 843 finished with value: 19.327141560973633 and parameters: {'x': 0.8013544915285395, 'y': 1.0813464513297393}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,188] Trial 844 finished with value: 0.3295636373884182 and parameters: {'x': 0.9616762474084839, 'y': 0.867541633656712}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,204] Trial 845 finished with value: 24.510740035911134 and parameters: {'x': 1.0966013167146993, 'y': 0.7075454760087151}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,221] Trial 846 finished with value: 878.7267800516429 and parameters: {'x': -1.9826568946013707, 'y': 0.9816394001117593}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,238] Trial 847 finished with value: 543.3272921774426 and parameters: {'x': 0.9141434256761538, 'y': -1.4952641915955618}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,254] Trial 848 finished with value: 4.521617249963979 and parameters: {'x': 1.0066252737349521, 'y': 1.225934356971008}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,271] Trial 849 finished with value: 645.5317328572954 and parameters: {'x': 0.7916931281644353, 'y': -1.9138682493593984}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,288] Trial 850 finished with value: 65.55690129395634 and parameters: {'x': -0.07820832601205444, 'y': 0.808577557508035}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,305] Trial 851 finished with value: 5.740602094419587 and parameters: {'x': 1.1633301320899174, 'y': 1.11429881178136}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,322] Trial 852 finished with value: 3.6613242397722208 and parameters: {'x': 1.0714571114404248, 'y': 1.3392327398159183}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,339] Trial 853 finished with value: 4.177125432617841 and parameters: {'x': 0.8594855223144366, 'y': 0.9426119327837137}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,357] Trial 854 finished with value: 1.6018219908271387 and parameters: {'x': 0.9578132158877635, 'y': 1.0438989336382076}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,374] Trial 855 finished with value: 3.868087801116101 and parameters: {'x': 1.0254764013085258, 'y': 0.8549438024091395}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,391] Trial 856 finished with value: 1.4519536326375464 and parameters: {'x': 0.9060603828091166, 'y': 0.7008151138353536}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,408] Trial 857 finished with value: 0.09976198508762064 and parameters: {'x': 1.0909742444591912, 'y': 1.1599782035979869}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,426] Trial 858 finished with value: 24.281951673480688 and parameters: {'x': 1.187978713893907, 'y': 0.9188848983720309}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,442] Trial 859 finished with value: 56.0711920081301 and parameters: {'x': 0.9686637803835778, 'y': 1.6871099604623327}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,459] Trial 860 finished with value: 156.0545613669089 and parameters: {'x': 1.4980614941253196, 'y': 0.9959635163632802}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,476] Trial 861 finished with value: 0.08370217741280667 and parameters: {'x': 0.8923709443001886, 'y': 0.7694710770070585}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,493] Trial 862 finished with value: 36.57997882340599 and parameters: {'x': 0.8170469212481193, 'y': 1.2721027422981763}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,510] Trial 863 finished with value: 5.035829156212018 and parameters: {'x': 1.0386909098086696, 'y': 0.8545056310355482}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,528] Trial 864 finished with value: 27.583065912647836 and parameters: {'x': 1.2560832076860224, 'y': 1.0531738921483853}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,545] Trial 865 finished with value: 0.02857647011274476 and parameters: {'x': 1.097746582302554, 'y': 1.2188396131044952}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,563] Trial 866 finished with value: 0.027935854228877358 and parameters: {'x': 0.980623612719727, 'y': 0.9450213416101074}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,580] Trial 867 finished with value: 12.708501415316187 and parameters: {'x': 0.7658196816210618, 'y': 0.9421996323039762}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,597] Trial 868 finished with value: 0.017446279721371118 and parameters: {'x': 0.8928207737354739, 'y': 0.8048483207630279}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,615] Trial 869 finished with value: 0.6970022347061386 and parameters: {'x': 0.783650629108232, 'y': 0.6947429899000489}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,632] Trial 870 finished with value: 0.4670102651021103 and parameters: {'x': 0.850268609591007, 'y': 0.7896343490730733}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,649] Trial 871 finished with value: 0.17030024362084983 and parameters: {'x': 0.8354196177636197, 'y': 0.7357695027724354}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,666] Trial 872 finished with value: 0.011365325216760152 and parameters: {'x': 0.8939956222183837, 'y': 0.8003612970219947}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,683] Trial 873 finished with value: 0.07467605436105257 and parameters: {'x': 0.9314752181756976, 'y': 0.8411922716435077}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,701] Trial 874 finished with value: 0.04066251426342462 and parameters: {'x': 0.8947833027539809, 'y': 0.8178394729421418}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,719] Trial 875 finished with value: 0.0046718985413771435 and parameters: {'x': 0.9481177907101005, 'y': 0.8944774743880765}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,736] Trial 876 finished with value: 1.2563635784427725 and parameters: {'x': 0.7565238381270097, 'y': 0.6817396030431958}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,753] Trial 877 finished with value: 7.243689068205826 and parameters: {'x': 0.7797863787299815, 'y': 0.876305403465548}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,771] Trial 878 finished with value: 359.3734464512024 and parameters: {'x': 0.8653417710702528, 'y': -1.146850556264968}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,788] Trial 879 finished with value: 0.7636267377070419 and parameters: {'x': 0.9167605287613201, 'y': 0.7534614811955276}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,806] Trial 880 finished with value: 0.17975339469060742 and parameters: {'x': 0.9555328525109175, 'y': 0.870879532908359}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,824] Trial 881 finished with value: 0.4290997803457848 and parameters: {'x': 0.8632151070438223, 'y': 0.8092019845572461}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,842] Trial 882 finished with value: 0.040885391931070825 and parameters: {'x': 0.9682492064743131, 'y': 0.9175372296458422}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,859] Trial 883 finished with value: 2.45530828622145 and parameters: {'x': 0.7985931754956088, 'y': 0.7931455121420693}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,876] Trial 884 finished with value: 0.709964885589711 and parameters: {'x': 0.9017963050753003, 'y': 0.8969217549350927}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,894] Trial 885 finished with value: 3.7935894967152133 and parameters: {'x': 0.9774636147934714, 'y': 0.7606767652007117}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,912] Trial 886 finished with value: 0.975666589974776 and parameters: {'x': 0.7588246705272994, 'y': 0.6716011557361108}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,929] Trial 887 finished with value: 6.223735499283934 and parameters: {'x': 0.8513124288253936, 'y': 0.9737635223840697}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,947] Trial 888 finished with value: 1.9240875231015822 and parameters: {'x': 0.9958646099695816, 'y': 0.8530354558583275}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,964] Trial 889 finished with value: 0.3316490286673161 and parameters: {'x': 0.9245890631051886, 'y': 0.9119580367086931}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:27,982] Trial 890 finished with value: 0.5102234555415952 and parameters: {'x': 0.8429417587790619, 'y': 0.7802326689159935}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,001] Trial 891 finished with value: 0.5978986487878492 and parameters: {'x': 1.0161262139898684, 'y': 0.9552053942072254}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,019] Trial 892 finished with value: 118.11066103034041 and parameters: {'x': -1.3875698578485514, 'y': 0.8651134801983029}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,036] Trial 893 finished with value: 1.5815006844482926 and parameters: {'x': 0.9246956659388923, 'y': 0.9805941399010556}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,054] Trial 894 finished with value: 385.3595540903186 and parameters: {'x': 1.6328683801632096, 'y': 0.7042218595650771}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,071] Trial 895 finished with value: 132.36044658214496 and parameters: {'x': 0.7512298380331655, 'y': -0.5858648414568126}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,089] Trial 896 finished with value: 2.6108103768824087 and parameters: {'x': 0.9930834877808792, 'y': 0.8246362712309261}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,106] Trial 897 finished with value: 0.162056369963974 and parameters: {'x': 0.8389523136608958, 'y': 0.6669465388511949}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,123] Trial 898 finished with value: 1.9503755847193167 and parameters: {'x': 0.9322231996687043, 'y': 1.008531380110341}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,141] Trial 899 finished with value: 2.595846669004023 and parameters: {'x': 1.026009015070312, 'y': 0.8915991793920045}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,159] Trial 900 finished with value: 101.16368658841311 and parameters: {'x': 0.8815875440333277, 'y': -0.2285353002709931}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,178] Trial 901 finished with value: 2.352108956176181 and parameters: {'x': 0.7861099317604122, 'y': 0.7698358717608222}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,196] Trial 902 finished with value: 4.014059203437142 and parameters: {'x': 1.0455950097360989, 'y': 0.892969640848513}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,216] Trial 903 finished with value: 105.29978166914806 and parameters: {'x': 0.9459981947186776, 'y': 1.921055193769825}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,234] Trial 904 finished with value: 58.647220692077084 and parameters: {'x': -0.4945383141195804, 'y': 0.9956578586118432}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,251] Trial 905 finished with value: 0.31029344847504603 and parameters: {'x': 0.8576053419000126, 'y': 0.7893401683947152}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,269] Trial 906 finished with value: 24.37078270561541 and parameters: {'x': -0.6093125134711168, 'y': 0.837961813420038}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,287] Trial 907 finished with value: 0.008304403737744473 and parameters: {'x': 1.0094745615457112, 'y': 1.0281023535546623}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,305] Trial 908 finished with value: 2.037903525753553 and parameters: {'x': 1.1066098959750315, 'y': 1.0822289426712837}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,323] Trial 909 finished with value: 0.03514582262151627 and parameters: {'x': 1.01856778246003, 'y': 1.0188252852313286}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,340] Trial 910 finished with value: 15.513380973491607 and parameters: {'x': 1.170280591469785, 'y': 0.9760546234666201}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,358] Trial 911 finished with value: 0.7613180546884211 and parameters: {'x': 1.0691962736235128, 'y': 1.0562019424982427}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,376] Trial 912 finished with value: 0.10428018814820676 and parameters: {'x': 0.9984820066704788, 'y': 1.0292584039576804}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,395] Trial 913 finished with value: 11.822075756101778 and parameters: {'x': 1.1314174838221176, 'y': 0.936524306665719}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,413] Trial 914 finished with value: 0.0020683744128554347 and parameters: {'x': 1.0454768568871633, 'y': 1.0930698062292565}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,431] Trial 915 finished with value: 1.8641881440346124 and parameters: {'x': 0.9700244614994449, 'y': 1.0774498225946907}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,449] Trial 916 finished with value: 0.1766562757151207 and parameters: {'x': 1.0243877918926478, 'y': 1.0074106637993143}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,467] Trial 917 finished with value: 499.85337820784747 and parameters: {'x': 1.783034630246581, 'y': 0.944844047268048}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,484] Trial 918 finished with value: 3.6102480606420535 and parameters: {'x': 0.9473113850894688, 'y': 1.0873323215769812}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,502] Trial 919 finished with value: 105.55624639194667 and parameters: {'x': 0.08250498855293795, 'y': 1.0301078563661366}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,520] Trial 920 finished with value: 3.016757674361977 and parameters: {'x': 1.0471389389535377, 'y': 0.9228757772789581}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,538] Trial 921 finished with value: 8.437700055033243 and parameters: {'x': 0.8996868858538216, 'y': 1.0997404252782934}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,556] Trial 922 finished with value: 7.577349434411607 and parameters: {'x': 0.8266347459890291, 'y': 0.9580483900331591}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,575] Trial 923 finished with value: 0.008481447076757674 and parameters: {'x': 0.9573990799626281, 'y': 0.9084480680081787}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,593] Trial 924 finished with value: 11.846096099420945 and parameters: {'x': 0.7749882903747911, 'y': 0.9440521272266573}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,611] Trial 925 finished with value: 1.1786674402963826 and parameters: {'x': 0.8754895081942923, 'y': 0.874331986873236}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,629] Trial 926 finished with value: 0.11497525588070694 and parameters: {'x': 0.9537273089296738, 'y': 0.8760049932724925}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,647] Trial 927 finished with value: 0.24782632782104336 and parameters: {'x': 0.9178322498614739, 'y': 0.891515406362171}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,665] Trial 928 finished with value: 4.3042227832723645 and parameters: {'x': 0.758370933086415, 'y': 0.7811807896808312}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,682] Trial 929 finished with value: 11.10761676124021 and parameters: {'x': 0.8294595394508261, 'y': 1.020847426208559}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,700] Trial 930 finished with value: 1.3800755271124439 and parameters: {'x': 0.9778768164018251, 'y': 0.8387872852330176}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,719] Trial 931 finished with value: 1.5333663113750604 and parameters: {'x': 0.9011969993556646, 'y': 0.9355903979496984}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,737] Trial 932 finished with value: 11.954686095088752 and parameters: {'x': 1.0175526811822606, 'y': 0.6896624214054387}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,755] Trial 933 finished with value: 7.79224043619418 and parameters: {'x': 0.8379894533023114, 'y': 0.9809016370502324}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,772] Trial 934 finished with value: 0.20432419435037247 and parameters: {'x': 0.9603634015380976, 'y': 0.8772697457521917}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,790] Trial 935 finished with value: 3.9631634260716315 and parameters: {'x': 0.7457542894078538, 'y': 0.7535962262798944}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,808] Trial 936 finished with value: 0.021105671715492436 and parameters: {'x': 0.8986770704665675, 'y': 0.8180316900078822}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,825] Trial 937 finished with value: 0.08027304961535857 and parameters: {'x': 1.002810692722428, 'y': 0.977298180690866}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,843] Trial 938 finished with value: 4.16710542990309 and parameters: {'x': 0.8303127421621841, 'y': 0.8928476564767437}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,861] Trial 939 finished with value: 1.1847037418184465 and parameters: {'x': 1.0679342200760422, 'y': 1.0318516115823466}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,879] Trial 940 finished with value: 3.2658881154125563 and parameters: {'x': 0.9194073934770693, 'y': 0.6647720658831653}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,897] Trial 941 finished with value: 5.213669029275126 and parameters: {'x': 0.9869775665349723, 'y': 0.7457938287553767}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,915] Trial 942 finished with value: 7.132085776492043 and parameters: {'x': 1.0926555389255308, 'y': 0.9269972558665798}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,934] Trial 943 finished with value: 62.930550930636606 and parameters: {'x': -0.26986289755236237, 'y': 0.8558840857388895}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,953] Trial 944 finished with value: 28.708298634971243 and parameters: {'x': 0.7062795970236547, 'y': 1.0338264482176174}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,971] Trial 945 finished with value: 0.40724318493926065 and parameters: {'x': 0.8715768956595779, 'y': 0.8221563395357112}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:28,989] Trial 946 finished with value: 0.1695945893117629 and parameters: {'x': 0.9888167009739531, 'y': 0.936591791900831}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,007] Trial 947 finished with value: 11.321661740687544 and parameters: {'x': 1.0465663049090523, 'y': 0.7588564835809277}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,025] Trial 948 finished with value: 1103.9451345601633 and parameters: {'x': -1.6915471289505613, 'y': -0.4503154967786154}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,044] Trial 949 finished with value: 7.8173073542865605 and parameters: {'x': -0.89289336140544, 'y': 1.003031781395648}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,062] Trial 950 finished with value: 4.503425183106819 and parameters: {'x': 0.933778103459062, 'y': 1.084050948543108}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,080] Trial 951 finished with value: 8.031941526422056 and parameters: {'x': 0.7909228741808859, 'y': 0.9081935322264173}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,098] Trial 952 finished with value: 20.14694407738879 and parameters: {'x': 1.1373272466426465, 'y': 0.8448699186884698}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,116] Trial 953 finished with value: 0.723606601516726 and parameters: {'x': 0.8676601867016482, 'y': 0.6688048728414794}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,134] Trial 954 finished with value: 0.35253011800392214 and parameters: {'x': 0.9732049722425592, 'y': 1.0064416722618412}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,153] Trial 955 finished with value: 582.2692760329392 and parameters: {'x': 1.0493595004247869, 'y': -1.3118652348695625}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,172] Trial 956 finished with value: 1.4384246040344566 and parameters: {'x': 0.9258214318180388, 'y': 0.7374405975558186}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,189] Trial 957 finished with value: 18.458829947690564 and parameters: {'x': 1.1654441826450601, 'y': 0.9289414014799457}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,208] Trial 958 finished with value: 14.442030261414498 and parameters: {'x': 0.8370943155163832, 'y': 1.0804042824331992}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,226] Trial 959 finished with value: 5.511009077100848 and parameters: {'x': 1.0210437523605034, 'y': 0.8077843913920423}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,243] Trial 960 finished with value: 19.454259076992678 and parameters: {'x': 0.7313118890008905, 'y': 0.9750677550412366}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,261] Trial 961 finished with value: 10.376134274953477 and parameters: {'x': 1.0911662872599597, 'y': 0.8686528264683289}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,279] Trial 962 finished with value: 3.873457891321196 and parameters: {'x': 0.9325302135538588, 'y': 1.066307940244886}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,297] Trial 963 finished with value: 1.4326021678355438 and parameters: {'x': 0.825670878548329, 'y': 0.8001474124661759}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,315] Trial 964 finished with value: 1.060840282153053 and parameters: {'x': 1.0113567623510034, 'y': 0.9198516610584031}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,333] Trial 965 finished with value: 2.940452214852096 and parameters: {'x': 0.9028934132941364, 'y': 0.9864188091444829}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,351] Trial 966 finished with value: 730.0128190699521 and parameters: {'x': 1.8550217143435739, 'y': 0.7405838389850616}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,369] Trial 967 finished with value: 11.11362966107739 and parameters: {'x': 1.2069619644012093, 'y': 1.1240291202768697}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,388] Trial 968 finished with value: 7.351687903054511 and parameters: {'x': 1.0645419188291656, 'y': 0.8621863632695321}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,406] Trial 969 finished with value: 0.15607917462865495 and parameters: {'x': 0.9672952803335578, 'y': 0.9750314148524732}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,425] Trial 970 finished with value: 4.178065368756724 and parameters: {'x': 0.7851131417443332, 'y': 0.8196731303320866}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,443] Trial 971 finished with value: 35.45687846209774 and parameters: {'x': 1.1291900365567629, 'y': 0.6797535136891891}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,462] Trial 972 finished with value: 10.999364197787193 and parameters: {'x': 0.8491556542410287, 'y': 1.0523750017334346}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,481] Trial 973 finished with value: 0.17797666971389678 and parameters: {'x': 0.9323034842782937, 'y': 0.9108303741459745}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,499] Trial 974 finished with value: 1.185491294826966 and parameters: {'x': 1.0013363448188013, 'y': 1.1115546623124433}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,516] Trial 975 finished with value: 20.341143019796135 and parameters: {'x': 1.0997097521004318, 'y': 0.7584602083596348}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,534] Trial 976 finished with value: 14.68352006047538 and parameters: {'x': 0.7070329236334487, 'y': 0.8819647942801206}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,552] Trial 977 finished with value: 3.597451125378639 and parameters: {'x': 0.8977837644865865, 'y': 0.9954095357902634}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,571] Trial 978 finished with value: 4.304529482755299 and parameters: {'x': 1.017323503513739, 'y': 0.827480742949199}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,589] Trial 979 finished with value: 23.617434168807147 and parameters: {'x': 1.1893262720551578, 'y': 0.9288881909991601}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,608] Trial 980 finished with value: 0.30943090634000886 and parameters: {'x': 0.7852370917909903, 'y': 0.6679108167669796}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,626] Trial 981 finished with value: 2.2604092317930964 and parameters: {'x': 0.9592445182068567, 'y': 1.070441370226555}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,644] Trial 982 finished with value: 5.5741688089393175 and parameters: {'x': 0.8766117139135159, 'y': 1.0042222291349692}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,662] Trial 983 finished with value: 18.454066858216517 and parameters: {'x': 1.0915970484369484, 'y': 0.7620998109839479}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,679] Trial 984 finished with value: 0.8040822336077305 and parameters: {'x': 1.0242524620648794, 'y': 1.1387309357659867}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,697] Trial 985 finished with value: 2.7677917224531203 and parameters: {'x': 0.8516904373233825, 'y': 0.8910810365774119}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,716] Trial 986 finished with value: 0.8450974113855114 and parameters: {'x': 0.9689041760990258, 'y': 0.8468987297615012}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,735] Trial 987 finished with value: 15.785353771085441 and parameters: {'x': 1.1644229611712948, 'y': 0.9589133418827867}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,754] Trial 988 finished with value: 0.6720350946204761 and parameters: {'x': 1.0626472044676614, 'y': 1.0474810592641361}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,772] Trial 989 finished with value: 1.3503717536087145 and parameters: {'x': 0.934049837572296, 'y': 0.7564308966312635}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,790] Trial 990 finished with value: 9.228283612703555 and parameters: {'x': 0.7879065271892224, 'y': 0.9238362964895503}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,808] Trial 991 finished with value: 11.691898902946912 and parameters: {'x': 0.8840374632536381, 'y': 1.1232597320816096}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,826] Trial 992 finished with value: 1.8091416579457613 and parameters: {'x': 0.994533780491238, 'y': 0.854594214614425}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,844] Trial 993 finished with value: 28.397854676471688 and parameters: {'x': 1.0943918034147069, 'y': 0.6648806486642272}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,862] Trial 994 finished with value: 1.830736979306249 and parameters: {'x': 0.9249323935097273, 'y': 0.9905962624087503}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,881] Trial 995 finished with value: 7.542668684217663 and parameters: {'x': 0.7260318800543646, 'y': 0.8003915747763028}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,900] Trial 996 finished with value: 37.04340942596152 and parameters: {'x': 1.2350097193864955, 'y': 0.9170699226811103}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,918] Trial 997 finished with value: 0.09305171487764412 and parameters: {'x': 1.0319921573537136, 'y': 1.03467166002721}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,937] Trial 998 finished with value: 22.090226917792883 and parameters: {'x': 0.830225123719477, 'y': 1.1589694384833014}. Best is trial 593 with value: 0.0011202322795740847.\n",
      "[I 2024-01-31 07:13:29,955] Trial 999 finished with value: 17.006901739385846 and parameters: {'x': 1.1099221758174505, 'y': 0.8196795092138804}. Best is trial 593 with value: 0.0011202322795740847.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# while the logging from Optuna is very interesting,\n",
    "#                                  ^^^^\n",
    "# it is also very verbose!\n",
    "#            ^^^^^^^^^^^^\n",
    "# uncomment the line below to see it in all its glory\n",
    "#\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0165f97d-69bb-4be2-bc55-b577f6e3b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.9917703579551643, 'y': 0.9868526766822685}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110333d2-8616-4f71-8d24-902c76235463",
   "metadata": {},
   "source": [
    "Right, let's use `transformers.Trainer.hyperparameter_search` and Optuna to do a hyperparameter search for the best values (given a range) of:\n",
    "\n",
    "1. `num_train_epochs`\n",
    "2. `alpha`\n",
    "3. `temperature`\n",
    "\n",
    "We set up a function `hp_space` which returns a `dict` with the target hyperparameter names as keys, with `suggest_*` values. This function is what we pass to `hyperparameter_search` as the [`hp_space`](optuna.logging.set_verbosity(optuna.logging.WARNING)) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea88ba72-25dd-4631-ab13-fb5edbeb92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 2, 20)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8350b-43a1-4f93-8df9-ec3fd2a99988",
   "metadata": {},
   "source": [
    "#### For reference\n",
    "\n",
    "    print(best_run)\n",
    "\n",
    "    BestRun(run_id='12', objective=0.9319354838709677, hyperparameters={'num_train_epochs': 10, 'alpha': 0.19598992738199783, 'temperature': 2.713801879785792})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5ebab0e-3726-4d78-9d35-00fe39739961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-31 07:13:37,604] A new study created in memory with name: no-name-a158cdc1-dac0-484a-94df-329398d52f3c\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1908' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1908/1908 07:40, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221007</td>\n",
       "      <td>0.577742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.108601</td>\n",
       "      <td>0.808065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.074753</td>\n",
       "      <td>0.865484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.894839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.899677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.049959</td>\n",
       "      <td>0.902903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 07:21:19,101] Trial 0 finished with value: 0.9029032258064517 and parameters: {'num_train_epochs': 6, 'alpha': 0.6215164171858861, 'temperature': 7.154378661005527}. Best is trial 0 with value: 0.9029032258064517.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1908' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1908/1908 07:39, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212970</td>\n",
       "      <td>0.574194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.105906</td>\n",
       "      <td>0.803226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>0.862903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.059032</td>\n",
       "      <td>0.893871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.051974</td>\n",
       "      <td>0.899355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.049820</td>\n",
       "      <td>0.901290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 07:28:59,159] Trial 1 finished with value: 0.9012903225806451 and parameters: {'num_train_epochs': 6, 'alpha': 0.6375458261706345, 'temperature': 9.015247843980397}. Best is trial 0 with value: 0.9029032258064517.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2544\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2544' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2544/2544 10:42, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195098</td>\n",
       "      <td>0.567419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.095799</td>\n",
       "      <td>0.806129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.065948</td>\n",
       "      <td>0.869032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.111400</td>\n",
       "      <td>0.051008</td>\n",
       "      <td>0.903548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>0.905484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.038651</td>\n",
       "      <td>0.908387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.912581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.035365</td>\n",
       "      <td>0.915806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 07:39:42,881] Trial 2 finished with value: 0.9158064516129032 and parameters: {'num_train_epochs': 8, 'alpha': 0.2532303692242144, 'temperature': 18.797593524656225}. Best is trial 2 with value: 0.9158064516129032.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 06:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203117</td>\n",
       "      <td>0.551290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.105408</td>\n",
       "      <td>0.779355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.076848</td>\n",
       "      <td>0.851613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.065022</td>\n",
       "      <td>0.877097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.061329</td>\n",
       "      <td>0.880323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 07:46:22,592] Trial 3 finished with value: 0.8803225806451613 and parameters: {'num_train_epochs': 5, 'alpha': 0.1526795477015528, 'temperature': 17.992160394746126}. Best is trial 2 with value: 0.9158064516129032.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 06:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.210165</td>\n",
       "      <td>0.558710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.785161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.078341</td>\n",
       "      <td>0.853226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.065968</td>\n",
       "      <td>0.878387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.062133</td>\n",
       "      <td>0.883226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 07:53:01,485] Trial 4 finished with value: 0.8832258064516129 and parameters: {'num_train_epochs': 5, 'alpha': 0.08611537482779208, 'temperature': 11.686051023261992}. Best is trial 2 with value: 0.9158064516129032.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193541</td>\n",
       "      <td>0.575806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.093334</td>\n",
       "      <td>0.814839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.873548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.047731</td>\n",
       "      <td>0.906774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.040021</td>\n",
       "      <td>0.909677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.034680</td>\n",
       "      <td>0.914839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.031366</td>\n",
       "      <td>0.918387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.029782</td>\n",
       "      <td>0.923548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.028407</td>\n",
       "      <td>0.926452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.028028</td>\n",
       "      <td>0.925484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-5/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 08:06:19,132] Trial 5 finished with value: 0.925483870967742 and parameters: {'num_train_epochs': 10, 'alpha': 0.5815004287071903, 'temperature': 17.589136456148452}. Best is trial 5 with value: 0.925483870967742.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/1908 00:53 < 04:29, 5.89 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203375</td>\n",
       "      <td>0.564516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 08:07:19,936] Trial 6 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2226' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2226/2226 09:11, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313801</td>\n",
       "      <td>0.621613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.128323</td>\n",
       "      <td>0.825806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.075489</td>\n",
       "      <td>0.880323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.054920</td>\n",
       "      <td>0.909032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.045993</td>\n",
       "      <td>0.913226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>0.917097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.919677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-7/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 08:16:32,919] Trial 7 finished with value: 0.9196774193548387 and parameters: {'num_train_epochs': 7, 'alpha': 0.26318226182977766, 'temperature': 2.764331064142487}. Best is trial 5 with value: 0.925483870967742.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248238</td>\n",
       "      <td>0.613548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.108064</td>\n",
       "      <td>0.830645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>0.884839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.911935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.039419</td>\n",
       "      <td>0.914516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.033839</td>\n",
       "      <td>0.920645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.030607</td>\n",
       "      <td>0.922581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.029091</td>\n",
       "      <td>0.927097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.027794</td>\n",
       "      <td>0.928710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>0.928387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 08:29:39,634] Trial 8 finished with value: 0.9283870967741935 and parameters: {'num_train_epochs': 10, 'alpha': 0.33862775017274593, 'temperature': 4.110889679920053}. Best is trial 8 with value: 0.9283870967741935.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2863' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2862/2862 11:37, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250801</td>\n",
       "      <td>0.610968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.109993</td>\n",
       "      <td>0.827097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.068586</td>\n",
       "      <td>0.882581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.049432</td>\n",
       "      <td>0.909355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.040902</td>\n",
       "      <td>0.912581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.918387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.032356</td>\n",
       "      <td>0.920645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.923871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.925806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-9/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 08:41:24,192] Trial 9 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2863' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2863/3180 11:38 < 01:17, 4.09 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221233</td>\n",
       "      <td>0.595806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.101669</td>\n",
       "      <td>0.827097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.065924</td>\n",
       "      <td>0.879355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.908065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>0.910323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>0.916774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.029309</td>\n",
       "      <td>0.926774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.027974</td>\n",
       "      <td>0.926774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 08:53:09,770] Trial 10 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2227' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2227/3180 09:05 < 03:53, 4.08 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195574</td>\n",
       "      <td>0.579032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.094035</td>\n",
       "      <td>0.817742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.873548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.047815</td>\n",
       "      <td>0.906774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.915161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.031351</td>\n",
       "      <td>0.918710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:02:22,767] Trial 11 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2227' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2227/2862 09:06 < 02:35, 4.07 it/s, Epoch 7/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.196111</td>\n",
       "      <td>0.575161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.813548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.064719</td>\n",
       "      <td>0.872581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.049225</td>\n",
       "      <td>0.905161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.908387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.912903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.033249</td>\n",
       "      <td>0.916129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:11:36,035] Trial 12 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2227' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2227/2862 09:06 < 02:35, 4.07 it/s, Epoch 7/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203717</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.097629</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>0.874839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.049504</td>\n",
       "      <td>0.906129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.041525</td>\n",
       "      <td>0.907419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.036204</td>\n",
       "      <td>0.914194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.916774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-13/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:20:49,261] Trial 13 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2863' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2863/3180 11:38 < 01:17, 4.09 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.198561</td>\n",
       "      <td>0.579355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.095034</td>\n",
       "      <td>0.819677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.876129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.906774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.040070</td>\n",
       "      <td>0.909032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.034636</td>\n",
       "      <td>0.915161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.919032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.923871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.028332</td>\n",
       "      <td>0.926452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-14/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:32:34,904] Trial 14 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2544\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2227' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2227/2544 09:06 < 01:17, 4.07 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212757</td>\n",
       "      <td>0.582903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.101725</td>\n",
       "      <td>0.818387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.872258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>0.904516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.043556</td>\n",
       "      <td>0.907097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.911935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.035940</td>\n",
       "      <td>0.914839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-15/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:41:47,932] Trial 15 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/2862 00:53 < 07:11, 5.89 it/s, Epoch 1/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195036</td>\n",
       "      <td>0.574194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:42:48,757] Trial 16 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/3180 00:54 < 08:14, 5.79 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192155</td>\n",
       "      <td>0.574839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:43:50,522] Trial 17 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2544\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2227' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2227/2544 09:06 < 01:17, 4.07 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.603871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.070067</td>\n",
       "      <td>0.879032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.051913</td>\n",
       "      <td>0.906452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.038131</td>\n",
       "      <td>0.913226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.035478</td>\n",
       "      <td>0.916452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-18/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-01-31 09:53:03,663] Trial 18 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2226' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2226/2226 09:12, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371107</td>\n",
       "      <td>0.629677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.828387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.077482</td>\n",
       "      <td>0.880968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>0.912903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.047172</td>\n",
       "      <td>0.918710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.042958</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>0.922258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-19/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-01-31 10:02:17,153] Trial 19 finished with value: 0.922258064516129 and parameters: {'num_train_epochs': 7, 'alpha': 0.49768972988073445, 'temperature': 2.251144865718066}. Best is trial 8 with value: 0.9283870967741935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 12min 51s, sys: 2min 17s, total: 2h 15min 8s\n",
      "Wall time: 2h 48min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_run = distilbert_trainer.hyperparameter_search(\n",
    "    n_trials=20,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0888eed-5024-4d50-a5ea-ef4982626bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='8', objective=0.9283870967741935, hyperparameters={'num_train_epochs': 10, 'alpha': 0.33862775017274593, 'temperature': 4.110889679920053})\n"
     ]
    }
   ],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "738b70ea-fd1e-45d2-ad48-0e7f24c28578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/a_naughty_alpaca/dev/github/transformers-gcp/distilbert-base-uncased-distilled-clinc is already a clone of https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.258028</td>\n",
       "      <td>0.692581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.531000</td>\n",
       "      <td>0.655215</td>\n",
       "      <td>0.875806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.531000</td>\n",
       "      <td>0.378394</td>\n",
       "      <td>0.917419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.268497</td>\n",
       "      <td>0.937097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.229872</td>\n",
       "      <td>0.941935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.213391</td>\n",
       "      <td>0.942581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>0.203924</td>\n",
       "      <td>0.944194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.197387</td>\n",
       "      <td>0.944839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.194058</td>\n",
       "      <td>0.944194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.944839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3180, training_loss=0.4675868832090366, metrics={'train_runtime': 796.643, 'train_samples_per_second': 191.428, 'train_steps_per_second': 3.992, 'total_flos': 826980744547356.0, 'train_loss': 0.4675868832090366, 'epoch': 10.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(student_training_args, k, v)\n",
    "\n",
    "distilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\n",
    "student_training_args.output_dir = distilled_ckpt\n",
    "\n",
    "distil_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc[\"train\"],\n",
    "    eval_dataset=clinc_enc[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer\n",
    ")\n",
    "\n",
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71361c83-e4dc-42a5-bfec-f8ee66667dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10fd8a17ee541269cfe2f5887bfcf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc\n",
      "   9780787..0b339d7  main -> main\n",
      "\n",
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc\n",
      "   0b339d7..436149d  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't forget to add the knowledge-distillation hyperparameters to your model card! (see best_run)\n"
     ]
    }
   ],
   "source": [
    "distil_trainer.push_to_hub(\"Distilled student model training completed\")\n",
    "\n",
    "print(\"don't forget to add the knowledge-distillation hyperparameters to your model card! (see best_run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c58fc-9801-4fb1-b279-9d1321fe7f1a",
   "metadata": {},
   "source": [
    "### Benchmarking Our Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45535ffb-dd79-4f2f-91f6-a1aa8aeffedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/a_naughty_alpaca/.cache/huggingface/transformers/tmp4qgc78fs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6049d585824f93b2527520b9bcc759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin in cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.72045d108e33b4fb45fe53d9f2e6b4ce38ecd5054e3d4e300f15436511f91e6b\n",
      "creating metadata file for /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.72045d108e33b4fb45fe53d9f2e6b4ce38ecd5054e3d4e300f15436511f91e6b\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.72045d108e33b4fb45fe53d9f2e6b4ce38ecd5054e3d4e300f15436511f91e6b\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-distilled-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d6a6960a277dee54cae995412a5d1d91c3c01a7205ee9a021c5acf723c1009ac.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d765a80fb92038143cb9ca38cacd10d0ef078b1c6be4bf707d68bc482fca65ed.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/830798532de151eb7a28a667da288d249368317cbced033a2a391b4e43660895.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/95607bd84ec8aee89fe4703d88bbb0a4801beac0f3d0d02b83e17841172ec344.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 255.88\n",
      "Average latency (ms) - 11.74 +\\- 0.25\n",
      "Accuracy on test set - 0.871\n"
     ]
    }
   ],
   "source": [
    "distilled_ckpt = \"buruzaemon/distilbert-base-uncased-distilled-clinc\"\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=distilled_ckpt)\n",
    "optim_type = \"Distillation\"\n",
    "\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad1d0e79-6f9d-409f-b75a-4b19ac196d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69af08a-6a04-42f4-a6b0-24643e9c2853",
   "metadata": {},
   "source": [
    "## Making Models Faster with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b34fe-cbfb-475d-91fd-34f302e630da",
   "metadata": {},
   "source": [
    "#### The range of model weight values is actually pretty limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a51eeec-bd96-4aae-b6e7-8bf2654dee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_dict = pipe.model.state_dict()\n",
    "\n",
    "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
    "\n",
    "plt.hist(\n",
    "    weights.flatten().numpy(),\n",
    "    bins=250,\n",
    "    range=(-0.3, 0.3),\n",
    "    edgecolor=\"C0\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf83e0e-a475-471f-b69f-746ad441c409",
   "metadata": {},
   "source": [
    "_Discretization_ means taking the floating-point values $f$ in a tensor; map their range $\\left[ f_{max}, f_{min} \\right]$ into a smaller one $\\left[ q_{max}, q_{min} \\right]$ of fixed-point number $q$; then linearly distributing all values in between (the affine bit).\n",
    "\n",
    "\\begin{align}\n",
    "  f = \\left( \\frac{f_{max} - f_{min}}{q_{max} - q_{min}} \\right) \\left(q - Z\\right)\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "$\\left( \\frac{f_{max} - f_{min}}{q_{max} - q_{min}} \\right)$ is a scaling factor \n",
    "\n",
    "\n",
    "and _zero point_ $Z$, of the same type as $q$, is the quantized value of floating-point value $f = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bb0f809-7c94-4adf-a377-67352d6200c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_point = 0\n",
    "scale = (weights.max() - weights.min()) / (127 - (-128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb06ad-8cf8-442a-a110-606afd68cf67",
   "metadata": {},
   "source": [
    "#### Quantization the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a374a2db-936c-4346-9211-cbd55d869d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,  -8,   0,  ...,  -6,  -3,   9],\n",
       "        [  9,   2,   1,  ...,  -4,   7,   0],\n",
       "        [ -9,  -6,   5,  ...,   1,   6,  -3],\n",
       "        ...,\n",
       "        [  6,   0,  13,  ...,   0,   6,  -1],\n",
       "        [  0,  -2, -12,  ...,  12,  -7, -13],\n",
       "        [-13,  -1,  -9,  ...,   8,   2,  -1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights / scale + zero_point).clamp(-128, 127).round().char()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d861966-10f2-435f-aefc-867196abfe9e",
   "metadata": {},
   "source": [
    "#### Quantization the PyTorch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "691922d2-13e3-4ab4-8a4e-b891c3fea581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,  -8,   0,  ...,  -6,  -3,   9],\n",
       "        [  9,   2,   1,  ...,  -4,   7,   0],\n",
       "        [ -9,  -6,   5,  ...,   1,   6,  -3],\n",
       "        ...,\n",
       "        [  6,   0,  13,  ...,   0,   6,  -1],\n",
       "        [  0,  -2, -12,  ...,  12,  -7, -13],\n",
       "        [-13,  -1,  -9,  ...,   8,   2,  -1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import quantize_per_tensor\n",
    "\n",
    "dtype = torch.qint8\n",
    "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
    "quantized_weights.int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99a8aff2-98e7-4fec-ade5-e11454183fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984 Âµs Â± 20.3 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "weights @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30278aee-9146-43bc-a5c0-ea5fa591dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.quantized import QFunctional\n",
    "\n",
    "q_fn = QFunctional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5258e466-426d-4cea-9349-7baf9d26a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 Âµs Â± 7.86 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "q_fn.mul(quantized_weights, quantized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3194aad8-025d-4417-ae05-0b7b4482628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2637/2099325647.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.999755879241598"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7ed1a75-bde4-448f-8560-5517af1cb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d6a6960a277dee54cae995412a5d1d91c3c01a7205ee9a021c5acf723c1009ac.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d765a80fb92038143cb9ca38cacd10d0ef078b1c6be4bf707d68bc482fca65ed.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/830798532de151eb7a28a667da288d249368317cbced033a2a391b4e43660895.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/95607bd84ec8aee89fe4703d88bbb0a4801beac0f3d0d02b83e17841172ec344.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.72045d108e33b4fb45fe53d9f2e6b4ce38ecd5054e3d4e300f15436511f91e6b\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-distilled-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import random\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# take our last, best model...\n",
    "model_ckpt = \"buruzaemon/distilbert-base-uncased-distilled-clinc\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = (\n",
    "    AutoModelForSequenceClassification\n",
    "        .from_pretrained(model_ckpt)\n",
    "        #.to(\"cpu\")\n",
    ")\n",
    "\n",
    "model_quantized = quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97395aa2-0875-40cf-85b8-5d1df464b1f1",
   "metadata": {},
   "source": [
    "### Benchmarking Our Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87e9afea-3edc-4905-bf09-9bab941bd8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 132.39\n",
      "Average latency (ms) - 8.15 +\\- 0.27\n",
      "Accuracy on test set - 0.884\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_quantized,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "optim_type = \"Distillation + quantization\"\n",
    "\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bfafac9-0da4-4044-a54b-3e91692f1c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4c0bb-7f8f-49e1-a6b7-004a30dbd457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff77b96-779e-4e89-a221-0f3e1c95ff67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d3cb0-25cc-4b7b-bba5-9c5e4f5b5941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280d880-4862-4cf1-9f10-bb4a0e671a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f73425-081f-4193-acd0-48bae066e72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

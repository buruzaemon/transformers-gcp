{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bed5f6-166b-434e-ac4c-efee0dfa07e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Chapter 8: Making Transformers Efficient in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570a361e-fe0d-4e0b-8e3c-a6a76607ce04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".pad-left {\n",
       "    padding-left: 20px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".pad-left {\n",
    "    padding-left: 20px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597880b-f340-404f-9502-cc0770d6ee86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "> (W)hen developing a new machine learning model for your business, do you first make it accurate, then worry about making it fast in production? Or do you first make sure it can be fast, then make it accurate? \n",
    "> <p/>\n",
    "> ...\n",
    "> <p/>\n",
    "> While this was a stressful experience for us, it doesnâ€™t have to be for you, because in this article we are going to share the optimizations that made Bert inference fast for us. So you can start with an egg (a known playbook for making certain Bert models fast in production), then focus on the chicken (making your Bert model accurate).\n",
    "\n",
    "* Blogpost@Robolox: [How We Scaled BERT to Serve 1+ Billion Daily Requests on CPUs](https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26)\n",
    "* And the [video from Databricks on YouTube](https://youtu.be/Nw77sEAn_Js)\n",
    "\n",
    "#### Key takeaways\n",
    "\n",
    "1. _Smaller Model_: model distillation\n",
    "1. _Smaller Inputs_: do away with padding inputs and go with dynamically shaped input\n",
    "1. _Smaller Weights_: although this may necessarily trade off accuracy, use quantization \n",
    "1. _Smaller number of requests_: use caching\n",
    "1. _Smaller number of thread per core_: thread tuning with [`torch.set_num_threads`](https://www.theatlantic.com/ideas/archive/2024/01/the-daily-show-jon-stewart/677240/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cda04-461c-47cc-9b74-b1ddc0c22c75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Intent Detection as a Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee31293-5fb4-4c02-afe4-407bc2640b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 04:31:00.903813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=teacher_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70424d28-eac9-406f-9b9d-2b3d58ebc4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490034222602844}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\"\"\n",
    "\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eed3b-cec2-4a70-80c5-41770a222f7a",
   "metadata": {},
   "source": [
    "### CLINC150\n",
    "\n",
    "A dataset for task-oriented dialog systems, this dataset was used to fine-tune the baseline model in this example. \n",
    "\n",
    "The important thing is that it actually includes queries that are out-of-scope.\n",
    "\n",
    "Please see: [`clinc_oos` at ðŸ¤—](https://huggingface.co/datasets/clinc_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ce07bd-94bc-473e-b750-b658699b6d59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/home/a_naughty_alpaca/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3251ffd48e3d4a0aa6e5b82fa24169ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac08cd-d61b-4856-9935-a98a2039dd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58852802-a4ec-4e2d-92b0-afc81f74c548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0f8fe-188d-4d0c-b128-d9d73a34a949",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e1766-76d5-4255-bdaa-244fb222dce6",
   "metadata": {},
   "source": [
    "## Creating a Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c1b1fb-a497-4033-aed4-281233412b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def compute_size(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        # tbd\n",
    "        pass\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c54a7-1145-4e5e-a5d5-7bfcc87640af",
   "metadata": {},
   "source": [
    "#### Implementing `compute_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e411d1-2cd2-4049-a5fc-4e51bd8e8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd019f56-8a6f-4ba3-88bc-74aa4e5b7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n",
    "    preds, labels = [], []\n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred))\n",
    "        labels.append(label)\n",
    "\n",
    "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "    return accuracy\n",
    "\n",
    "PerformanceBenchmark.compute_accuracy = compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ccd7d-30ec-425c-98e0-6325eac2a68c",
   "metadata": {},
   "source": [
    "#### Implementing `compute_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0294f3-5b7e-403b-8a9c-811229becc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert.encoder.layer.2.attention.self.value.weight',\n",
       " tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\n",
       "           4.6521e-03,  2.9844e-02],\n",
       "         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\n",
       "          -2.6890e-02, -2.1943e-02],\n",
       "         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\n",
       "           3.1152e-02, -9.7786e-03],\n",
       "         ...,\n",
       "         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\n",
       "           1.1093e-02,  2.9703e-03],\n",
       "         [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\n",
       "           6.7487e-03,  1.0511e-03],\n",
       "         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\n",
       "           2.3981e-02, -4.2880e-02]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipe.model.state_dict().items())[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbcb685-e465-4904-96d5-cb4f8a927096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5eecb9-5c6d-458b-beaa-0ec42e9a16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def compute_size(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    # calculate size in megabytes\n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "    # delete tmp file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "    return {\"size_mb\": size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a6a24-9699-4c78-a55c-1e5257e09e57",
   "metadata": {},
   "source": [
    "#### Implementing `time_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9270fb87-87be-4490-b1b7-bcb673fc9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency (ms) - 56.510\n",
      "Latency (ms) - 40.367\n",
      "Latency (ms) - 36.167\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = perf_counter()\n",
    "    _ = pipe(query)\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f\"Latency (ms) - {1000 * latency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844f5595-4454-48bb-b791-3d0509d6135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline method\"\"\"\n",
    "    latencies = []\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "\n",
    "    # now we observed the elapsed time over 100 runs\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "    # compute run stats\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return { \"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms }\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb19910e-b81b-4d21-b1a6-5cd3f2330659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 418.15\n",
      "Average latency (ms) - 22.95 +\\- 0.32\n",
      "Accuracy on test set - 0.867\n"
     ]
    }
   ],
   "source": [
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbda83-15ff-471a-9b6f-f13904a12bd9",
   "metadata": {},
   "source": [
    "## Making Models Smaller via Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82239c28-989f-44d3-b2f4-3a6ec8a3954a",
   "metadata": {},
   "source": [
    "### Creating a Knowledge Distillation Trainer\n",
    "\n",
    "In addition to the _105_ parameters that [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments), we will add two more to support training of a student model with knowledge distillation:\n",
    "\n",
    "* `alpha` ... $\\alpha$ controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see below). Ranges from 0.0 to 1.0; $\\alpha = 1.0$ means that we only use the cross-entropy of the student and ignore any signal from the teacher.\n",
    "* `temperature` ... $T$ softens the probability distributions by scaling the logits before applying softmax:\n",
    "\n",
    "<p class=\"pad-left\">\\(p_{i} = \\frac{exp(z_i(x)/T)}{\\sum_\\limits{j}exp(z_{i}(x)/T)}\\)</p>\n",
    "<p>Ranges from 1.0 to $\\infty$. $T=1$ recovers the original softmax distribution. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7483a47a-87b3-4701-abdf-c2fab1d4ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edc9f0-a726-43fb-aedf-6656242ef85c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "During training, loss is calculated as a weighted average of the usual cross-entropy loss of the student; and the knowledge-distillation loss between the teacher and student. \n",
    "\n",
    "<p class=\"pad-left\">\\(L_{student} = \\alpha L_{CE} + (1 - \\alpha) L_{KD}\\)</p>\n",
    "<p>where</p>\n",
    "\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{CE}\\)</p>\n",
    "<p>is the cross-entropy loss of the ground truth labels.</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(L_{KD} = T^{2}D_{KL}\\)</p><p>is knowledge-distillation loss where \\(T^{2}\\) is a normalization factor to account for the gradients produced by soft labels scales as \\(\\frac{1}{T^{2}}\\).</p>\n",
    "\n",
    "<p class=\"pad-left\">\\(D_{KL}(p, q) = \\sum_\\limits{i} p_i \\  log\\frac{p_i(x)}{q_i(x)}\\)</p>\n",
    "<p>which is the expectation of the log difference between $p_i(x)$ and $q_i(x)$ when the expectation is taken using the probabilities of $p_i(x)$. For our case, $p_i(x)$ is the <i>teacher</i> and $q_i(x)$ is the <i>student</i>. In other words, we measure loss by seeing how far off the student is from the teacher, and that makes perfect sense.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c4cfc5-ef8b-48ed-bcd4-1794ff7fc1b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs_stu = model(**inputs)\n",
    "        # extract cross-entropy loss and logits from student\n",
    "        loss_ce = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        \n",
    "        # extract logits from teacher\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "\n",
    "        # soften probabilities and compute distillation loss\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)\n",
    "        )\n",
    "\n",
    "        # return weighted student loss\n",
    "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151b3da-d2a4-4fee-9362-853737d7dc04",
   "metadata": {},
   "source": [
    "## Choosing a Good Student Initialization\n",
    "\n",
    "> A good rule of thumb from the literature is that knowledge distillation works best when teacher and student are of the same _model type_.\n",
    "\n",
    "So if we are using [BERT (`transformersbook/bert-base-uncased-finetuned-clinc`)](https://huggingface.co/transformersbook/bert-base-uncased-finetuned-clinc) for teacher, then [DistilBERT (`distilbert-base-uncased`)](https://huggingface.co/distilbert-base-uncased) for the student is a natural choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8baac9e7-1c82-455e-8cd1-867537761400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf0be30d-9766-4890-9793-907f6d3afb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_text at 0x7f49c430f040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cdbfe1f9ea4e66bda5a59279483b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5001620dc4254fc785bf169d8056cce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aac87cf8f6148ee9e74ce23126ad1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40ddc06-303f-46ba-9e9a-699f5d572b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d22b46fd8a4eb18a259a9dbce32771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bafb8-7792-431d-9d1e-7c416b3e5aa3",
   "metadata": {},
   "source": [
    "We implement `compute_metrics` for tracking metrics during training. Here, we can reuse `accuracy_score` which we use above in `PerformanceBenchmark.compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25319fbb-cdc9-4ab4-8c51-c2361562089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9700d-f9a8-4434-a4d0-1ceb429f9965",
   "metadata": {},
   "source": [
    "#### Training arguments\n",
    "\n",
    "* [`output_dir`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.output_dir) ... output directory where the model predictions and checkpoints will be written.\n",
    "* [`evaluation_strategy`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy) ... `\"no\"`: No evaluation is done during training; `\"steps\"`: Evaluation is done (and logged) every eval_steps; or `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
    "* [`num_train_epochs`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,) ... number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training); defaults to 3.0.\n",
    "* [`learning_rate`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.learning_rate) ... initial learning rate for [`transformers.AdamW`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/optimizer_schedules#transformers.AdamW) optimizer.\n",
    "* [`weight_decay`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.weight_decay) ... weight decay to apply (if not zero) to all layers except all bias and `LayerNorm` weights in `transformers.AdamW` optimizer.\n",
    "* [`per_device_train_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size) ... batch size per GPU/TPU core/CPU for _training_; defaults to 8.\n",
    "* [`per_device_eval_batch_size`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size) ... batch size per GPU/TPU core/CPU for _evaluation_; defaults to 8.\n",
    "* `alpha` ... controls the weighted average of cross-entropy and knowledge-distillation loss for the student model (see explanation above).\n",
    "* `temperature` ... controls the softening of the probability distributions by scaling the logits before applying softmax (see explanation above).\n",
    "* [`push_to_hub`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments.push_to_hub) ... push the model to the Hub every time the model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa22bd3-7ecd-4c15-91f3-16ffd4f4d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 8675309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7068223-596f-4b60-9af8-a03a06e8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    seed=SEED,\n",
    "    output_dir=finetuned_ckpt,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # here are the 2 hyperparams for knowledge-distillation\n",
    "    alpha=1,\n",
    "    temperature=2.0,\n",
    "    # to avoid those deprecation warnings\n",
    "    optim=\"adamw_torch\",\n",
    "    #\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d0666-36e1-4ee2-af98-7fe442f177ed",
   "metadata": {},
   "source": [
    "#### Student model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f17bc5b-4200-4dc4-9c96-6df9f12f46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n",
    "\n",
    "num_labels = intents.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7454d3d7-6ddb-427b-8246-4ccb9276aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    student_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb4fb67-194e-46fb-acda-1809ff1e2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6899940a-08df-487c-b3c2-b09ab6e12cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def student_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(\n",
    "        student_ckpt,\n",
    "        config=student_config\n",
    "    ).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693a7068-eec9-4eef-9190-6ac645947295",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = (AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_ckpt,\n",
    "    num_labels=num_labels\n",
    ").to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12eb147c-5c75-4a90-8c3a-7da459af8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/a_naughty_alpaca/dev/github/transformers-gcp/distilbert-base-uncased-finetuned-clinc is already a clone of https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 06:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.306145</td>\n",
       "      <td>0.668065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>1.912227</td>\n",
       "      <td>0.827097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.803300</td>\n",
       "      <td>1.195120</td>\n",
       "      <td>0.883226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.732300</td>\n",
       "      <td>0.890680</td>\n",
       "      <td>0.903871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>0.807984</td>\n",
       "      <td>0.908387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 5s, sys: 6.51 s, total: 5min 12s\n",
      "Wall time: 6min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1590, training_loss=2.0806282571276777, metrics={'train_runtime': 392.659, 'train_samples_per_second': 194.189, 'train_steps_per_second': 4.049, 'total_flos': 413328758053176.0, 'train_loss': 2.0806282571276777, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "distilbert_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc[\"train\"],\n",
    "    eval_dataset=clinc_enc[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer\n",
    ")\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2760b1d-6252-449f-892f-e8771daf5dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ea1f756e524f45880aedbc8f573a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb01_04-33-55_t4-us-west4-b-n1-standard-16/events.out.tfevents.1706762045.t4-us-west4-b-n1-sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc\n",
      "   3b81257..f39150d  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't forget to add the knowledge-distillation hyperparameters to your model card!\n"
     ]
    }
   ],
   "source": [
    "distilbert_trainer.push_to_hub(\"Fine-tuned student model training completed\")\n",
    "\n",
    "print(\"don't forget to add the knowledge-distillation hyperparameters to your model card!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d99b5-bcaf-46ad-a6bd-4194b58016fb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237333b7-7717-4b96-a4b4-49491ababaef",
   "metadata": {},
   "source": [
    "#### ???\n",
    "\n",
    "So, how is that fine-tuned model of ours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ec9f59-7e34-4824-8cae-1d81eb711a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/f7e1cbec0db82ab08daffa0c52e1525ccdeaacb9521852321d65babd4fc65057.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-finetuned-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/5d7a292d7fb16e9bde665dc8fd3842fd30c3ad1d04184c12e7b1f984bca74af1.5b96114a0c4b9c999327ec4f9afd47e27cac15d8eee35088a0e3a85979a5929d\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-finetuned-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/e1ad7abe1ca6ca7a6d4f1cfc9de256bad2b501ebe7162a2aafff54bb220931dd.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/06f82810f2064792bf87b0151487e96479ade2ffb878fce1b686eefc65f0f8c9.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/47165d38838c8d9d5d1fb8c51331f47ae9876726f3016f84a65a3a72bbcc8221.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-finetuned-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/1402d8bf7e99ac0ebec29cb0ceb0226bca0a22660c6bc278dbec17cb2ba182b3.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    }
   ],
   "source": [
    "finetuned_ckpt = \"buruzaemon/distilbert-base-uncased-finetuned-clinc\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=finetuned_ckpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9692b5a-1cac-4ea4-9e0c-69cb8fff6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 255.88\n",
      "Average latency (ms) - 11.99 +\\- 0.19\n",
      "Accuracy on test set - 0.855\n"
     ]
    }
   ],
   "source": [
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(\n",
    "    pipe,\n",
    "    clinc[\"test\"],\n",
    "    optim_type=optim_type\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b5118-570c-4f55-a35b-02085434e1a8",
   "metadata": {},
   "source": [
    "### Comparison: base-line model (Teacher) vs. fine-tuned (Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c848824e-20fb-4e96-9453-66f3278df011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        # add a dashed circle around the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx,\n",
    "                marker='$\\u25CC$'\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                alpha=0.5,\n",
    "                s=df_opt[\"size_mb\"],\n",
    "                label=idx\n",
    "            )\n",
    "   \n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "    #for handle in legend.legendHandles:\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80, 90)\n",
    "\n",
    "    # use the slowest model to define the x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(perf_metrics, optim_type)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4a82a-2bb6-42c5-b7f0-9bb16253cd9f",
   "metadata": {},
   "source": [
    "#### Optuna\n",
    "\n",
    "The default backend for hyperparameter search in HF is Optuna. Pretty neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f07d5065-eb88-4c70-abd0-f1669de62c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -2, 2)\n",
    "    y = trial.suggest_float(\"y\", -2, 2)\n",
    "    return (1 - x)**2 + 100*(y - x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a89ec71-9bb7-4c91-90d8-8fb82a464fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-01 04:42:25,648] A new study created in memory with name: no-name-0699a764-64b4-4d2e-8215-3f2c05968209\n",
      "[I 2024-02-01 04:42:25,650] Trial 0 finished with value: 5.027235589188395 and parameters: {'x': -1.2421065441164991, 'y': 1.5442209097979012}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,652] Trial 1 finished with value: 34.2026330381243 and parameters: {'x': -0.3854191667518041, 'y': 0.7167314147687169}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,653] Trial 2 finished with value: 649.4488144705452 and parameters: {'x': 1.7967980159252854, 'y': 0.6813004950790953}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,654] Trial 3 finished with value: 1522.0095350428498 and parameters: {'x': 1.626569104433421, 'y': -1.2550638249791453}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,655] Trial 4 finished with value: 125.90382351269803 and parameters: {'x': 1.4794903370822992, 'y': 1.0678478915172351}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,656] Trial 5 finished with value: 97.64347647913233 and parameters: {'x': -1.58777404945873, 'y': 1.5673655807409541}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,657] Trial 6 finished with value: 333.27744857618364 and parameters: {'x': 0.4560806330573475, 'y': -1.6167687996713487}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,658] Trial 7 finished with value: 540.3833269447435 and parameters: {'x': -0.6301182896667314, 'y': -1.921843006877968}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,660] Trial 8 finished with value: 1118.7103538460678 and parameters: {'x': -1.4545080470300933, 'y': -1.2201007877027061}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,661] Trial 9 finished with value: 18.672556654098585 and parameters: {'x': -0.9606980101946068, 'y': 1.3080149449112741}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,671] Trial 10 finished with value: 17.10229726349232 and parameters: {'x': 0.36944120128957403, 'y': -0.2722269478227186}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,681] Trial 11 finished with value: 10.675610386934544 and parameters: {'x': 0.23723922039484657, 'y': -0.2614250652807325}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,691] Trial 12 finished with value: 1738.545080815391 and parameters: {'x': -1.999009665512609, 'y': -0.1627474662389793}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,701] Trial 13 finished with value: 56.75440358542652 and parameters: {'x': 0.20381541533586028, 'y': -0.7075954035710101}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,710] Trial 14 finished with value: 87.77942439781845 and parameters: {'x': 1.16296031606193, 'y': 0.4157116870701806}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,720] Trial 15 finished with value: 112.37189244824118 and parameters: {'x': 0.88385570432153, 'y': 1.841193372784102}. Best is trial 0 with value: 5.027235589188395.\n",
      "[I 2024-02-01 04:42:25,730] Trial 16 finished with value: 4.284450719720226 and parameters: {'x': -0.23936308248916704, 'y': 0.22307857680078258}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,740] Trial 17 finished with value: 81.99912095682322 and parameters: {'x': -1.0185409371492251, 'y': 1.9201748317664118}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,750] Trial 18 finished with value: 6.785921722436442 and parameters: {'x': -0.26186304166794266, 'y': 0.29646747822189623}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,759] Trial 19 finished with value: 11.508602376000724 and parameters: {'x': -0.8015843190128233, 'y': 0.929989961222028}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,768] Trial 20 finished with value: 670.551484189872 and parameters: {'x': -1.3402500577408643, 'y': -0.7826340342578688}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,778] Trial 21 finished with value: 6.901232126265613 and parameters: {'x': -0.2083767173043538, 'y': 0.27668160816410214}. Best is trial 16 with value: 4.284450719720226.\n",
      "[I 2024-02-01 04:42:25,788] Trial 22 finished with value: 3.644837576214333 and parameters: {'x': -0.3435597322181153, 'y': 0.2536682711580291}. Best is trial 22 with value: 3.644837576214333.\n",
      "[I 2024-02-01 04:42:25,798] Trial 23 finished with value: 55.824581333803465 and parameters: {'x': 0.754576914019902, 'y': 1.3161416261008587}. Best is trial 22 with value: 3.644837576214333.\n",
      "[I 2024-02-01 04:42:25,808] Trial 24 finished with value: 4.515092020398862 and parameters: {'x': -0.4861927899165982, 'y': 0.08451759964380232}. Best is trial 22 with value: 3.644837576214333.\n",
      "[I 2024-02-01 04:42:25,818] Trial 25 finished with value: 3.1093116199903585 and parameters: {'x': -0.41583707648231094, 'y': 0.0678149561086839}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,827] Trial 26 finished with value: 42.519297678235695 and parameters: {'x': -0.076377046731615, 'y': -0.637289475351223}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,837] Trial 27 finished with value: 30.666238411449157 and parameters: {'x': 0.05233957039155196, 'y': 0.5483416494805974}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,847] Trial 28 finished with value: 12.684135718404232 and parameters: {'x': -0.6025250916362754, 'y': 0.04497911624075397}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,857] Trial 29 finished with value: 86.48233643129203 and parameters: {'x': 0.6240920898671389, 'y': -0.5397078024646675}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,867] Trial 30 finished with value: 308.7930750106655 and parameters: {'x': -1.1586649349005342, 'y': -0.40143719812960355}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,876] Trial 31 finished with value: 3.4486562937535385 and parameters: {'x': -0.4569985321977401, 'y': 0.0937037671067214}. Best is trial 25 with value: 3.1093116199903585.\n",
      "[I 2024-02-01 04:42:25,886] Trial 32 finished with value: 1.9538772387734094 and parameters: {'x': -0.38717685408673796, 'y': 0.13269614752757886}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,896] Trial 33 finished with value: 6.811456129686645 and parameters: {'x': -0.7415071778719303, 'y': 0.7442193367266685}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,906] Trial 34 finished with value: 11.74759237429531 and parameters: {'x': -0.48869471553412347, 'y': -0.06990681307452065}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,915] Trial 35 finished with value: 32.766659008089384 and parameters: {'x': -0.011826632506688306, 'y': 0.5635479440262897}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,924] Trial 36 finished with value: 109.98015413455771 and parameters: {'x': -0.3587719832333882, 'y': -0.9111571412416865}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,934] Trial 37 finished with value: 8.652193511665141 and parameters: {'x': -0.8194627225751199, 'y': 0.9026413921880555}. Best is trial 32 with value: 1.9538772387734094.\n",
      "[I 2024-02-01 04:42:25,944] Trial 38 finished with value: 1.2859094971284135 and parameters: {'x': 0.1095362311136272, 'y': 0.08221098913433272}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:25,953] Trial 39 finished with value: 104.13829015601141 and parameters: {'x': 0.0741530704268657, 'y': -1.0107744036505035}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:25,963] Trial 40 finished with value: 52.48196296147082 and parameters: {'x': 0.5089765112479563, 'y': -0.4637212926264506}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:25,972] Trial 41 finished with value: 2.446622595429597 and parameters: {'x': -0.10899198202647681, 'y': 0.12218606876364624}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:25,983] Trial 42 finished with value: 1.356789889027484 and parameters: {'x': -0.0831370274496121, 'y': -0.03593728002319807}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:25,993] Trial 43 finished with value: 5.180594650985104 and parameters: {'x': 0.26281239778571297, 'y': -0.14627005060342238}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,003] Trial 44 finished with value: 12.478663696687255 and parameters: {'x': -0.1215984792560941, 'y': -0.32018665274211067}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,013] Trial 45 finished with value: 22.77620716081017 and parameters: {'x': 0.14513537940632976, 'y': 0.49058971580548344}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,022] Trial 46 finished with value: 11.636227503319033 and parameters: {'x': 1.0436918217392028, 'y': 0.7482014501896183}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,032] Trial 47 finished with value: 5.670750784691001 and parameters: {'x': 0.4475242623103279, 'y': -0.031357984874886585}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,042] Trial 48 finished with value: 2.5831519584999287 and parameters: {'x': -0.6004887879976908, 'y': 0.3752795031135671}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,051] Trial 49 finished with value: 2.758195781734168 and parameters: {'x': -0.6480123239051636, 'y': 0.3993648844870682}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,061] Trial 50 finished with value: 8.171852420221862 and parameters: {'x': -0.9772919855604005, 'y': 1.161549832695135}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,071] Trial 51 finished with value: 2.6578543775918626 and parameters: {'x': -0.6044011200357241, 'y': 0.33636087568043793}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,081] Trial 52 finished with value: 4.494628421436827 and parameters: {'x': -0.12402944881456462, 'y': -0.16437170186648853}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,091] Trial 53 finished with value: 7.692701766664155 and parameters: {'x': -0.6517987242004875, 'y': 0.6476478321988464}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,101] Trial 54 finished with value: 827.4939476810227 and parameters: {'x': -1.7442706957978569, 'y': 0.17898078225654068}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,111] Trial 55 finished with value: 914.1323567764891 and parameters: {'x': 1.8419483292335297, 'y': 0.37048398608867156}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,121] Trial 56 finished with value: 12.052099669135869 and parameters: {'x': 0.302839668833508, 'y': -0.24837734595028166}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,131] Trial 57 finished with value: 158.07117774062576 and parameters: {'x': -1.1894417460351154, 'y': 0.17671866922856838}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,141] Trial 58 finished with value: 73.66081136224169 and parameters: {'x': -0.2341601438576995, 'y': 0.9041698929648545}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,152] Trial 59 finished with value: 3.8450584689285545 and parameters: {'x': -0.8482217292938657, 'y': 0.6539717126373961}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,162] Trial 60 finished with value: 378.668869095687 and parameters: {'x': -0.568348874113872, 'y': -1.616590732000303}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,172] Trial 61 finished with value: 4.105484310354 and parameters: {'x': -0.7121633683226588, 'y': 0.39882626352622025}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,182] Trial 62 finished with value: 11.64889431236073 and parameters: {'x': -0.29783054005156523, 'y': 0.404369471655668}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,193] Trial 63 finished with value: 84.53196539375121 and parameters: {'x': -0.8984005581801333, 'y': -0.0924765744251792}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,203] Trial 64 finished with value: 70.61779932494998 and parameters: {'x': -1.0602687453610695, 'y': 0.3094729852087852}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,213] Trial 65 finished with value: 5.706764366922734 and parameters: {'x': -0.5508941659043682, 'y': 0.4851844550347892}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,223] Trial 66 finished with value: 3.1337759025683045 and parameters: {'x': -0.14579938412443608, 'y': 0.15619891712131262}. Best is trial 38 with value: 1.2859094971284135.\n",
      "[I 2024-02-01 04:42:26,233] Trial 67 finished with value: 1.0416138918279985 and parameters: {'x': -0.02046945159843394, 'y': 0.0020189677684145585}. Best is trial 67 with value: 1.0416138918279985.\n",
      "[I 2024-02-01 04:42:26,244] Trial 68 finished with value: 1.0009216191206671 and parameters: {'x': 0.004563521474480414, 'y': -0.009993082770125526}. Best is trial 68 with value: 1.0009216191206671.\n",
      "[I 2024-02-01 04:42:26,254] Trial 69 finished with value: 0.7580559902644574 and parameters: {'x': 0.14234489673172188, 'y': 0.005267499240809742}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,264] Trial 70 finished with value: 12.246666485213055 and parameters: {'x': 0.1594458567299123, 'y': -0.3142847640093104}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,274] Trial 71 finished with value: 1.0262225762441628 and parameters: {'x': -0.011591479381842962, 'y': 0.005524404218046397}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,285] Trial 72 finished with value: 0.8741904494542548 and parameters: {'x': 0.06506190546481705, 'y': 0.005134211024581406}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,296] Trial 73 finished with value: 30.61111579142717 and parameters: {'x': 0.011222141888926049, 'y': -0.5442400630091333}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,307] Trial 74 finished with value: 3.541131029414982 and parameters: {'x': 0.41724545795372403, 'y': -0.004834374656421456}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,318] Trial 75 finished with value: 44.71658558582344 and parameters: {'x': 0.6583025027219425, 'y': -0.23446884448395555}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,329] Trial 76 finished with value: 0.8335138163020681 and parameters: {'x': 0.12817914583245885, 'y': -0.010670330657725024}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,340] Trial 77 finished with value: 27.28556757556652 and parameters: {'x': 0.32826695837854947, 'y': -0.41025955173200096}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,350] Trial 78 finished with value: 11.96416931137651 and parameters: {'x': 0.5424074033423931, 'y': -0.04864663079548396}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,360] Trial 79 finished with value: 0.8420786947352122 and parameters: {'x': 0.09379184813640096, 'y': 0.023241799226107196}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,370] Trial 80 finished with value: 4.731703208814615 and parameters: {'x': 0.12544177575777746, 'y': -0.18343391482366572}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,381] Trial 81 finished with value: 1.0170743700864588 and parameters: {'x': 0.0036798251578566456, 'y': 0.015640594350760813}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,391] Trial 82 finished with value: 1.0086781903899658 and parameters: {'x': 0.051985663503485394, 'y': 0.035860767334690465}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,402] Trial 83 finished with value: 12.693084379730097 and parameters: {'x': -0.0024956458926002623, 'y': -0.3418722130095276}. Best is trial 69 with value: 0.7580559902644574.\n",
      "[I 2024-02-01 04:42:26,412] Trial 84 finished with value: 0.6294901468518385 and parameters: {'x': 0.22329368825871157, 'y': 0.03366826706930147}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,422] Trial 85 finished with value: 4.448593208846967 and parameters: {'x': 0.22301091880946872, 'y': 0.24581755552634021}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,432] Trial 86 finished with value: 7.691902928122482 and parameters: {'x': 0.3890816014868539, 'y': -0.11914612743473675}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,443] Trial 87 finished with value: 36.57232310188841 and parameters: {'x': 0.262855996992406, 'y': -0.5311478582300234}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,453] Trial 88 finished with value: 34.049306863622064 and parameters: {'x': 0.8073621282987712, 'y': 0.06863383328585292}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,463] Trial 89 finished with value: 1.3996677028157736 and parameters: {'x': 0.5837911441519092, 'y': 0.23006744363471254}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,474] Trial 90 finished with value: 628.7110891087264 and parameters: {'x': 1.5210701015486872, 'y': -0.19321545770336596}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,484] Trial 91 finished with value: 0.9297772932595028 and parameters: {'x': 0.039454957170624846, 'y': 0.010000930707034592}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,494] Trial 92 finished with value: 1.4745240093464518 and parameters: {'x': -0.21145497485170756, 'y': 0.0530203438348301}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,504] Trial 93 finished with value: 1.8128793108799446 and parameters: {'x': 0.0808026688993812, 'y': -0.09185566213728648}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,514] Trial 94 finished with value: 4.6922184547790975 and parameters: {'x': 0.1748448104868204, 'y': 0.23085394140088972}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,526] Trial 95 finished with value: 28.59420267971831 and parameters: {'x': 0.36425493990347496, 'y': -0.39826118782991154}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,537] Trial 96 finished with value: 2.495529053041328 and parameters: {'x': 0.05440695874890447, 'y': 0.12950587384059958}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,548] Trial 97 finished with value: 8.954214804401245 and parameters: {'x': -0.05724591063250614, 'y': -0.2766594322509268}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,560] Trial 98 finished with value: 1.3492594598718655 and parameters: {'x': -0.1596327908697498, 'y': 0.0187660438102475}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,571] Trial 99 finished with value: 4.057431927974591 and parameters: {'x': -0.3059496317453745, 'y': -0.05975477482099652}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,581] Trial 100 finished with value: 20.89107616406787 and parameters: {'x': 0.46877116939249475, 'y': -0.23422354673025822}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,592] Trial 101 finished with value: 1.066549380910845 and parameters: {'x': -0.032708828207952484, 'y': 0.0002833748985708361}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,602] Trial 102 finished with value: 5.554705390104116 and parameters: {'x': 0.24731385687759955, 'y': -0.16217794717014247}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,613] Trial 103 finished with value: 7.111975085750942 and parameters: {'x': 0.17745713759267948, 'y': 0.28517190459574965}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,623] Trial 104 finished with value: 1.639281452421732 and parameters: {'x': -0.003872357827890055, 'y': 0.07948333738089174}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,634] Trial 105 finished with value: 2.570473881033074 and parameters: {'x': 0.10888741577831429, 'y': 0.14513783497695631}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,645] Trial 106 finished with value: 268.47736305124016 and parameters: {'x': -0.2348568957277867, 'y': 1.6890258451860793}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,656] Trial 107 finished with value: 17.269538379666248 and parameters: {'x': 0.2870795077598316, 'y': 0.4918200999118429}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,666] Trial 108 finished with value: 2.953357634962833 and parameters: {'x': -0.07952467263938433, 'y': -0.12739134970369662}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,677] Trial 109 finished with value: 4.444719739252217 and parameters: {'x': -0.3993570075732923, 'y': 0.0017989970576272798}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,688] Trial 110 finished with value: 4.8876439016334965 and parameters: {'x': 0.023381524443254115, 'y': 0.19888630985338132}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,699] Trial 111 finished with value: 1.0534621836982627 and parameters: {'x': -0.024163946386217695, 'y': -0.0061617649827690555}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,709] Trial 112 finished with value: 2.122049601669722 and parameters: {'x': 0.1991363384655463, 'y': -0.08202737964086917}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,720] Trial 113 finished with value: 1.36935701790044 and parameters: {'x': -0.1561610533790162, 'y': 0.006317340880140014}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,730] Trial 114 finished with value: 9.760734614051788 and parameters: {'x': 0.1186377309392728, 'y': 0.3138070443601425}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,740] Trial 115 finished with value: 21.094990713561355 and parameters: {'x': 0.3207825502326904, 'y': -0.3513413800599848}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,750] Trial 116 finished with value: 2.38500660441132 and parameters: {'x': 0.055034469683479956, 'y': 0.12517815842373434}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,760] Trial 117 finished with value: 4.430471914119414 and parameters: {'x': -0.07975131095864192, 'y': -0.17432201919287676}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,771] Trial 118 finished with value: 12.108829212967649 and parameters: {'x': -0.2872373625834973, 'y': -0.240787895505609}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,781] Trial 119 finished with value: 1.5316705728139521 and parameters: {'x': -0.17928109473677548, 'y': 0.06968723961325463}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,791] Trial 120 finished with value: 39.29919487718679 and parameters: {'x': 0.39724740194803315, 'y': -0.4661807529582442}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,802] Trial 121 finished with value: 1.0506105548049367 and parameters: {'x': -0.016434604738335565, 'y': -0.012947789034927965}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,812] Trial 122 finished with value: 1.5969618366940033 and parameters: {'x': -0.024159273427740574, 'y': -0.07344737835493508}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,823] Trial 123 finished with value: 3.148305130996472 and parameters: {'x': 0.21762133632837366, 'y': 0.20661320633608665}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,833] Trial 124 finished with value: 1.0278845583580645 and parameters: {'x': 0.08888919205370215, 'y': -0.03656911225399052}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,844] Trial 125 finished with value: 1690.41767603537 and parameters: {'x': 1.9968874694169596, 'y': -0.12270063477539828}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,855] Trial 126 finished with value: 2.178096561863055 and parameters: {'x': 0.07971279218509242, 'y': 0.12173038403048309}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,866] Trial 127 finished with value: 10.223034669705225 and parameters: {'x': 0.12791125400850445, 'y': -0.291250412579242}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,876] Trial 128 finished with value: 3.1632327952701713 and parameters: {'x': 0.3161103123392194, 'y': 0.2641063523696907}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,886] Trial 129 finished with value: 1.527394078527462 and parameters: {'x': -0.11194067521290844, 'y': 0.06647346343035956}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,897] Trial 130 finished with value: 1.2535074885114106 and parameters: {'x': 0.20355524530881525, 'y': -0.03725345952189365}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,907] Trial 131 finished with value: 1.0460996156016427 and parameters: {'x': -0.020212691067613697, 'y': 0.007665053818126634}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,918] Trial 132 finished with value: 4.221474302586069 and parameters: {'x': 0.013816350369307995, 'y': 0.18043839115367075}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,928] Trial 133 finished with value: 7.0276360688449575 and parameters: {'x': -0.2059321867411144, 'y': -0.19367165891800361}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,938] Trial 134 finished with value: 0.9070805776360062 and parameters: {'x': 0.06522198561824777, 'y': 0.02249414903591358}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,949] Trial 135 finished with value: 1.1341961193676222 and parameters: {'x': 0.10816078081784475, 'y': 0.06990690913310466}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,959] Trial 136 finished with value: 3.0300058693118084 and parameters: {'x': 0.2661475330550222, 'y': -0.0870092876782759}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,970] Trial 137 finished with value: 7.056595321887562 and parameters: {'x': -0.3496913058008203, 'y': 0.3510836750196859}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,980] Trial 138 finished with value: 2.8516569618123944 and parameters: {'x': -0.10983122439674427, 'y': 0.13933943202774032}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:26,990] Trial 139 finished with value: 3.4304161379374514 and parameters: {'x': 0.16266408293317253, 'y': -0.13874586515911141}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:27,001] Trial 140 finished with value: 0.9890528013319206 and parameters: {'x': 0.049896612123296005, 'y': 0.03187612348661663}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:27,012] Trial 141 finished with value: 1.0296679367421904 and parameters: {'x': 0.04219917699104723, 'y': 0.03528980218476653}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:27,022] Trial 142 finished with value: 1.1312142647144563 and parameters: {'x': 0.06295267155350326, 'y': 0.05427770560069331}. Best is trial 84 with value: 0.6294901468518385.\n",
      "[I 2024-02-01 04:42:27,032] Trial 143 finished with value: 0.28189082219715644 and parameters: {'x': 0.4733215844869867, 'y': 0.21732461991095167}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,042] Trial 144 finished with value: 0.3737100745099642 and parameters: {'x': 0.48000484496173335, 'y': 0.26254731959598665}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,053] Trial 145 finished with value: 5.0515464596390744 and parameters: {'x': 0.6887316564087623, 'y': 0.2517606730408734}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,064] Trial 146 finished with value: 0.32879125179372853 and parameters: {'x': 0.4280983506009419, 'y': 0.17912120464960452}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,074] Trial 147 finished with value: 5.5068290450306385 and parameters: {'x': 0.5032196258549708, 'y': 0.48257772575408525}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,085] Trial 148 finished with value: 7.3604148030906495 and parameters: {'x': 0.5605654213688992, 'y': 0.5819519521216638}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,095] Trial 149 finished with value: 4.4992612991125425 and parameters: {'x': 0.35330804020983386, 'y': 0.3268426797498593}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,106] Trial 150 finished with value: 0.3230126560426855 and parameters: {'x': 0.4339005405609231, 'y': 0.1933135447450419}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,116] Trial 151 finished with value: 0.3596910738334754 and parameters: {'x': 0.4746424718259498, 'y': 0.19635615850526558}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,126] Trial 152 finished with value: 9.419222043387503 and parameters: {'x': 0.7061674994299718, 'y': 0.19317483668706256}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,137] Trial 153 finished with value: 3.6426470992943454 and parameters: {'x': 0.4745739738043574, 'y': 0.40870273342116326}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,147] Trial 154 finished with value: 1.2442110780666251 and parameters: {'x': 0.5969339537954688, 'y': 0.2523229896485308}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,158] Trial 155 finished with value: 0.3818477945216212 and parameters: {'x': 0.41288697246997397, 'y': 0.15120233176460415}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,169] Trial 156 finished with value: 36.66088422277242 and parameters: {'x': 0.8703632113631115, 'y': 0.15218859304349544}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,179] Trial 157 finished with value: 2.340249739182488 and parameters: {'x': 0.39640794719527306, 'y': 0.2977069088324929}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,189] Trial 158 finished with value: 0.31852936488923933 and parameters: {'x': 0.44915586597060203, 'y': 0.1894527435274065}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,200] Trial 159 finished with value: 2.4477320337103374 and parameters: {'x': 0.4538241586061226, 'y': 0.3525655066219575}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,210] Trial 160 finished with value: 3.453483612018468 and parameters: {'x': 0.524160786021476, 'y': 0.4543847437658162}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,221] Trial 161 finished with value: 6.941047639131609 and parameters: {'x': 0.6199204323011558, 'y': 0.12359869258300718}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,231] Trial 162 finished with value: 0.3552098264162484 and parameters: {'x': 0.4164855471608173, 'y': 0.18559310419540187}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,241] Trial 163 finished with value: 1.325130603101627 and parameters: {'x': 0.38793710883649557, 'y': 0.24798928332836828}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,251] Trial 164 finished with value: 2.1435383949877336 and parameters: {'x': 0.26787418154212245, 'y': 0.19854499068222958}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,262] Trial 165 finished with value: 487.55550535988476 and parameters: {'x': 0.4751124838660175, 'y': -1.9817100826426564}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,272] Trial 166 finished with value: 27.54049007626329 and parameters: {'x': 0.7949911571148198, 'y': 0.10762118550379494}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,282] Trial 167 finished with value: 1.1238438702967362 and parameters: {'x': 0.311126103253461, 'y': 0.17737839631571192}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,293] Trial 168 finished with value: 4.906842607010575 and parameters: {'x': 0.413700826381918, 'y': 0.38476241556340945}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,304] Trial 169 finished with value: 53.80845599043364 and parameters: {'x': 1.0207845518068555, 'y': 0.30846157266908175}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,315] Trial 170 finished with value: 5.403066902577515 and parameters: {'x': 0.5718302183824476, 'y': 0.09852234931762562}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,326] Trial 171 finished with value: 1.680036256331465 and parameters: {'x': 0.18257652375700717, 'y': -0.06725682235328227}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,336] Trial 172 finished with value: 0.9405248561784614 and parameters: {'x': 0.3444772238450582, 'y': 0.19013586290752227}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,347] Trial 173 finished with value: 1.5869163995165732 and parameters: {'x': 0.35283317866852015, 'y': 0.2325695340099103}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,357] Trial 174 finished with value: 1.4151523136143818 and parameters: {'x': 0.5124509532398165, 'y': 0.15409569237800125}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,368] Trial 175 finished with value: 0.6655901746247299 and parameters: {'x': 0.24681763777133464, 'y': 0.09227281431442136}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,378] Trial 176 finished with value: 0.5181230527112688 and parameters: {'x': 0.2879282780912525, 'y': 0.09342738598955072}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,389] Trial 177 finished with value: 6.985553572709856 and parameters: {'x': 0.2675802049093995, 'y': 0.32555024041166725}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,399] Trial 178 finished with value: 4.567978550922248 and parameters: {'x': 0.6509087002764059, 'y': 0.21282403652665025}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,410] Trial 179 finished with value: 13.493206366389343 and parameters: {'x': 0.4344766640302662, 'y': 0.5517216174415612}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,420] Trial 180 finished with value: 211.70355020762776 and parameters: {'x': 0.23774707737915904, 'y': -1.3964819042330205}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,430] Trial 181 finished with value: 0.7726984032370204 and parameters: {'x': 0.36897158663268975, 'y': 0.07494350040588887}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,441] Trial 182 finished with value: 0.4669013909269346 and parameters: {'x': 0.3389843772494879, 'y': 0.09760152665540091}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,451] Trial 183 finished with value: 0.7280112972391328 and parameters: {'x': 0.43135674580306127, 'y': 0.12245605324670929}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,462] Trial 184 finished with value: 1.9512807624846087 and parameters: {'x': 0.4684762858315613, 'y': 0.09028940862919851}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,472] Trial 185 finished with value: 4.657741827379195 and parameters: {'x': 0.5603331933781717, 'y': 0.10268119294248894}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,483] Trial 186 finished with value: 7.182746865937412 and parameters: {'x': 0.3979460034896496, 'y': -0.1027955951330582}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,493] Trial 187 finished with value: 4.045201875339903 and parameters: {'x': 0.32398300579493655, 'y': 0.2943905116379153}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,503] Trial 188 finished with value: 14.535808817449306 and parameters: {'x': 0.7313557452475028, 'y': 0.15457031274960614}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,514] Trial 189 finished with value: 0.5965418755819205 and parameters: {'x': 0.2535999758268989, 'y': 0.0841696542881531}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,524] Trial 190 finished with value: 15.751215390565308 and parameters: {'x': 0.2071907597185042, 'y': 0.43180676964210135}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,534] Trial 191 finished with value: 0.5413720317003106 and parameters: {'x': 0.298298133291889, 'y': 0.06684887729283992}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,544] Trial 192 finished with value: 0.9395230461342348 and parameters: {'x': 0.43270110546168294, 'y': 0.10863667120577969}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,555] Trial 193 finished with value: 2.5143856812240917 and parameters: {'x': 0.306440852974319, 'y': 0.23650197775964282}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,565] Trial 194 finished with value: 0.8714190156611217 and parameters: {'x': 0.388160394178395, 'y': 0.08016520711720951}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,576] Trial 195 finished with value: 5.2130491808020745 and parameters: {'x': 0.5481584464216905, 'y': 0.07667222241291216}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,586] Trial 196 finished with value: 0.3544598423886235 and parameters: {'x': 0.40530909920301356, 'y': 0.16710844118414125}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,597] Trial 197 finished with value: 0.302030679625844 and parameters: {'x': 0.48932552195080936, 'y': 0.25974765618312884}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,608] Trial 198 finished with value: 2.214055688191866 and parameters: {'x': 0.6460954839556383, 'y': 0.27291230835125324}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,619] Trial 199 finished with value: 1.8744139979328138 and parameters: {'x': 0.49822552605026926, 'y': 0.3756114199200051}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,629] Trial 200 finished with value: 0.3409076899919378 and parameters: {'x': 0.44725409746095746, 'y': 0.18122674678352316}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,639] Trial 201 finished with value: 0.4594699657516603 and parameters: {'x': 0.45585011309630974, 'y': 0.1673801635287518}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,650] Trial 202 finished with value: 0.38746925891472345 and parameters: {'x': 0.4746763783858475, 'y': 0.19192539690567242}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,660] Trial 203 finished with value: 0.37604930150229265 and parameters: {'x': 0.47914700173537184, 'y': 0.19721497489385484}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,671] Trial 204 finished with value: 2.487738262190415 and parameters: {'x': 0.5922439822233648, 'y': 0.1983891168537743}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,681] Trial 205 finished with value: 0.41668837766127165 and parameters: {'x': 0.49896078414764194, 'y': 0.2896617513209218}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,691] Trial 206 finished with value: 0.7704475795074444 and parameters: {'x': 0.4996388948971847, 'y': 0.3217560374346099}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,702] Trial 207 finished with value: 2.5187094937576626 and parameters: {'x': 0.647937276082698, 'y': 0.26507254953123394}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,713] Trial 208 finished with value: 5.311059995564763 and parameters: {'x': 0.48266874667261606, 'y': 0.4575449051067702}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,723] Trial 209 finished with value: 1.0494688960789331 and parameters: {'x': 0.5377576802414711, 'y': 0.1977611735971404}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,734] Trial 210 finished with value: 4.638749103771077 and parameters: {'x': 0.32069319068443264, 'y': 0.3072283529261288}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,744] Trial 211 finished with value: 0.38048776320447386 and parameters: {'x': 0.4403179966079243, 'y': 0.1679485250855559}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,755] Trial 212 finished with value: 4.356715010192588 and parameters: {'x': 0.4303145646286441, 'y': 0.385973351478121}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,766] Trial 213 finished with value: 2.420794055597571 and parameters: {'x': 0.5904752424311686, 'y': 0.19855826283087583}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,776] Trial 214 finished with value: 0.41980743052970726 and parameters: {'x': 0.4644749886380507, 'y': 0.25220897590812963}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,786] Trial 215 finished with value: 0.3075295140692985 and parameters: {'x': 0.48513321716115637, 'y': 0.25595562426847657}. Best is trial 143 with value: 0.28189082219715644.\n",
      "[I 2024-02-01 04:42:27,796] Trial 216 finished with value: 0.22619913969338767 and parameters: {'x': 0.5245170940200471, 'y': 0.274045121742698}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,807] Trial 217 finished with value: 3.007752640102375 and parameters: {'x': 0.7093358154595559, 'y': 0.33218165837465696}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,817] Trial 218 finished with value: 0.30120124724630654 and parameters: {'x': 0.5436133756842889, 'y': 0.2650339510169307}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,828] Trial 219 finished with value: 0.8261512443259438 and parameters: {'x': 0.5703049702562935, 'y': 0.4053422923543981}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,839] Trial 220 finished with value: 0.33360994132136984 and parameters: {'x': 0.4991789691964529, 'y': 0.27795258829306224}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,850] Trial 221 finished with value: 0.313104322510268 and parameters: {'x': 0.49856163130524417, 'y': 0.2733959141487026}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,863] Trial 222 finished with value: 2.40672637629067 and parameters: {'x': 0.6433770749400138, 'y': 0.2629523906241563}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,875] Trial 223 finished with value: 0.2646138945936394 and parameters: {'x': 0.5035355089863416, 'y': 0.2670153410463969}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,886] Trial 224 finished with value: 1.0769477040118616 and parameters: {'x': 0.5146962413893803, 'y': 0.356641603646031}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,897] Trial 225 finished with value: 6.997457592837981 and parameters: {'x': 0.48334293456124267, 'y': 0.4930529090409471}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,908] Trial 226 finished with value: 1.2605995529990226 and parameters: {'x': 0.6010586085703976, 'y': 0.256321685848554}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,919] Trial 227 finished with value: 7.05331455121231 and parameters: {'x': 0.7477498868953945, 'y': 0.2947497800369655}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,930] Trial 228 finished with value: 3.5879094901240096 and parameters: {'x': 0.4514305988869851, 'y': 0.38508991937526177}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,941] Trial 229 finished with value: 0.7205721845482079 and parameters: {'x': 0.5202440494545344, 'y': 0.20062484754796708}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,952] Trial 230 finished with value: 0.7499519423965418 and parameters: {'x': 0.42848287743445707, 'y': 0.2486606319013953}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,963] Trial 231 finished with value: 1.385930209251832 and parameters: {'x': 0.5621135315091792, 'y': 0.2066928206118669}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,974] Trial 232 finished with value: 3.5599850793401786 and parameters: {'x': 0.3954033672075079, 'y': 0.3350740102592447}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,985] Trial 233 finished with value: 0.7881203756166912 and parameters: {'x': 0.49325023790650296, 'y': 0.1704037500130561}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:27,996] Trial 234 finished with value: 1.4296156315061774 and parameters: {'x': 0.6551024711175889, 'y': 0.31467512972319217}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,007] Trial 235 finished with value: 0.34518233279387317 and parameters: {'x': 0.4333100907787968, 'y': 0.1722512233287682}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,018] Trial 236 finished with value: 0.532493002773816 and parameters: {'x': 0.45501010507452233, 'y': 0.2555603759943872}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,029] Trial 237 finished with value: 1.8336996956925296 and parameters: {'x': 0.5508899840790735, 'y': 0.17573002351027253}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,040] Trial 238 finished with value: 8.427035507482552 and parameters: {'x': 0.3879821480554815, 'y': 0.4342988850295373}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,050] Trial 239 finished with value: 1.6436174608282956 and parameters: {'x': 0.6150813900701069, 'y': 0.25603631379324887}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,061] Trial 240 finished with value: 1.1383515044280696 and parameters: {'x': 0.5192853718417619, 'y': 0.17440684068012852}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,072] Trial 241 finished with value: 2.2716026219182304 and parameters: {'x': 0.376092225257561, 'y': 0.27864382072923677}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,083] Trial 242 finished with value: 0.6668866790262548 and parameters: {'x': 0.454704428657795, 'y': 0.14596636346052316}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,094] Trial 243 finished with value: 3.0864385749590255 and parameters: {'x': 0.4327114677255024, 'y': 0.35351074812787436}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,106] Trial 244 finished with value: 0.6834987056211532 and parameters: {'x': 0.3620253242612116, 'y': 0.1836443683394764}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,117] Trial 245 finished with value: 0.2571697314124783 and parameters: {'x': 0.5108699402676193, 'y': 0.2476009691875323}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,128] Trial 246 finished with value: 0.40021452331809015 and parameters: {'x': 0.5644710476779893, 'y': 0.27274411839419427}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,139] Trial 247 finished with value: 0.8622144422135773 and parameters: {'x': 0.6809269847631872, 'y': 0.3764602482008443}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,151] Trial 248 finished with value: 0.3913926437272881 and parameters: {'x': 0.5753510897234985, 'y': 0.28508696238834347}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,162] Trial 249 finished with value: 1.7436807790211553 and parameters: {'x': 0.5733918054173175, 'y': 0.4537456073738232}. Best is trial 216 with value: 0.22619913969338767.\n",
      "[I 2024-02-01 04:42:28,173] Trial 250 finished with value: 0.18525761268476287 and parameters: {'x': 0.6079642370327942, 'y': 0.3518538105979792}. Best is trial 250 with value: 0.18525761268476287.\n",
      "[I 2024-02-01 04:42:28,184] Trial 251 finished with value: 13.045256572747347 and parameters: {'x': 0.8434477149458365, 'y': 0.35056131264555257}. Best is trial 250 with value: 0.18525761268476287.\n",
      "[I 2024-02-01 04:42:28,195] Trial 252 finished with value: 0.072281188568281 and parameters: {'x': 0.7388261806559584, 'y': 0.5394849209952403}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,207] Trial 253 finished with value: 14.99215086514841 and parameters: {'x': 0.9730301973727399, 'y': 0.5596001684146491}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,218] Trial 254 finished with value: 0.1491056371390014 and parameters: {'x': 0.6952214493551645, 'y': 0.4596230192311349}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,229] Trial 255 finished with value: 2.3990508776157524 and parameters: {'x': 0.7376504924841443, 'y': 0.39147754930864576}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,241] Trial 256 finished with value: 0.1788750156368463 and parameters: {'x': 0.8122204070321248, 'y': 0.6218055728688654}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,253] Trial 257 finished with value: 44.1724370258691 and parameters: {'x': 1.195567903956245, 'y': 0.7650469280879704}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,265] Trial 258 finished with value: 0.3488750502604999 and parameters: {'x': 0.7930822616957328, 'y': 0.6843021725788208}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,276] Trial 259 finished with value: 0.7990295394987267 and parameters: {'x': 0.7669086991231612, 'y': 0.6744448382636262}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,288] Trial 260 finished with value: 3.7331093958332704 and parameters: {'x': 0.7730851172527711, 'y': 0.7895360537437988}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,300] Trial 261 finished with value: 0.9517372222830657 and parameters: {'x': 0.8744368091084764, 'y': 0.6678941333594731}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,312] Trial 262 finished with value: 1.6468981378818088 and parameters: {'x': 0.6913867322781686, 'y': 0.6025810979095608}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,324] Trial 263 finished with value: 19.65029760564357 and parameters: {'x': 0.9634284408099434, 'y': 0.48492288475120304}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,336] Trial 264 finished with value: 0.9089443729176945 and parameters: {'x': 0.7981847221302875, 'y': 0.5439207970815043}. Best is trial 252 with value: 0.072281188568281.\n",
      "[I 2024-02-01 04:42:28,348] Trial 265 finished with value: 0.005947979141609836 and parameters: {'x': 0.9397935216565853, 'y': 0.8880317794331529}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,359] Trial 266 finished with value: 0.759432878173911 and parameters: {'x': 0.945148573758358, 'y': 0.9802784773059325}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,371] Trial 267 finished with value: 15.638558386908565 and parameters: {'x': 0.8958324082044075, 'y': 1.197834657090504}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,383] Trial 268 finished with value: 8.825751941778593 and parameters: {'x': 1.0576304043735247, 'y': 0.8215563043786727}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,395] Trial 269 finished with value: 4.445762908045229 and parameters: {'x': 0.8228279678185397, 'y': 0.8871499573750509}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,406] Trial 270 finished with value: 3.5551033740992404 and parameters: {'x': 0.6498001902720033, 'y': 0.6075093744822166}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,418] Trial 271 finished with value: 26.715976461257036 and parameters: {'x': 1.1065881473732946, 'y': 0.7077722417402745}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,430] Trial 272 finished with value: 0.14221294230029174 and parameters: {'x': 0.6994633233240096, 'y': 0.5120284596064821}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,442] Trial 273 finished with value: 0.3337952864381972 and parameters: {'x': 0.6906840047973722, 'y': 0.525841822852678}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,454] Trial 274 finished with value: 0.10623179842494228 and parameters: {'x': 0.7181501234013606, 'y': 0.49937120168712834}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,465] Trial 275 finished with value: 0.30218691003035153 and parameters: {'x': 0.7396709994681832, 'y': 0.5955296847989117}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,477] Trial 276 finished with value: 0.45576944932539243 and parameters: {'x': 0.7355271769375152, 'y': 0.6031149263697566}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,488] Trial 277 finished with value: 4.2357795962603975 and parameters: {'x': 0.8469361385272808, 'y': 0.5120606938561062}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,500] Trial 278 finished with value: 0.2860382332362659 and parameters: {'x': 0.7091850794420603, 'y': 0.5478283201375833}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,511] Trial 279 finished with value: 2.0505649035585694 and parameters: {'x': 0.7114970390784146, 'y': 0.6464896117225111}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,522] Trial 280 finished with value: 9.551872476479836 and parameters: {'x': 0.9246569793993823, 'y': 0.5460213416751626}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,533] Trial 281 finished with value: 0.908411300078553 and parameters: {'x': 0.7673331770915712, 'y': 0.4963730739301323}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,545] Trial 282 finished with value: 5.946131737703719 and parameters: {'x': 0.7001578136648283, 'y': 0.7322173772530811}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,557] Trial 283 finished with value: 0.40092824507408936 and parameters: {'x': 0.8264086093321733, 'y': 0.6220583103427397}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,569] Trial 284 finished with value: 1.4273622196961187 and parameters: {'x': 0.6416787001218491, 'y': 0.5257238344054653}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,581] Trial 285 finished with value: 0.19581632030723628 and parameters: {'x': 0.6751837607550092, 'y': 0.48592485466845303}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,593] Trial 286 finished with value: 1.8278058309384224 and parameters: {'x': 0.6799888894846949, 'y': 0.593739323325468}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,604] Trial 287 finished with value: 0.27443804547711304 and parameters: {'x': 0.6374989395445123, 'y': 0.44422434088900964}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,616] Trial 288 finished with value: 0.3012867950305793 and parameters: {'x': 0.6308960125542618, 'y': 0.43865600682073147}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,628] Trial 289 finished with value: 0.21620806279553 and parameters: {'x': 0.6406746181865209, 'y': 0.4399755459429392}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,639] Trial 290 finished with value: 0.22417664071323307 and parameters: {'x': 0.6429184432001153, 'y': 0.4444358280832864}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,651] Trial 291 finished with value: 0.4839573346612155 and parameters: {'x': 0.6324609713931766, 'y': 0.4590723014474464}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,662] Trial 292 finished with value: 2.029555440176688 and parameters: {'x': 0.7616104086790221, 'y': 0.43959665556538385}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,674] Trial 293 finished with value: 0.7049089932352607 and parameters: {'x': 0.6100218348868455, 'y': 0.4464789102864865}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,687] Trial 294 finished with value: 10.482097587098043 and parameters: {'x': 0.8702267873901796, 'y': 0.4337941746233761}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,699] Trial 295 finished with value: 0.19990816966156594 and parameters: {'x': 0.7161070047023079, 'y': 0.5473509469596007}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,710] Trial 296 finished with value: 0.2520127119972733 and parameters: {'x': 0.7459229494177789, 'y': 0.5131046766012678}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,722] Trial 297 finished with value: 0.08159236499396816 and parameters: {'x': 0.741389722830492, 'y': 0.5375289686330381}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,734] Trial 298 finished with value: 0.27681754598252173 and parameters: {'x': 0.7674836868862034, 'y': 0.5418344184193996}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,746] Trial 299 finished with value: 0.8876195742786399 and parameters: {'x': 0.809058256396257, 'y': 0.56231688458389}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,757] Trial 300 finished with value: 0.10217907965117469 and parameters: {'x': 0.7386107370533712, 'y': 0.527146165223259}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,769] Trial 301 finished with value: 3.28799489509728 and parameters: {'x': 0.8907011359620597, 'y': 0.6123499320235481}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,781] Trial 302 finished with value: 0.1518623817910568 and parameters: {'x': 0.7402793447073215, 'y': 0.5189605284898828}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,793] Trial 303 finished with value: 0.12183505615044918 and parameters: {'x': 0.7366829873309725, 'y': 0.5197891183978365}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,805] Trial 304 finished with value: 2.32394538088397 and parameters: {'x': 0.8250735263943747, 'y': 0.5293083462327698}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,816] Trial 305 finished with value: 105.71728101783565 and parameters: {'x': 1.235676668026456, 'y': 0.49897787426132406}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,828] Trial 306 finished with value: 2.6481725655063335 and parameters: {'x': 0.7312332886836705, 'y': 0.6951993816580562}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,839] Trial 307 finished with value: 25.070441130285968 and parameters: {'x': 0.9990540016921703, 'y': 0.4974049914282873}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,851] Trial 308 finished with value: 7.394401862132914 and parameters: {'x': 0.9121278895059104, 'y': 0.5601928068615438}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,863] Trial 309 finished with value: 0.8223363911275678 and parameters: {'x': 0.7678475912020254, 'y': 0.6772507269967214}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,874] Trial 310 finished with value: 0.2864824996077074 and parameters: {'x': 0.7019209717245488, 'y': 0.4482372981056761}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,886] Trial 311 finished with value: 3.398145411967517 and parameters: {'x': 0.8277013258393101, 'y': 0.5015558753713387}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,898] Trial 312 finished with value: 10.415693370238785 and parameters: {'x': 0.6967421059488662, 'y': 0.8067551491428184}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,911] Trial 313 finished with value: 2.5914478384646986 and parameters: {'x': 0.698845881872015, 'y': 0.6465232925194941}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,923] Trial 314 finished with value: 2.302677622933099 and parameters: {'x': 0.7675977426575382, 'y': 0.4392507406606349}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,935] Trial 315 finished with value: 5.4598496457553995 and parameters: {'x': 0.8822193656022211, 'y': 0.544944829661991}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,947] Trial 316 finished with value: 0.17829783461281917 and parameters: {'x': 0.6682510884493568, 'y': 0.4204366356158515}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,959] Trial 317 finished with value: 0.14518417518400756 and parameters: {'x': 0.6678967951653124, 'y': 0.42740682586737005}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,971] Trial 318 finished with value: 0.059235178024182544 and parameters: {'x': 0.7945671953214619, 'y': 0.6182861502147068}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,983] Trial 319 finished with value: 16.404114720907078 and parameters: {'x': 0.8073966154565282, 'y': 1.0564510162355032}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:28,994] Trial 320 finished with value: 2.0508307376434773 and parameters: {'x': 0.926803795048691, 'y': 0.7159452390941949}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,007] Trial 321 finished with value: 121.85751877386612 and parameters: {'x': -1.5833689776564168, 'y': 1.433820516554822}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,018] Trial 322 finished with value: 3.3120467020774376 and parameters: {'x': 0.6460561711779789, 'y': 0.5959038548822141}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,030] Trial 323 finished with value: 4.339477090064906 and parameters: {'x': 0.7848659063449187, 'y': 0.40881423969249076}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,041] Trial 324 finished with value: 6.174890257385492 and parameters: {'x': 0.6285490427987968, 'y': 0.640775230981925}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,053] Trial 325 finished with value: 5.204167682482663 and parameters: {'x': 0.8619111304785282, 'y': 0.5151826690432393}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,066] Trial 326 finished with value: 44.3276555891072 and parameters: {'x': 1.0467957221629245, 'y': 0.43000755548805014}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,077] Trial 327 finished with value: 10.053199909989706 and parameters: {'x': 0.7321003056163837, 'y': 0.8519048642593667}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,089] Trial 328 finished with value: 12.30557434023105 and parameters: {'x': 0.6426135099294485, 'y': 0.7619198757650024}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,101] Trial 329 finished with value: 6.216477901677036 and parameters: {'x': 0.7998008098522222, 'y': 0.39115773121038633}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,113] Trial 330 finished with value: 1.4221502973794384 and parameters: {'x': 0.6854752002365059, 'y': 0.5849077435536107}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,125] Trial 331 finished with value: 16.13396359378287 and parameters: {'x': 0.9428915575931626, 'y': 0.48741403461403277}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,137] Trial 332 finished with value: 10.05202592028266 and parameters: {'x': 0.8467927579584527, 'y': 0.40037906086698916}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,149] Trial 333 finished with value: 6.903897725392399 and parameters: {'x': 0.6134869046826412, 'y': 0.6362604944392026}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,161] Trial 334 finished with value: 0.5333017169634083 and parameters: {'x': 0.7622112161296296, 'y': 0.5119182580017296}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,174] Trial 335 finished with value: 0.8966452184758065 and parameters: {'x': 0.6960949214151372, 'y': 0.5742301847396987}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,186] Trial 336 finished with value: 1.1878568251574269 and parameters: {'x': 0.6079266087561579, 'y': 0.47126720388174786}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,199] Trial 337 finished with value: 1.6298005049195738 and parameters: {'x': 0.7755327792411641, 'y': 0.727125869247424}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,211] Trial 338 finished with value: 675.5486422682807 and parameters: {'x': 0.8908212384084974, 'y': -1.8055464497877969}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,223] Trial 339 finished with value: 0.17149259671512945 and parameters: {'x': 0.9977317786612843, 'y': 0.9540576587096281}. Best is trial 265 with value: 0.005947979141609836.\n",
      "[I 2024-02-01 04:42:29,235] Trial 340 finished with value: 0.001572722084633599 and parameters: {'x': 1.0072231761459391, 'y': 1.0105991063380972}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,247] Trial 341 finished with value: 13.498905664009907 and parameters: {'x': 1.1689592458727525, 'y': 0.9994458493320265}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,259] Trial 342 finished with value: 1.425665942034152 and parameters: {'x': 0.9893459251572325, 'y': 1.0982018601067418}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,272] Trial 343 finished with value: 1.985321131972234 and parameters: {'x': 1.111359281526565, 'y': 1.094658773354879}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,285] Trial 344 finished with value: 6.611263885395084 and parameters: {'x': 1.074286198039037, 'y': 0.8970743874543123}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,298] Trial 345 finished with value: 55.25172307066931 and parameters: {'x': 1.3127202781749348, 'y': 0.9805776153572217}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,311] Trial 346 finished with value: 3.5056106947766126 and parameters: {'x': 0.8813575890899605, 'y': 0.9636476868387599}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,324] Trial 347 finished with value: 26.485473757597358 and parameters: {'x': 0.9549617718026576, 'y': 0.39733129698421943}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,336] Trial 348 finished with value: 15.237861955306863 and parameters: {'x': 0.68361979801523, 'y': 0.856408843057139}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,348] Trial 349 finished with value: 3.860445689524131 and parameters: {'x': 1.0102254911995492, 'y': 1.217033051422206}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,359] Trial 350 finished with value: 879.9713557217216 and parameters: {'x': -1.8942392506583352, 'y': 0.6358640068667526}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,372] Trial 351 finished with value: 10.309173014683784 and parameters: {'x': 0.8459063412105856, 'y': 1.036266567580347}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,384] Trial 352 finished with value: 3.0619510328671042 and parameters: {'x': 0.9739725503659522, 'y': 0.7736575722398188}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,396] Trial 353 finished with value: 1.0241922219738921 and parameters: {'x': 0.6056680202741164, 'y': 0.4600375335246367}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,408] Trial 354 finished with value: 2.339140893581112 and parameters: {'x': 0.7216194331397441, 'y': 0.37034693546129477}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,420] Trial 355 finished with value: 0.038617702691065806 and parameters: {'x': 0.8218426344799002, 'y': 0.6837184791789821}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,432] Trial 356 finished with value: 0.39941321587257456 and parameters: {'x': 0.9169371485092553, 'y': 0.9034246543873256}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,445] Trial 357 finished with value: 0.03140867698385519 and parameters: {'x': 0.827201078294912, 'y': 0.6881976239848744}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,457] Trial 358 finished with value: 0.23504298322436898 and parameters: {'x': 0.8296889874612303, 'y': 0.7337751303074385}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,470] Trial 359 finished with value: 0.050539931166502794 and parameters: {'x': 0.8783337246363883, 'y': 0.7525658337494254}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,483] Trial 360 finished with value: 7.828707541707558 and parameters: {'x': 1.030408441231462, 'y': 0.7819598038827447}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,495] Trial 361 finished with value: 0.02858629981852804 and parameters: {'x': 0.8511274849420646, 'y': 0.716403462525206}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,507] Trial 362 finished with value: 0.029521294071601093 and parameters: {'x': 0.9249680171464586, 'y': 0.8401089586676783}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,520] Trial 363 finished with value: 15.808056480864103 and parameters: {'x': 1.1254727172952024, 'y': 0.869293403606425}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,532] Trial 364 finished with value: 1.8967481703842883 and parameters: {'x': 0.9646251800720382, 'y': 0.7928246959159804}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,545] Trial 365 finished with value: 0.008139649740069503 and parameters: {'x': 0.9110115879531209, 'y': 0.8284564745948363}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,557] Trial 366 finished with value: 1.3570790225489453 and parameters: {'x': 0.9061014222357524, 'y': 0.9371344754943078}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,570] Trial 367 finished with value: 2.872440306762584 and parameters: {'x': 1.0098377529767792, 'y': 0.8502923910255941}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,583] Trial 368 finished with value: 109.38769962447853 and parameters: {'x': 1.312244370725779, 'y': 0.6765657407723142}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,595] Trial 369 finished with value: 0.45105161934823873 and parameters: {'x': 0.9012292913546506, 'y': 0.7457841244665451}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,607] Trial 370 finished with value: 9.188077433653993 and parameters: {'x': 1.0649730821929961, 'y': 0.8311188919613985}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,620] Trial 371 finished with value: 0.09119417853356794 and parameters: {'x': 0.8505451756880812, 'y': 0.6971863958149176}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,632] Trial 372 finished with value: 0.03014650926754785 and parameters: {'x': 0.8355708787072442, 'y': 0.7037550481910213}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,645] Trial 373 finished with value: 2.906140756994089 and parameters: {'x': 0.9387788880755815, 'y': 0.7109416984785921}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,657] Trial 374 finished with value: 3.699588881014662 and parameters: {'x': 0.8577193256879545, 'y': 0.9274986320788028}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,669] Trial 375 finished with value: 4.0321761956307745 and parameters: {'x': 1.0142771205580436, 'y': 0.8279603591864545}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,682] Trial 376 finished with value: 0.07154184962272878 and parameters: {'x': 0.8106741038880004, 'y': 0.6760862992320192}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,695] Trial 377 finished with value: 0.03235176091115556 and parameters: {'x': 0.8302670126753754, 'y': 0.6952951808880399}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,707] Trial 378 finished with value: 0.049567399219326354 and parameters: {'x': 0.8474503535509363, 'y': 0.7019560588785847}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,720] Trial 379 finished with value: 2.2074017930987573 and parameters: {'x': 0.9532651184050923, 'y': 0.7602146336468332}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,733] Trial 380 finished with value: 0.25330483708126134 and parameters: {'x': 0.8577436322098907, 'y': 0.6874470257060936}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,745] Trial 381 finished with value: 11.807372841561909 and parameters: {'x': 1.0741009866198574, 'y': 0.8101542579032394}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,758] Trial 382 finished with value: 2.5072225009260873 and parameters: {'x': 0.9164120721338131, 'y': 0.6816897540823723}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,770] Trial 383 finished with value: 5.11328666558788 and parameters: {'x': 0.8354746285303998, 'y': 0.9235443071964655}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,784] Trial 384 finished with value: 2.657352711089968 and parameters: {'x': 0.9517456477019584, 'y': 0.74287732707963}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,795] Trial 385 finished with value: 1.7739988388474761 and parameters: {'x': 0.8314693597829801, 'y': 0.8234623113219697}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,808] Trial 386 finished with value: 12.920781312694839 and parameters: {'x': 1.018717573918965, 'y': 0.6783354854639023}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,821] Trial 387 finished with value: 22.373395641438776 and parameters: {'x': 0.8092290950449801, 'y': 1.127472105560356}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,833] Trial 388 finished with value: 11.125167010473648 and parameters: {'x': 1.1546398406654075, 'y': 1.0000077248429544}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,847] Trial 389 finished with value: 0.8235166506515849 and parameters: {'x': 0.9003880058772328, 'y': 0.7204991098388679}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,861] Trial 390 finished with value: 6.253443984464844 and parameters: {'x': 0.792640362695487, 'y': 0.8774864082860825}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,876] Trial 391 finished with value: 2.3244774104204557 and parameters: {'x': 0.8880108712874838, 'y': 0.6365127946854149}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,890] Trial 392 finished with value: 5.936583835999387 and parameters: {'x': 0.9950283935445602, 'y': 0.7464309525726686}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,905] Trial 393 finished with value: 95.22327307511546 and parameters: {'x': -1.3270335900701462, 'y': 0.8133463296831313}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,919] Trial 394 finished with value: 0.1544832124947081 and parameters: {'x': 0.7943854183945259, 'y': 0.6645453355711964}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,933] Trial 395 finished with value: 1.2365141731635496 and parameters: {'x': 0.863438925317292, 'y': 0.6351698456624746}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,947] Trial 396 finished with value: 9.156055755627879 and parameters: {'x': 0.7863071882879865, 'y': 0.9201132392271059}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,961] Trial 397 finished with value: 498.29028003867694 and parameters: {'x': 1.7132221278204183, 'y': 0.7040281022935437}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,974] Trial 398 finished with value: 2.2972548162659727 and parameters: {'x': 0.9632901721193997, 'y': 0.7764054423645979}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:29,988] Trial 399 finished with value: 33.61750053974616 and parameters: {'x': 1.0975302516782515, 'y': 0.6248486811884354}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,001] Trial 400 finished with value: 4.869588369404067 and parameters: {'x': 0.8998606932245519, 'y': 1.0301933755156605}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,014] Trial 401 finished with value: 271.459968134742 and parameters: {'x': 0.8143060951808525, 'y': -0.9844051612221385}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,027] Trial 402 finished with value: 51.64548760072667 and parameters: {'x': 1.2570445071454763, 'y': 0.8619727858161746}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,040] Trial 403 finished with value: 0.4649444939414589 and parameters: {'x': 0.7843714173086994, 'y': 0.6799261395288543}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,052] Trial 404 finished with value: 7.836084952356679 and parameters: {'x': 1.0290793117411192, 'y': 0.7790892544416598}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,065] Trial 405 finished with value: 2.5980285640144105 and parameters: {'x': 0.8786386748858885, 'y': 0.6112794455016294}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,077] Trial 406 finished with value: 13.351734744231365 and parameters: {'x': 0.7566794687501415, 'y': 0.9371530374581762}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,090] Trial 407 finished with value: 4.896737002452583 and parameters: {'x': 0.9527057554786595, 'y': 0.6864130817219581}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,103] Trial 408 finished with value: 244.50983830536495 and parameters: {'x': 1.4634567441821167, 'y': 0.5787115725204}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,115] Trial 409 finished with value: 0.6561393164973313 and parameters: {'x': 0.8310892206701468, 'y': 0.7699310362649524}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,128] Trial 410 finished with value: 0.06012124297563307 and parameters: {'x': 0.9343845834314891, 'y': 0.8494491693164056}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,141] Trial 411 finished with value: 8.00318603093614 and parameters: {'x': 0.7584291995754219, 'y': 0.8570805914913636}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,155] Trial 412 finished with value: 1.018078234739649 and parameters: {'x': 0.9033891277497492, 'y': 0.7156756391194956}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,168] Trial 413 finished with value: 1.0452757037863487 and parameters: {'x': 0.8448513826849267, 'y': 0.6127191899403315}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,182] Trial 414 finished with value: 6.454917846072313 and parameters: {'x': 0.7413879355174482, 'y': 0.8024017463869311}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,197] Trial 415 finished with value: 5.063345661216779 and parameters: {'x': 0.9403405502655651, 'y': 0.6593006605283258}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,211] Trial 416 finished with value: 0.07786996525843123 and parameters: {'x': 0.8223542888947085, 'y': 0.6977867917469074}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,225] Trial 417 finished with value: 15.133587080617358 and parameters: {'x': 1.068457086601736, 'y': 0.7526416700895269}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,238] Trial 418 finished with value: 0.7574039193534166 and parameters: {'x': 0.8784485640300783, 'y': 0.8578478140926771}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,252] Trial 419 finished with value: 11.497934856881448 and parameters: {'x': 0.9595766735306219, 'y': 0.5817254391726107}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,265] Trial 420 finished with value: 1.9844552500442112 and parameters: {'x': 0.7659620575714722, 'y': 0.7256108496906079}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,278] Trial 421 finished with value: 2.1793991006449045 and parameters: {'x': 0.8520203330293022, 'y': 0.579054301595012}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,292] Trial 422 finished with value: 181.67520973724191 and parameters: {'x': 0.7440773636340412, 'y': 1.9012776044888469}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,306] Trial 423 finished with value: 0.28696133961458903 and parameters: {'x': 0.9185961674649123, 'y': 0.7908722706716182}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,319] Trial 424 finished with value: 2.4653005510633865 and parameters: {'x': 1.021126290221258, 'y': 0.8857003582016525}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,332] Trial 425 finished with value: 42.75969794683449 and parameters: {'x': 1.1339163516082646, 'y': 0.631994433478441}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,345] Trial 426 finished with value: 0.04937492003854821 and parameters: {'x': 0.8343388211944808, 'y': 0.7109304865853379}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,358] Trial 427 finished with value: 1.2220272569601884 and parameters: {'x': 0.8341323825423773, 'y': 0.8050707102285125}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,372] Trial 428 finished with value: 0.37229064593304834 and parameters: {'x': 0.8793767874598781, 'y': 0.7134921067755413}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,385] Trial 429 finished with value: 1.0682848629116615 and parameters: {'x': 0.9375200717506307, 'y': 0.9821127330183224}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,398] Trial 430 finished with value: 0.2167713000294316 and parameters: {'x': 0.8127112847445411, 'y': 0.7031252393459082}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,411] Trial 431 finished with value: 4.113315987268771 and parameters: {'x': 1.024489100216494, 'y': 0.846779586310515}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,424] Trial 432 finished with value: 0.6903041151987169 and parameters: {'x': 0.7216140343637397, 'y': 0.5990087007033685}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,437] Trial 433 finished with value: 0.3535387744650912 and parameters: {'x': 0.8355415347438169, 'y': 0.755269151175749}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,449] Trial 434 finished with value: 3.433645645753167 and parameters: {'x': 0.9185988215033121, 'y': 0.6587016866820946}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,462] Trial 435 finished with value: 445.60515408757857 and parameters: {'x': 0.9684950768033073, 'y': -1.1729511116210327}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,476] Trial 436 finished with value: 7.942272015878635 and parameters: {'x': 0.7834297051213218, 'y': 0.8947491012001194}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,490] Trial 437 finished with value: 3.260652141307078 and parameters: {'x': 0.8610656769752173, 'y': 0.5613966207669296}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,503] Trial 438 finished with value: 331.91379894218846 and parameters: {'x': 1.0610987469650197, 'y': -0.6959093595142853}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,516] Trial 439 finished with value: 5.467955743642114 and parameters: {'x': 0.7366722829668043, 'y': 0.7750352349106616}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,530] Trial 440 finished with value: 8.36829588556731 and parameters: {'x': 0.979768847208123, 'y': 0.6706739985724082}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,544] Trial 441 finished with value: 31.4546324744376 and parameters: {'x': 0.6969796981239369, 'y': 1.045805798776324}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,556] Trial 442 finished with value: 8.174484466698463 and parameters: {'x': 0.8063840490201816, 'y': 0.935509460253691}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,570] Trial 443 finished with value: 17.195354094421994 and parameters: {'x': 0.9274953559053717, 'y': 1.2748570558981474}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,583] Trial 444 finished with value: 0.7536649329121594 and parameters: {'x': 0.8499575514918166, 'y': 0.8079352774096212}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,596] Trial 445 finished with value: 42.48587161665282 and parameters: {'x': 1.1200675450695694, 'y': 0.6028500288486092}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,610] Trial 446 finished with value: 4.164679064174843 and parameters: {'x': 0.7212408281819456, 'y': 0.7223509496588448}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,623] Trial 447 finished with value: 0.394245652467832 and parameters: {'x': 0.7734110241160997, 'y': 0.6567225398238902}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,636] Trial 448 finished with value: 7.254226197801108 and parameters: {'x': 0.8988988407220555, 'y': 0.5388722374541758}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,649] Trial 449 finished with value: 3.0418980752330946 and parameters: {'x': 1.0162420571379749, 'y': 0.8583451013216177}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,663] Trial 450 finished with value: 7.2726414265704555 and parameters: {'x': 0.6832847662268557, 'y': 0.7346901866305318}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,676] Trial 451 finished with value: 0.05440431941066774 and parameters: {'x': 0.7669859497155235, 'y': 0.5872245099551001}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,689] Trial 452 finished with value: 0.5130834602669779 and parameters: {'x': 0.8296745540612311, 'y': 0.618784532187668}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,702] Trial 453 finished with value: 4.0497078277614795 and parameters: {'x': 0.9507245369993107, 'y': 0.7026986236229437}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,716] Trial 454 finished with value: 0.4723610166611634 and parameters: {'x': 0.8633048601688378, 'y': 0.8126507160717857}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,729] Trial 455 finished with value: 0.17330250042529635 and parameters: {'x': 0.781982827125202, 'y': 0.5760329328119386}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,742] Trial 456 finished with value: 1.6762770975661208 and parameters: {'x': 1.0458695552947754, 'y': 0.964453286231071}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,755] Trial 457 finished with value: 0.11223247625181794 and parameters: {'x': 0.890870052370593, 'y': 0.761975623352915}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,769] Trial 458 finished with value: 6.148497156144491 and parameters: {'x': 0.9513737337160686, 'y': 1.1530259308400395}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,782] Trial 459 finished with value: 27.137595487669987 and parameters: {'x': 1.1843177303877659, 'y': 0.8819970914959419}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,796] Trial 460 finished with value: 0.08407116253244282 and parameters: {'x': 0.892013912158055, 'y': 0.7687796821199694}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,809] Trial 461 finished with value: 3.7103618016797277 and parameters: {'x': 0.9875751801629936, 'y': 0.7826857492081303}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,823] Trial 462 finished with value: 20.029569744648466 and parameters: {'x': 1.0990475610105466, 'y': 0.7604710844278457}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,837] Trial 463 finished with value: 3.495772040698805 and parameters: {'x': 0.9124883465645801, 'y': 1.019399908424296}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,850] Trial 464 finished with value: 66.48801963935163 and parameters: {'x': 0.901488149735447, 'y': 1.6280232358572277}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,863] Trial 465 finished with value: 4.478109103908106 and parameters: {'x': 0.8417202088616877, 'y': 0.9195155773105388}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,876] Trial 466 finished with value: 5.26618391451552 and parameters: {'x': 1.0375281674769767, 'y': 0.8470137111417815}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,889] Trial 467 finished with value: 817.8149094734093 and parameters: {'x': 1.886812393994382, 'y': 0.7016900173251756}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,903] Trial 468 finished with value: 0.021646277401465935 and parameters: {'x': 0.8902588097207768, 'y': 0.7827611827044882}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,917] Trial 469 finished with value: 2.242086977297919 and parameters: {'x': 0.9926368126945585, 'y': 0.8355936520741911}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,930] Trial 470 finished with value: 0.5322966903500507 and parameters: {'x': 0.910617436360266, 'y': 0.7568150380948151}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,943] Trial 471 finished with value: 9.119444747836951 and parameters: {'x': 1.1015022568929937, 'y': 0.9114936695660067}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,956] Trial 472 finished with value: 0.5550284069163207 and parameters: {'x': 0.8633774981870727, 'y': 0.8186574915059192}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,969] Trial 473 finished with value: 9.358746152285951 and parameters: {'x': 0.9892780258732828, 'y': 0.6727522129298997}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,983] Trial 474 finished with value: 17.188873461938833 and parameters: {'x': 0.8159098626891226, 'y': 1.0798946628817974}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:30,996] Trial 475 finished with value: 1.2891752629113749 and parameters: {'x': 0.9257266934197851, 'y': 0.7436712463617084}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,009] Trial 476 finished with value: 10.753111232004198 and parameters: {'x': 0.8137087178440978, 'y': 0.9895116572717777}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,023] Trial 477 finished with value: 8.63858633972738 and parameters: {'x': 0.9759703919389404, 'y': 0.6586133077572947}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,036] Trial 478 finished with value: 0.9440716784310231 and parameters: {'x': 0.8889314699826673, 'y': 0.8867256034103416}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,050] Trial 479 finished with value: 13.282195417646227 and parameters: {'x': 1.0712244668895452, 'y': 0.7831439983647526}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,063] Trial 480 finished with value: 0.41838621648118707 and parameters: {'x': 0.8099634227403233, 'y': 0.7178689206436079}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,076] Trial 481 finished with value: 0.011843695061960528 and parameters: {'x': 0.9048256551358563, 'y': 0.8239872868991432}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,090] Trial 482 finished with value: 3.00426634556082 and parameters: {'x': 1.0103692411038208, 'y': 0.8475209092709504}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,103] Trial 483 finished with value: 9.544839658654709 and parameters: {'x': 0.7874145153230687, 'y': 0.9282365937619509}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,116] Trial 484 finished with value: 50.02787026226092 and parameters: {'x': 1.16046270183667, 'y': 0.6395518966369206}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,129] Trial 485 finished with value: 0.2722552472131181 and parameters: {'x': 0.9360001577568299, 'y': 0.8243121985916053}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,143] Trial 486 finished with value: 0.7185648698149015 and parameters: {'x': 0.8688668727392944, 'y': 0.6711818680808104}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,157] Trial 487 finished with value: 16.505362358016075 and parameters: {'x': 0.7657178011848086, 'y': 0.9919155928002957}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,170] Trial 488 finished with value: 12.496249965608303 and parameters: {'x': 1.0467483882026434, 'y': 0.7422127474633617}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,184] Trial 489 finished with value: 0.06129534470277741 and parameters: {'x': 0.9700314593737354, 'y': 0.9163851840153142}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,197] Trial 490 finished with value: 0.006048240051516741 and parameters: {'x': 0.9715385048779455, 'y': 0.951124595974703}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,211] Trial 491 finished with value: 3.9179295055461645 and parameters: {'x': 1.1172129701897542, 'y': 1.0505745723883493}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,225] Trial 492 finished with value: 22.60221974062011 and parameters: {'x': 1.1937794889268518, 'y': 0.9500866345176793}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,238] Trial 493 finished with value: 0.16958809636099256 and parameters: {'x': 0.993021595665974, 'y': 1.0272670516607352}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,252] Trial 494 finished with value: 17.341037697158026 and parameters: {'x': 1.2498184779309747, 'y': 1.1463705317688713}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,266] Trial 495 finished with value: 5.410129201269037 and parameters: {'x': 1.0522448489804135, 'y': 0.8746810602404264}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,280] Trial 496 finished with value: 0.006990123817204319 and parameters: {'x': 0.9546151222301005, 'y': 0.9183116672168058}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,295] Trial 497 finished with value: 1.3974630698540904 and parameters: {'x': 0.9977778018980659, 'y': 1.113774675281797}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,309] Trial 498 finished with value: 3.084902088936695 and parameters: {'x': 1.0786758634976823, 'y': 0.988079024261119}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,323] Trial 499 finished with value: 0.0024204261852307112 and parameters: {'x': 0.9698416372244161, 'y': 0.9367057724288881}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,339] Trial 500 finished with value: 8.673850120196965 and parameters: {'x': 1.149072041946532, 'y': 1.026230064666927}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,355] Trial 501 finished with value: 0.09424571492119521 and parameters: {'x': 0.9830838668976146, 'y': 0.9358010657712028}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,371] Trial 502 finished with value: 123.44229211337984 and parameters: {'x': 1.4303863772779297, 'y': 0.9357932280008546}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,388] Trial 503 finished with value: 7.580771305383581 and parameters: {'x': 0.9610671801524254, 'y': 1.1989546024072055}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,404] Trial 504 finished with value: 6.107125269652928 and parameters: {'x': 1.075438597303608, 'y': 0.9095573612668557}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,420] Trial 505 finished with value: 4.765368279065974 and parameters: {'x': 0.9330529144761914, 'y': 1.088782295837644}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,437] Trial 506 finished with value: 5.40922085620519 and parameters: {'x': 1.044891683525229, 'y': 0.8592646415382118}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,453] Trial 507 finished with value: 0.8324721880940885 and parameters: {'x': 0.9436714977638574, 'y': 0.9815817667924167}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,470] Trial 508 finished with value: 6.553071429208025 and parameters: {'x': 0.8997049185481654, 'y': 1.0652620663612322}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,487] Trial 509 finished with value: 29.53560870044843 and parameters: {'x': 1.1778444850845986, 'y': 0.844141968485824}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,504] Trial 510 finished with value: 2.170494904423585 and parameters: {'x': 1.0223226764855562, 'y': 0.8978345715474446}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,520] Trial 511 finished with value: 1.0868494678707121 and parameters: {'x': 0.9666046727248697, 'y': 0.8301260223267052}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,535] Trial 512 finished with value: 4.297464637383563 and parameters: {'x': 0.869186911199772, 'y': 0.9623760162152984}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,549] Trial 513 finished with value: 4.809024104657941 and parameters: {'x': 1.115725852628605, 'y': 1.4638334846223664}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,563] Trial 514 finished with value: 1.0625799143500188 and parameters: {'x': 0.8478304589707617, 'y': 0.8207686497218536}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,577] Trial 515 finished with value: 1.0935072223070792 and parameters: {'x': 0.9605483909775047, 'y': 1.0271496607679378}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,591] Trial 516 finished with value: 5.0645469909782 and parameters: {'x': 0.8117924133514073, 'y': 0.8832640269514598}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,605] Trial 517 finished with value: 26.908717717792175 and parameters: {'x': -1.1240517760776574, 'y': 0.7902364204828007}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,619] Trial 518 finished with value: 2.9869204400372364 and parameters: {'x': 1.050725659823703, 'y': 1.276777048877006}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,633] Trial 519 finished with value: 1.257583368483588 and parameters: {'x': 0.9166354488587178, 'y': 0.952052282212587}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,647] Trial 520 finished with value: 2.6527341234628907 and parameters: {'x': 0.8585863799613387, 'y': 0.8994276614238144}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,661] Trial 521 finished with value: 2.9638998484239085 and parameters: {'x': 0.9843777498261277, 'y': 0.7968468374493157}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,675] Trial 522 finished with value: 18.234467274669917 and parameters: {'x': 0.8144306445120341, 'y': 1.0899122215865884}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,690] Trial 523 finished with value: 1.2614276001313076 and parameters: {'x': 0.9221335804189316, 'y': 0.7382872939069904}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,704] Trial 524 finished with value: 69.28895699285576 and parameters: {'x': 1.3016418715113294, 'y': 0.8624183944947186}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,718] Trial 525 finished with value: 28.114828108042705 and parameters: {'x': 1.1071527355393407, 'y': 0.6956612853370825}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,731] Trial 526 finished with value: 12.203053669547533 and parameters: {'x': 0.7832329555609852, 'y': 0.9621093619592929}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,745] Trial 527 finished with value: 4.459632674194838 and parameters: {'x': 1.0062770270006094, 'y': 0.8014159640071197}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,759] Trial 528 finished with value: 1.6582336226940002 and parameters: {'x': 0.888017442078457, 'y': 0.6602903903093116}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,773] Trial 529 finished with value: 2.1035160682620315 and parameters: {'x': 0.7719662900755851, 'y': 0.7391631191536807}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,786] Trial 530 finished with value: 0.00936719735547744 and parameters: {'x': 0.9432478188786014, 'y': 0.8975563376739762}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,800] Trial 531 finished with value: 0.20470966470687993 and parameters: {'x': 1.0222465478479197, 'y': 0.9997978778095911}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,815] Trial 532 finished with value: 8.961518956767032 and parameters: {'x': 1.2063013130281055, 'y': 1.1565166000934344}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,829] Trial 533 finished with value: 9.209344764575627 and parameters: {'x': 1.0962459116064638, 'y': 0.8984387376471835}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,843] Trial 534 finished with value: 1.6876629502024452 and parameters: {'x': 0.9448012268326588, 'y': 1.022442117870144}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,857] Trial 535 finished with value: 0.028754136321376533 and parameters: {'x': 0.9075097384449108, 'y': 0.8377864859575679}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,871] Trial 536 finished with value: 3.632664085193887 and parameters: {'x': 1.0528892452785412, 'y': 0.9180536692768836}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,886] Trial 537 finished with value: 0.6413206083629454 and parameters: {'x': 0.9564683909771263, 'y': 0.8348676905356686}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,900] Trial 538 finished with value: 2.280214014972774 and parameters: {'x': 0.8829667320329322, 'y': 0.930179817433974}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,915] Trial 539 finished with value: 20.542479865568605 and parameters: {'x': 1.1315778848075153, 'y': 0.8274214133148743}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,930] Trial 540 finished with value: 3.023014303684662 and parameters: {'x': 1.028154471588803, 'y': 0.8832562370258427}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,945] Trial 541 finished with value: 1.4075754689466793 and parameters: {'x': 0.9498867384020666, 'y': 1.0208202168315819}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,958] Trial 542 finished with value: 9.27135044601809 and parameters: {'x': 0.8922041114449446, 'y': 1.1003262292569422}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,973] Trial 543 finished with value: 3.945338640393605 and parameters: {'x': 0.9979604287002664, 'y': 0.7972963566675406}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:31,987] Trial 544 finished with value: 4.426932863957156 and parameters: {'x': 0.8661475176650513, 'y': 0.9601881014543794}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,002] Trial 545 finished with value: 0.49134601357594065 and parameters: {'x': 0.923326638581382, 'y': 0.782856604535023}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,016] Trial 546 finished with value: 6.17274471028881 and parameters: {'x': 1.0559057925003763, 'y': 0.8665498599421002}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,030] Trial 547 finished with value: 12.688483104835191 and parameters: {'x': 0.8392028971827955, 'y': 1.0601073596702684}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,044] Trial 548 finished with value: 599.6529248712956 and parameters: {'x': 0.9718784020565137, 'y': -1.504231933044637}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,057] Trial 549 finished with value: 38.801591764286556 and parameters: {'x': 1.1731500041825387, 'y': 0.7536123941622673}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,071] Trial 550 finished with value: 10.001979633288625 and parameters: {'x': 0.7971656178349131, 'y': 0.951080972834468}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,085] Trial 551 finished with value: 0.6268538709032206 and parameters: {'x': 0.8897081218494607, 'y': 0.8699826831600751}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,099] Trial 552 finished with value: 56.59780208045835 and parameters: {'x': 1.00743077135522, 'y': 1.7672314994852267}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,113] Trial 553 finished with value: 20.86664859956773 and parameters: {'x': 1.0935501295045345, 'y': 0.7391474192313652}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,127] Trial 554 finished with value: 4.816102708089745 and parameters: {'x': 0.7795032138884805, 'y': 0.8259709468271025}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,141] Trial 555 finished with value: 3.095902740170415 and parameters: {'x': 0.9000971019394604, 'y': 0.6345068632590625}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,155] Trial 556 finished with value: 0.014569960722430284 and parameters: {'x': 0.9738004048040627, 'y': 0.9600700726771193}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,169] Trial 557 finished with value: 0.05309846527162922 and parameters: {'x': 1.0760059460551301, 'y': 1.1360352763604449}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,184] Trial 558 finished with value: 4.586879928621895 and parameters: {'x': 1.1908935178436904, 'y': 1.2049097767841719}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,198] Trial 559 finished with value: 0.05513740556180905 and parameters: {'x': 1.1098977915601997, 'y': 1.2526239826247754}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,213] Trial 560 finished with value: 11.849497664049721 and parameters: {'x': 1.3376651632262229, 'y': 1.4467772217648343}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,228] Trial 561 finished with value: 0.8685038272630896 and parameters: {'x': 1.2142944692044553, 'y': 1.3838147676232588}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,242] Trial 562 finished with value: 5.439740193673575 and parameters: {'x': 1.1039893746128604, 'y': 0.9857919726570409}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,257] Trial 563 finished with value: 18.110605419720947 and parameters: {'x': 1.2502384385979626, 'y': 1.1382669389149536}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,271] Trial 564 finished with value: 0.1905100066918748 and parameters: {'x': 1.1501473196903396, 'y': 1.281855227444225}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,286] Trial 565 finished with value: 200.98028313897095 and parameters: {'x': 1.5732892234447906, 'y': 1.0587234579066382}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,301] Trial 566 finished with value: 1.3640699174295832 and parameters: {'x': 1.0945864402606902, 'y': 1.3145292395158947}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,316] Trial 567 finished with value: 0.011609767513213277 and parameters: {'x': 1.0868681302896461, 'y': 1.174907620525659}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,331] Trial 568 finished with value: 21.30533531333791 and parameters: {'x': 1.3045299835253532, 'y': 1.2412271283683831}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,345] Trial 569 finished with value: 2.0630710569190738 and parameters: {'x': 1.1039583356729856, 'y': 1.0754667655015406}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,360] Trial 570 finished with value: 14.128344367279821 and parameters: {'x': 1.2409691566427665, 'y': 1.1649007523046273}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,374] Trial 571 finished with value: 42.532523915083324 and parameters: {'x': 1.3674933008267296, 'y': 1.2189045114546901}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,389] Trial 572 finished with value: 0.30075752150205115 and parameters: {'x': 1.1776290644632152, 'y': 1.3349252024383484}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,403] Trial 573 finished with value: 5.22947793800337 and parameters: {'x': 1.0717212797632114, 'y': 1.377154521920812}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,418] Trial 574 finished with value: 1.0991502915662468 and parameters: {'x': 1.0436093908454311, 'y': 1.193870191332977}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,432] Trial 575 finished with value: 2.7601541600975783 and parameters: {'x': 1.1508652894729743, 'y': 1.1590402020785646}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,446] Trial 576 finished with value: 0.6784899372007027 and parameters: {'x': 1.0406070662769502, 'y': 1.1651334136176241}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,461] Trial 577 finished with value: 26.172418203004913 and parameters: {'x': -0.8919161280159386, 'y': 1.2708360764534665}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,475] Trial 578 finished with value: 27.25065646963138 and parameters: {'x': -0.7866086884178576, 'y': 1.1092497716394782}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,489] Trial 579 finished with value: 0.5886521928423953 and parameters: {'x': 1.0146356294799201, 'y': 1.1061951725892447}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,503] Trial 580 finished with value: 8.501686931369793 and parameters: {'x': 1.1466244812894417, 'y': 1.0235400742543772}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,517] Trial 581 finished with value: 0.3557944881425159 and parameters: {'x': 0.9999476692556082, 'y': 1.0595438521619}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,531] Trial 582 finished with value: 33.00620420656565 and parameters: {'x': 1.252678567835488, 'y': 0.9952492603476131}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,546] Trial 583 finished with value: 8.032175611134724 and parameters: {'x': 0.9685939798540565, 'y': 1.2215678276056912}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,560] Trial 584 finished with value: 80.0211456907427 and parameters: {'x': -0.45398317321728043, 'y': 1.0887505877216959}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,574] Trial 585 finished with value: 2.99504580657598 and parameters: {'x': 1.077999100946525, 'y': 1.3349682076112706}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,589] Trial 586 finished with value: 0.837002138041567 and parameters: {'x': 0.9347415250487503, 'y': 0.9649964976753783}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,603] Trial 587 finished with value: 368.3685329126584 and parameters: {'x': 1.0471177356859172, 'y': -0.8228315882755366}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,617] Trial 588 finished with value: 14.282901281420978 and parameters: {'x': 1.1381499011747285, 'y': 0.9177105242308025}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,631] Trial 589 finished with value: 4.10343413792285 and parameters: {'x': 0.9797915267616735, 'y': 1.1625507051546609}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,645] Trial 590 finished with value: 5.516295773094485 and parameters: {'x': 0.8872721328724731, 'y': 1.021849113993838}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,660] Trial 591 finished with value: 3.3013676294782175 and parameters: {'x': 1.0244852143452043, 'y': 1.2312501157878222}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,676] Trial 592 finished with value: 30.676695861764973 and parameters: {'x': 1.2085610847540222, 'y': 0.90714723920491}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,691] Trial 593 finished with value: 0.20699087004883124 and parameters: {'x': 0.9194768723661177, 'y': 0.8006597224379468}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,705] Trial 594 finished with value: 57.109387938309006 and parameters: {'x': 0.8683306578009236, 'y': 1.509590952329659}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,721] Trial 595 finished with value: 2.81531294529695 and parameters: {'x': 1.0725595676661512, 'y': 0.9827520467122388}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,735] Trial 596 finished with value: 3.687760721210451 and parameters: {'x': 0.978673773871538, 'y': 0.7657787656212354}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,749] Trial 597 finished with value: 2.37326820252456 and parameters: {'x': 0.8390741051572703, 'y': 0.857256680514159}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,764] Trial 598 finished with value: 2.7409513441562243 and parameters: {'x': 0.9499267692321377, 'y': 1.067843313577667}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,779] Trial 599 finished with value: 3.1149704109239003 and parameters: {'x': 0.8641692378505511, 'y': 0.9227577989453938}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,794] Trial 600 finished with value: 0.042785030412470455 and parameters: {'x': 1.1519924809346744, 'y': 1.313056951916132}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,808] Trial 601 finished with value: 3.705222261413842 and parameters: {'x': 1.1887977347349332, 'y': 1.221678636367857}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,822] Trial 602 finished with value: 0.6651779306870277 and parameters: {'x': 1.1231174798964327, 'y': 1.3420166925135013}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,837] Trial 603 finished with value: 41.46233040668709 and parameters: {'x': 1.3770328251043988, 'y': 1.2534116801212942}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,851] Trial 604 finished with value: 21.849304220227225 and parameters: {'x': 1.2480130685138018, 'y': 1.0907626509902746}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,866] Trial 605 finished with value: 2.892727614508138 and parameters: {'x': 1.1022866141010548, 'y': 1.3848081290559457}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,880] Trial 606 finished with value: 5.457406160867953 and parameters: {'x': 1.2493939956489906, 'y': 1.3287094687688}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,895] Trial 607 finished with value: 2.59192419045641 and parameters: {'x': 1.1429933060126682, 'y': 1.466791956026682}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,910] Trial 608 finished with value: 5.413186114517229 and parameters: {'x': 1.0347849755402367, 'y': 1.3034164887348025}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,925] Trial 609 finished with value: 0.5802619876122929 and parameters: {'x': 1.0295942828899962, 'y': 0.983946967143857}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,939] Trial 610 finished with value: 3.4876257217223996 and parameters: {'x': 1.1597144321959225, 'y': 1.1588699146675911}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,954] Trial 611 finished with value: 3.4832989906426373 and parameters: {'x': 0.979738784625952, 'y': 1.146513070327911}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,968] Trial 612 finished with value: 7.896539366061052 and parameters: {'x': 1.0506148818066874, 'y': 1.3847538604279082}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,983] Trial 613 finished with value: 0.26020832059419197 and parameters: {'x': 0.9375945573656235, 'y': 0.8284561028191239}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:32,997] Trial 614 finished with value: 10.497718721670536 and parameters: {'x': 1.1084678686635236, 'y': 0.9048807965534619}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,012] Trial 615 finished with value: 2.1786026147088053 and parameters: {'x': 0.9406692289363248, 'y': 0.7373769901155314}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,027] Trial 616 finished with value: 59.171638367651234 and parameters: {'x': 0.8696712369808259, 'y': 1.5254486802384397}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,042] Trial 617 finished with value: 0.005723607274624722 and parameters: {'x': 1.0242219986628804, 'y': 1.0418634840779648}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,057] Trial 618 finished with value: 0.0805227104515763 and parameters: {'x': 1.019303462086981, 'y': 1.0106687571583377}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,072] Trial 619 finished with value: 0.34840635184743135 and parameters: {'x': 0.9433170900438947, 'y': 0.948600293949033}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,087] Trial 620 finished with value: 10.598148862813343 and parameters: {'x': 0.8502947846697331, 'y': 1.0482048064766243}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,101] Trial 621 finished with value: 1.3710959112090806 and parameters: {'x': 0.989115343469821, 'y': 0.8612604170514173}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,116] Trial 622 finished with value: 4.5898500773189514 and parameters: {'x': 0.7735634778922035, 'y': 0.8114398070026577}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,131] Trial 623 finished with value: 2.794311917897121 and parameters: {'x': 0.9057161016064631, 'y': 0.9872175063999918}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,146] Trial 624 finished with value: 5.083880244706901 and parameters: {'x': 0.8309575238591981, 'y': 0.9153304581831434}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,160] Trial 625 finished with value: 12.817620618363833 and parameters: {'x': 1.035692515176675, 'y': 0.7146597309277818}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,174] Trial 626 finished with value: 0.9861576710481278 and parameters: {'x': 0.9465055016269017, 'y': 0.7967113813542817}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,189] Trial 627 finished with value: 10.464065630750543 and parameters: {'x': 0.8958283473513752, 'y': 1.1258227316157199}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,205] Trial 628 finished with value: 23.01391248239586 and parameters: {'x': 0.7459260889260458, 'y': 1.0354606218816045}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,220] Trial 629 finished with value: 5.9790705381606015 and parameters: {'x': 1.0467808246693076, 'y': 0.8512734683519606}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,235] Trial 630 finished with value: 4.99893611826958 and parameters: {'x': 0.8555162235811845, 'y': 0.9550236863552728}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,251] Trial 631 finished with value: 2.9837637688092213 and parameters: {'x': 0.9694952846048294, 'y': 0.7672122990137308}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,265] Trial 632 finished with value: 932.745647496889 and parameters: {'x': 1.9825267789821384, 'y': 0.877904790364689}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,280] Trial 633 finished with value: 52.06317423321485 and parameters: {'x': 1.1890319337693258, 'y': 0.6924964393069097}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,295] Trial 634 finished with value: 337.8249540071999 and parameters: {'x': 1.7127009912873516, 'y': 1.0967254781426363}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,311] Trial 635 finished with value: 1.7589977741095288 and parameters: {'x': 0.793090488642893, 'y': 0.7599958142878972}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,327] Trial 636 finished with value: 7.859727344006221 and parameters: {'x': 1.0914662241373367, 'y': 0.9110957119897052}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,342] Trial 637 finished with value: 3.379555703464397 and parameters: {'x': 0.9105814652779597, 'y': 1.0127766875987743}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,358] Trial 638 finished with value: 2.4794833276876873 and parameters: {'x': 1.0000419574770982, 'y': 0.8426201645071021}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,373] Trial 639 finished with value: 3.3309653777041768 and parameters: {'x': 0.7131968858686437, 'y': 0.6888915620805725}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,388] Trial 640 finished with value: 6.013958561833951 and parameters: {'x': 0.8405529023871683, 'y': 0.9512440195308776}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,402] Trial 641 finished with value: 0.549946944599057 and parameters: {'x': 0.9242286369787225, 'y': 0.7804282782281229}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,418] Trial 642 finished with value: 0.8379589484250546 and parameters: {'x': 1.003144307173826, 'y': 1.097838058680641}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,433] Trial 643 finished with value: 27.931191321968548 and parameters: {'x': 1.0886594599602757, 'y': 0.6567541097868448}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,448] Trial 644 finished with value: 5.592524273355843 and parameters: {'x': 0.8004089232385125, 'y': 0.8762958600175705}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,464] Trial 645 finished with value: 4.99246497896548 and parameters: {'x': 0.900675813585355, 'y': 1.0344342969564668}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,480] Trial 646 finished with value: 37.85603003935287 and parameters: {'x': 1.1615018728963193, 'y': 0.7340260575070597}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,497] Trial 647 finished with value: 3.2959365055606282 and parameters: {'x': 0.9945188255594218, 'y': 0.8075213790263598}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,513] Trial 648 finished with value: 16.9963831073678 and parameters: {'x': 0.7252460173541724, 'y': 0.9373319206873134}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,529] Trial 649 finished with value: 0.289040771012817 and parameters: {'x': 0.8280318618894517, 'y': 0.6346987892277891}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,545] Trial 650 finished with value: 48.30199805773211 and parameters: {'x': 1.2326857220429621, 'y': 0.8249073271063164}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,561] Trial 651 finished with value: 22.342893064135637 and parameters: {'x': 1.0694197020784575, 'y': 1.6162902159572634}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,576] Trial 652 finished with value: 12.481250933072493 and parameters: {'x': 0.9213964758844446, 'y': 1.2021721512057695}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,591] Trial 653 finished with value: 7.094543366025209 and parameters: {'x': 0.8468612655232847, 'y': 0.983089250707839}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,607] Trial 654 finished with value: 65.71903686669754 and parameters: {'x': 1.3972381352476086, 'y': 1.142575458025981}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,622] Trial 655 finished with value: 354.96613171169867 and parameters: {'x': -1.6121805883222171, 'y': 0.7332681510990872}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,637] Trial 656 finished with value: 22.207881407908953 and parameters: {'x': -0.6567494772312374, 'y': 0.8724894869637958}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,652] Trial 657 finished with value: 109.63440333664938 and parameters: {'x': 1.2916069221684794, 'y': 0.6215900981907811}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,668] Trial 658 finished with value: 234.06993160475594 and parameters: {'x': 1.0061790445053485, 'y': -0.5175380212231255}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,684] Trial 659 finished with value: 502.5546997098951 and parameters: {'x': 0.7252418962741934, 'y': -1.7156289913444218}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,699] Trial 660 finished with value: 4.456478780536299 and parameters: {'x': 0.9294124733271103, 'y': 1.074793236598287}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,715] Trial 661 finished with value: 9.568265388031795 and parameters: {'x': 1.1068474170056521, 'y': 0.9159696670850593}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,729] Trial 662 finished with value: 0.6978192816764337 and parameters: {'x': 0.792185163096061, 'y': 0.7084666816096737}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,745] Trial 663 finished with value: 1.4385206343132528 and parameters: {'x': 0.9725102006015331, 'y': 0.8258692537198875}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,760] Trial 664 finished with value: 7.147578320949738 and parameters: {'x': 0.8542534200457893, 'y': 0.9967008911438606}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,774] Trial 665 finished with value: 10.62488458426018 and parameters: {'x': 1.047034844786236, 'y': 0.7703575530631223}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,789] Trial 666 finished with value: 0.5501417508624002 and parameters: {'x': 0.9000939343500316, 'y': 0.8836647047343391}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,805] Trial 667 finished with value: 2.501163392852125 and parameters: {'x': 0.6811174755164, 'y': 0.6188234792312408}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,820] Trial 668 finished with value: 37.50101666179678 and parameters: {'x': 1.149937231267971, 'y': 0.7101584822490411}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,835] Trial 669 finished with value: 21.480087042069396 and parameters: {'x': 0.7627048456377044, 'y': 1.0445769541631231}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,849] Trial 670 finished with value: 2.0600934359947183 and parameters: {'x': 0.9718349700204212, 'y': 0.8009605899164385}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,865] Trial 671 finished with value: 3.255556237298277 and parameters: {'x': 0.8718526325397524, 'y': 0.9401029693026629}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,880] Trial 672 finished with value: 6.806001730346435 and parameters: {'x': 1.0652641455372591, 'y': 0.8739861982061022}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,895] Trial 673 finished with value: 129.14275729860898 and parameters: {'x': 1.52735249866363, 'y': 1.1976199582910867}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,911] Trial 674 finished with value: 176.88311559941343 and parameters: {'x': 0.8094133665177367, 'y': 1.9849875531446495}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,926] Trial 675 finished with value: 0.33183798734320424 and parameters: {'x': 0.9598913830465888, 'y': 0.9788570470483349}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,942] Trial 676 finished with value: 50.59343836580046 and parameters: {'x': 1.1958341259327454, 'y': 0.7189982488916931}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,957] Trial 677 finished with value: 0.05649079564485514 and parameters: {'x': 1.0346033496678997, 'y': 1.0939186406478276}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,973] Trial 678 finished with value: 0.11101904942054541 and parameters: {'x': 0.9228801246158345, 'y': 0.8192929786551442}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:33,989] Trial 679 finished with value: 0.5319445200880403 and parameters: {'x': 0.7687675989778998, 'y': 0.6601755878725395}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,004] Trial 680 finished with value: 396.9853773351059 and parameters: {'x': 0.8737795984591292, 'y': -1.228918422654235}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,020] Trial 681 finished with value: 22.2272770813418 and parameters: {'x': 1.1158100760072274, 'y': 0.7737162538494754}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,035] Trial 682 finished with value: 1.7092635872162079 and parameters: {'x': 0.6807233553076302, 'y': 0.5901646481649028}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,050] Trial 683 finished with value: 1.1648764127451368 and parameters: {'x': 1.0151896328590377, 'y': 0.9226912393837593}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,066] Trial 684 finished with value: 9.201326412461878 and parameters: {'x': 0.8590235881807263, 'y': 1.0409306348771863}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,081] Trial 685 finished with value: 0.43674837371522923 and parameters: {'x': 0.9604800973818766, 'y': 0.8565533467955173}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,095] Trial 686 finished with value: 29.07140214156785 and parameters: {'x': 0.7906236095080816, 'y': 1.1638580340810092}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,110] Trial 687 finished with value: 18.687854579698982 and parameters: {'x': 1.065689329489649, 'y': 0.7034491461925506}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,125] Trial 688 finished with value: 2.148730596132521 and parameters: {'x': 0.9069629436481564, 'y': 0.9688717227832198}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,139] Trial 689 finished with value: 11.034440990463326 and parameters: {'x': -1.0211099780607311, 'y': 0.7790454923280794}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,154] Trial 690 finished with value: 0.127786489109169 and parameters: {'x': 1.1460524258295275, 'y': 1.2808086930319624}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,168] Trial 691 finished with value: 0.9908344782354169 and parameters: {'x': 0.9817531824151994, 'y': 0.8643153677374176}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,184] Trial 692 finished with value: 18.640060737650753 and parameters: {'x': 0.8280509015142896, 'y': 1.1170671161412056}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,200] Trial 693 finished with value: 101.22680392240478 and parameters: {'x': 1.2810198031467115, 'y': 0.6352889522182342}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,216] Trial 694 finished with value: 20.543360376020388 and parameters: {'x': 0.7258742586173442, 'y': 0.9793115584778042}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,230] Trial 695 finished with value: 0.18819665561426674 and parameters: {'x': 0.8940756328151696, 'y': 0.7573026403488404}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,246] Trial 696 finished with value: 1.595449427522815 and parameters: {'x': 1.0315842630330159, 'y': 0.9378944851960948}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,259] Trial 697 finished with value: 0.8323401657141292 and parameters: {'x': 0.9532696781653056, 'y': 0.817610158011945}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,274] Trial 698 finished with value: 0.03477962462935609 and parameters: {'x': 0.8146215486497874, 'y': 0.6615724521980983}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,290] Trial 699 finished with value: 54.41288921413919 and parameters: {'x': 1.1966114139204884, 'y': 0.6944900153766034}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,307] Trial 700 finished with value: 11.485045205764314 and parameters: {'x': 0.8382502305237967, 'y': 1.0411731566307292}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,322] Trial 701 finished with value: 9.401532608882425 and parameters: {'x': 1.0879005781070203, 'y': 0.8770345015718387}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,338] Trial 702 finished with value: 2.7882892030774973 and parameters: {'x': 0.965102758029718, 'y': 0.7644780914686202}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,353] Trial 703 finished with value: 22.24237767712849 and parameters: {'x': 0.6600051208032782, 'y': 0.9059978856703725}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,368] Trial 704 finished with value: 5.863594081732193 and parameters: {'x': 0.8882689395888181, 'y': 1.0309123920310281}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,383] Trial 705 finished with value: 32.54447872566054 and parameters: {'x': 0.7577619133643809, 'y': 1.1441662690935799}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,398] Trial 706 finished with value: 14.74718537331854 and parameters: {'x': 1.0261590423175655, 'y': 0.6689906475251791}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,413] Trial 707 finished with value: 1.8701345288353077 and parameters: {'x': 0.832089663410102, 'y': 0.8280913219881945}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,428] Trial 708 finished with value: 2.662285553856731 and parameters: {'x': 1.1139919877948414, 'y': 1.4037445883042763}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,443] Trial 709 finished with value: 9.498922063607024 and parameters: {'x': 0.9438609252862386, 'y': 0.5827213658430497}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,458] Trial 710 finished with value: 9.6831547100394 and parameters: {'x': 1.0260471060078455, 'y': 0.7416058875914576}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,473] Trial 711 finished with value: 1.7670870298902865 and parameters: {'x': 0.9078948903803994, 'y': 0.9568854873109198}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,489] Trial 712 finished with value: 55.64797857020352 and parameters: {'x': 0.7044524695542096, 'y': 1.2416433155175721}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,505] Trial 713 finished with value: 4.627434941839867 and parameters: {'x': 0.8145803467492942, 'y': 0.8778552680967531}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,520] Trial 714 finished with value: 1.4976941995007638 and parameters: {'x': 0.9776449878436119, 'y': 1.0781496198093157}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,536] Trial 715 finished with value: 23.037687303917377 and parameters: {'x': 1.0791143073351348, 'y': 0.6845769858694859}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,551] Trial 716 finished with value: 0.016080430860832252 and parameters: {'x': 0.8911926332635134, 'y': 0.8007369033864749}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,567] Trial 717 finished with value: 3.7600100510966428 and parameters: {'x': 0.7667616598999217, 'y': 0.7804230516064748}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,582] Trial 718 finished with value: 0.12532694302125466 and parameters: {'x': 0.8706617433922706, 'y': 0.7910061667868225}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,598] Trial 719 finished with value: 2.338190666953577 and parameters: {'x': 0.6664738321872199, 'y': 0.5934170899690062}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,615] Trial 720 finished with value: 0.2674892771145476 and parameters: {'x': 0.8089957885857846, 'y': 0.7025373390230263}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,631] Trial 721 finished with value: 0.11816739941036357 and parameters: {'x': 0.9055538240089147, 'y': 0.8530803104138145}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,647] Trial 722 finished with value: 5.5689756410116615 and parameters: {'x': 0.7524670893969376, 'y': 0.8008916817793836}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,662] Trial 723 finished with value: 0.5604572222521327 and parameters: {'x': 0.8490942519043616, 'y': 0.6476340633084913}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,678] Trial 724 finished with value: 0.0054764380253432025 and parameters: {'x': 0.9419053141698077, 'y': 0.8917697734587345}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,694] Trial 725 finished with value: 0.02516115706470148 and parameters: {'x': 0.9521756929886273, 'y': 0.9217627007816493}. Best is trial 340 with value: 0.001572722084633599.\n",
      "[I 2024-02-01 04:42:34,710] Trial 726 finished with value: 0.0009916955942820148 and parameters: {'x': 0.9728230277064955, 'y': 0.9479755793480127}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,725] Trial 727 finished with value: 0.2934213389599889 and parameters: {'x': 1.0144895227786253, 'y': 0.9750399943586157}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,741] Trial 728 finished with value: 830.8716743276585 and parameters: {'x': -1.9506900982314397, 'y': 0.9378497062775182}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,756] Trial 729 finished with value: 0.10227694153971076 and parameters: {'x': 0.9827090462517465, 'y': 0.997651058446133}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,771] Trial 730 finished with value: 18.772405916838792 and parameters: {'x': 1.1633076328675513, 'y': 0.9203211798607308}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,786] Trial 731 finished with value: 1.2136886880479896 and parameters: {'x': 0.9481298827301886, 'y': 1.0089956369444841}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,801] Trial 732 finished with value: 1.6686880281600027 and parameters: {'x': 1.0178454222312376, 'y': 0.9068439223953607}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,817] Trial 733 finished with value: 1.886922965236563 and parameters: {'x': 1.0811207562818088, 'y': 1.0316965126130848}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,833] Trial 734 finished with value: 0.023238490090968695 and parameters: {'x': 0.9473636379373682, 'y': 0.9118044705674305}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,849] Trial 735 finished with value: 0.018818214792737528 and parameters: {'x': 0.9553708669413122, 'y': 0.9257051761264901}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,864] Trial 736 finished with value: 0.3070617706359145 and parameters: {'x': 0.951270337415623, 'y': 0.9601137322370952}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,880] Trial 737 finished with value: 0.09009034225332958 and parameters: {'x': 0.9667660101741398, 'y': 0.9048060232639173}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,896] Trial 738 finished with value: 0.11665241676110644 and parameters: {'x': 1.0143089134633725, 'y': 0.9946981420083013}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,912] Trial 739 finished with value: 0.2740769658600002 and parameters: {'x': 0.9325892521782919, 'y': 0.9216392575527256}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,928] Trial 740 finished with value: 3.8488020079541885 and parameters: {'x': 0.9220595782344086, 'y': 1.0462226213220112}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,945] Trial 741 finished with value: 4.545920153860671 and parameters: {'x': 1.0426243165628477, 'y': 0.8738964404986262}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,960] Trial 742 finished with value: 1.4655566394869304 and parameters: {'x': 0.9159553719588572, 'y': 0.9597423327479067}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,976] Trial 743 finished with value: 1.306747737769429 and parameters: {'x': 1.002306111369062, 'y': 0.8903047061044537}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:34,991] Trial 744 finished with value: 5.4141088508790665 and parameters: {'x': 0.9068003003935337, 'y': 1.0547824335030278}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,007] Trial 745 finished with value: 5.166571709401131 and parameters: {'x': 1.0917591588045505, 'y': 0.964822408220758}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,022] Trial 746 finished with value: 0.037463424089465444 and parameters: {'x': 0.9940771280069144, 'y': 1.0075357427275997}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,038] Trial 747 finished with value: 0.4070762341896125 and parameters: {'x': 0.8997484316942541, 'y': 0.8725572282145302}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,054] Trial 748 finished with value: 5.755737783408441 and parameters: {'x': 1.0505155652298557, 'y': 0.8637249539434283}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,070] Trial 749 finished with value: 38.65030420032112 and parameters: {'x': -1.2310704242224537, 'y': 0.9352531734104422}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,086] Trial 750 finished with value: 0.7293584010028897 and parameters: {'x': 0.9614658844754933, 'y': 0.8391011429711974}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,102] Trial 751 finished with value: 9.11066881542034 and parameters: {'x': 0.8644829024321775, 'y': 1.048865163105654}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,118] Trial 752 finished with value: 0.009762199993865676 and parameters: {'x': 1.04030311458101, 'y': 1.091251578437151}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,134] Trial 753 finished with value: 0.28088268868311433 and parameters: {'x': 1.0278414423479065, 'y': 1.1093832176902625}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,150] Trial 754 finished with value: 6.27115473049569 and parameters: {'x': 1.1644128526031816, 'y': 1.1059748560625304}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,166] Trial 755 finished with value: 1.9511534703127944 and parameters: {'x': 1.079818124599852, 'y': 1.0265517220380487}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,182] Trial 756 finished with value: 1.5309359014660202 and parameters: {'x': 1.0987365742897959, 'y': 1.083885650201196}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,199] Trial 757 finished with value: 0.056162601466161075 and parameters: {'x': 1.0019445137889191, 'y': 0.9801949564182492}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,215] Trial 758 finished with value: 9.215227544566678 and parameters: {'x': 1.117882948614445, 'y': 0.9463253263943294}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,231] Trial 759 finished with value: 2.595095447392235 and parameters: {'x': 0.9800846818901261, 'y': 1.1216466723846772}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,247] Trial 760 finished with value: 0.4341080578338575 and parameters: {'x': 1.0458274664298894, 'y': 1.0280277815801042}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,262] Trial 761 finished with value: 27.00640810029615 and parameters: {'x': 1.206030437859335, 'y': 0.9352410898849397}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,277] Trial 762 finished with value: 0.025772778496555748 and parameters: {'x': 0.9407795677366466, 'y': 0.8701444931238901}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,293] Trial 763 finished with value: 11.09193413947333 and parameters: {'x': 1.109956502562343, 'y': 0.8991394457306403}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,310] Trial 764 finished with value: 1.093563909005945 and parameters: {'x': 0.9551090878158255, 'y': 1.0167105783874764}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,326] Trial 765 finished with value: 3.3102258688019117 and parameters: {'x': 1.0351961071662394, 'y': 0.8897247653669101}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,342] Trial 766 finished with value: 1.3249316732539878 and parameters: {'x': 0.9269752493044581, 'y': 0.9741569157130093}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,358] Trial 767 finished with value: 3.913229131548892 and parameters: {'x': 1.1291391214603457, 'y': 1.0775582909244417}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,374] Trial 768 finished with value: 1.6189412716562768 and parameters: {'x': 0.9928877372342788, 'y': 0.8585904235148435}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,390] Trial 769 finished with value: 0.2322112759800968 and parameters: {'x': 1.0537379578094612, 'y': 1.1582514178438372}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,407] Trial 770 finished with value: 1.4113300398624307 and parameters: {'x': 0.9144421783803994, 'y': 0.9546954212628687}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,424] Trial 771 finished with value: 48.379879023613434 and parameters: {'x': 1.2418406751892639, 'y': 0.847032361370899}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,439] Trial 772 finished with value: 0.5925988416856526 and parameters: {'x': 0.9720860431257666, 'y': 1.0218810908316085}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,456] Trial 773 finished with value: 5.080007415759006 and parameters: {'x': 1.0815783618980432, 'y': 0.9445707178851669}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,473] Trial 774 finished with value: 0.031533299730942696 and parameters: {'x': 0.909075739303172, 'y': 0.8416719219238589}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,489] Trial 775 finished with value: 7.758507788678292 and parameters: {'x': 1.1567044656059338, 'y': 1.059865391570139}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,505] Trial 776 finished with value: 0.9962540537842023 and parameters: {'x': 1.003052924359702, 'y': 0.9063031090017934}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,521] Trial 777 finished with value: 0.03047575922939677 and parameters: {'x': 0.9002937256286244, 'y': 0.8248586277439051}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,538] Trial 778 finished with value: 0.879856853182704 and parameters: {'x': 1.0428806798815307, 'y': 0.9938974924173816}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,554] Trial 779 finished with value: 0.36437880610701345 and parameters: {'x': 0.9451825703412641, 'y': 0.8332557123387125}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,570] Trial 780 finished with value: 11.574648979502829 and parameters: {'x': 0.9050553466585733, 'y': 1.159208030601903}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,587] Trial 781 finished with value: 9.69537335575128 and parameters: {'x': 1.103234660511495, 'y': 0.9059239533027407}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,603] Trial 782 finished with value: 0.5647123473927447 and parameters: {'x': 0.9978892356710183, 'y': 1.0709299753171673}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,619] Trial 783 finished with value: 3.460912584894098 and parameters: {'x': 0.887668514748706, 'y': 0.9736512246173306}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,635] Trial 784 finished with value: 29.19425192517572 and parameters: {'x': 1.171609814258539, 'y': 0.832625095763716}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,651] Trial 785 finished with value: 3.7811240402137773 and parameters: {'x': 1.0603881430495772, 'y': 0.9300656803205773}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,667] Trial 786 finished with value: 1.461796368857435 and parameters: {'x': 0.9749573627939956, 'y': 0.829663025917319}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,684] Trial 787 finished with value: 4.309070438896952 and parameters: {'x': 0.896838983166493, 'y': 1.011646673382103}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,701] Trial 788 finished with value: 73.42435630013378 and parameters: {'x': -1.3820622136918557, 'y': 1.086991255261647}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,717] Trial 789 finished with value: 1.0349228413429268 and parameters: {'x': 1.0047401530329323, 'y': 0.9077727219223086}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,734] Trial 790 finished with value: 0.17074612171214204 and parameters: {'x': 0.8772377897888746, 'y': 0.8090018823125816}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,750] Trial 791 finished with value: 0.049446375723397565 and parameters: {'x': 1.0879787873380988, 'y': 1.1632757681736925}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,765] Trial 792 finished with value: 27.159427966999647 and parameters: {'x': 1.224358172019131, 'y': 0.9783890170519752}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,781] Trial 793 finished with value: 0.0406148835608491 and parameters: {'x': 0.9558801806439768, 'y': 0.8940426564510974}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,798] Trial 794 finished with value: 0.5699721345880117 and parameters: {'x': 0.8601992479873555, 'y': 0.8141335734557152}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,814] Trial 795 finished with value: 61.38334299499526 and parameters: {'x': -0.5601998188781632, 'y': 1.0816071367082563}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,830] Trial 796 finished with value: 283.0430586022415 and parameters: {'x': 1.040086402728261, 'y': -0.6006038568129555}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,847] Trial 797 finished with value: 0.4127426703344232 and parameters: {'x': 0.9599236357188848, 'y': 0.9855733174286022}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,864] Trial 798 finished with value: 18.18360261314328 and parameters: {'x': 1.1482734069071585, 'y': 0.8923673228304321}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,881] Trial 799 finished with value: 0.011915247319572617 and parameters: {'x': 0.8942420979188619, 'y': 0.8023717309537742}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,898] Trial 800 finished with value: 2.5787234299865935 and parameters: {'x': 1.050014797760331, 'y': 0.9420249401191801}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,915] Trial 801 finished with value: 11.583306373433564 and parameters: {'x': 0.8425715271727079, 'y': 1.0499050519635171}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,931] Trial 802 finished with value: 9.045651250162416 and parameters: {'x': 0.953899902945363, 'y': 1.2106495837167939}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,948] Trial 803 finished with value: 18.756129267763686 and parameters: {'x': 1.116960711582904, 'y': 0.8146757242360836}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,965] Trial 804 finished with value: 59.36104102163402 and parameters: {'x': -0.35470422574607985, 'y': 0.8842728485001894}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,982] Trial 805 finished with value: 0.0015381291872518272 and parameters: {'x': 1.0006293694887383, 'y': 0.9973377410949374}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:35,998] Trial 806 finished with value: 18.384589837671484 and parameters: {'x': 1.2439769338383977, 'y': 1.1194007557623067}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,014] Trial 807 finished with value: 50.838851189996234 and parameters: {'x': 1.3146699742757901, 'y': 1.0160381537483874}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,030] Trial 808 finished with value: 0.013291428830625795 and parameters: {'x': 1.0511150830163987, 'y': 1.1151766862337775}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,046] Trial 809 finished with value: 3.227874419850363 and parameters: {'x': 1.1778388268857831, 'y': 1.2085237726568123}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,061] Trial 810 finished with value: 1.120328391112764 and parameters: {'x': 1.1228815770580105, 'y': 1.1557331873789722}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,077] Trial 811 finished with value: 0.036000891594128094 and parameters: {'x': 1.067392063237053, 'y': 1.1215890747377566}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,093] Trial 812 finished with value: 12.766270806135054 and parameters: {'x': 1.1993236533650284, 'y': 1.0816344486164666}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,109] Trial 813 finished with value: 0.0048283828748409115 and parameters: {'x': 1.0323937467736368, 'y': 1.059689468411171}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,125] Trial 814 finished with value: 0.628250817736315 and parameters: {'x': 1.1293264313683726, 'y': 1.197178093198162}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,141] Trial 815 finished with value: 1.1454317525720008 and parameters: {'x': 1.066626798430589, 'y': 1.2445099833346531}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,157] Trial 816 finished with value: 14.795969023586515 and parameters: {'x': 1.2133459864793898, 'y': 1.0881453043316016}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,173] Trial 817 finished with value: 0.287785099666131 and parameters: {'x': 1.029913924706374, 'y': 1.114284829749748}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,189] Trial 818 finished with value: 1.1697318011623263 and parameters: {'x': 1.1367833571778243, 'y': 1.1849907024654902}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,206] Trial 819 finished with value: 0.006285245990373133 and parameters: {'x': 1.0206256824669175, 'y': 1.0493317345359503}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,222] Trial 820 finished with value: 38.670444897839864 and parameters: {'x': 1.2976432157636117, 'y': 1.062734989723492}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,238] Trial 821 finished with value: 1.4896818757778134 and parameters: {'x': 1.1257699454832488, 'y': 1.1459551779041277}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,255] Trial 822 finished with value: 0.7479872329779061 and parameters: {'x': 1.0567640756908125, 'y': 1.0304505392092613}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,271] Trial 823 finished with value: 5.175090869404562 and parameters: {'x': 1.1991282682062483, 'y': 1.211293537867537}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,288] Trial 824 finished with value: 0.006592879348613249 and parameters: {'x': 1.0267472300316878, 'y': 1.0465434116769634}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,305] Trial 825 finished with value: 0.6928217644362501 and parameters: {'x': 1.1117842504895126, 'y': 1.1535823400641783}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,322] Trial 826 finished with value: 51.986201005712594 and parameters: {'x': 1.3356465807266011, 'y': 1.063718893089113}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,338] Trial 827 finished with value: 3.521400926050035 and parameters: {'x': 1.026905206853453, 'y': 1.2421689764553594}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,355] Trial 828 finished with value: 9.272132397682986 and parameters: {'x': 1.1987103028911366, 'y': 1.1330536855926183}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,371] Trial 829 finished with value: 3.555295967394399 and parameters: {'x': 1.1029258732747995, 'y': 1.0281716857801664}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,388] Trial 830 finished with value: 4.788019043172794 and parameters: {'x': 1.0253530214480813, 'y': 1.2701495559652385}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,404] Trial 831 finished with value: 2.5012908491265833 and parameters: {'x': 1.2535964407299944, 'y': 1.7276123221657458}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,421] Trial 832 finished with value: 469.07076893687815 and parameters: {'x': 1.8032282936853803, 'y': 1.0873180846620814}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,437] Trial 833 finished with value: 0.2713954956616566 and parameters: {'x': 1.0243387506329595, 'y': 0.9972311288612892}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,453] Trial 834 finished with value: 1.5387694115253896 and parameters: {'x': 1.138622019724632, 'y': 1.1731899392374234}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,469] Trial 835 finished with value: 1.3238741156170375 and parameters: {'x': 1.0664492957767857, 'y': 1.0224464110672282}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,486] Trial 836 finished with value: 1.6224093933661936 and parameters: {'x': 0.9960708352115044, 'y': 1.1195303380903172}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,502] Trial 837 finished with value: 13.133254054401872 and parameters: {'x': 1.1812384950437889, 'y': 1.0333795429738022}. Best is trial 726 with value: 0.0009916955942820148.\n",
      "[I 2024-02-01 04:42:36,518] Trial 838 finished with value: 0.00012804617707374996 and parameters: {'x': 0.9923757174556217, 'y': 0.9856457254219398}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,534] Trial 839 finished with value: 0.4027392263137222 and parameters: {'x': 1.0850984056062136, 'y': 1.1145499588586498}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,550] Trial 840 finished with value: 2.124803026990547 and parameters: {'x': 1.0209121322646162, 'y': 1.1880136215751924}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,567] Trial 841 finished with value: 8.160952010198653 and parameters: {'x': 1.1329872164816663, 'y': 0.9982959422851322}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,583] Trial 842 finished with value: 7.702500547168671 and parameters: {'x': 1.0003436339813443, 'y': 1.2782211755048687}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,599] Trial 843 finished with value: 283.3136685417612 and parameters: {'x': 1.6604193629035917, 'y': 1.0750961654275348}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,616] Trial 844 finished with value: 1.6677032934164948 and parameters: {'x': 1.057731816202665, 'y': 0.9897861174943415}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,632] Trial 845 finished with value: 5.728972006247074 and parameters: {'x': 1.1712004930557876, 'y': 1.1329709376204569}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,648] Trial 846 finished with value: 0.0980070541276226 and parameters: {'x': 0.9844087410674026, 'y': 1.0003277994235817}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,665] Trial 847 finished with value: 29.852910071355524 and parameters: {'x': 1.2668906793408428, 'y': 1.0592860600180516}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,681] Trial 848 finished with value: 5.287398411837599 and parameters: {'x': 1.0962649513040987, 'y': 0.972054999837857}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,698] Trial 849 finished with value: 3.27591265008832 and parameters: {'x': 0.9932523108297692, 'y': 1.1675437196714513}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,714] Trial 850 finished with value: 2.158306606377137 and parameters: {'x': 0.9656499359980147, 'y': 1.079351398580595}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,730] Trial 851 finished with value: 4.855437520174909 and parameters: {'x': 1.084327992092534, 'y': 0.9555780414345452}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,747] Trial 852 finished with value: 1047.603935304766 and parameters: {'x': 1.1557818807826508, 'y': -1.9008017841859712}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,764] Trial 853 finished with value: 471.43491122961836 and parameters: {'x': 1.0409309997807419, 'y': -1.087713988485787}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,780] Trial 854 finished with value: 11.871736726544347 and parameters: {'x': 0.9450957199230102, 'y': 1.2377160393571858}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,796] Trial 855 finished with value: 18.56362074174295 and parameters: {'x': 1.2151450512520872, 'y': 1.0462597833201523}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,813] Trial 856 finished with value: 0.13994104231698068 and parameters: {'x': 0.9772944995056845, 'y': 0.9924442632628201}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,830] Trial 857 finished with value: 76.94922552661872 and parameters: {'x': 1.4125333215505982, 'y': 1.119013876547681}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,847] Trial 858 finished with value: 2.7895368865493495 and parameters: {'x': 1.0657503186511084, 'y': 0.9689341440019478}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,863] Trial 859 finished with value: 4.591643978226319 and parameters: {'x': 0.9872664729548708, 'y': 1.188972521765618}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,880] Trial 860 finished with value: 7.764529810660546 and parameters: {'x': 1.1094240029414855, 'y': 0.9523874932705787}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,897] Trial 861 finished with value: 4.602731681894944 and parameters: {'x': 0.9218300485257948, 'y': 1.0641679597238367}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,914] Trial 862 finished with value: 1.3840956448112491 and parameters: {'x': 1.0378149521428994, 'y': 0.9594730697555168}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,931] Trial 863 finished with value: 0.4253969566434128 and parameters: {'x': 1.16190532951262, 'y': 1.2868430149442889}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,948] Trial 864 finished with value: 1.7103364974223934 and parameters: {'x': 0.9571659297135202, 'y': 1.0468762853477829}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,965] Trial 865 finished with value: 5.4313248222848145 and parameters: {'x': 1.0793785193646703, 'y': 0.9321411816102521}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,982] Trial 866 finished with value: 10.384413037296795 and parameters: {'x': 0.8944196066550649, 'y': 1.1220619800841911}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:36,998] Trial 867 finished with value: 37.3150978540245 and parameters: {'x': 1.2731550735907355, 'y': 1.0106740224029254}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,015] Trial 868 finished with value: 2.07929446825214 and parameters: {'x': 1.028992108343469, 'y': 1.2029931996124106}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,031] Trial 869 finished with value: 2.8268684306145486 and parameters: {'x': 0.9604880929390246, 'y': 1.090623879402133}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,048] Trial 870 finished with value: 1.9577139746884014 and parameters: {'x': 0.8959490642737084, 'y': 0.9422556319357935}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,065] Trial 871 finished with value: 6.859228965191135 and parameters: {'x': 1.137565700640018, 'y': 1.032515962563109}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,081] Trial 872 finished with value: 1.2074317660221376 and parameters: {'x': 1.0190631343964003, 'y': 0.9286230089288878}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,097] Trial 873 finished with value: 9.689240817396588 and parameters: {'x': 0.901237542562747, 'y': 1.1233478442109013}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,114] Trial 874 finished with value: 4.494624665995509 and parameters: {'x': 1.095771375319179, 'y': 0.988926037816963}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,130] Trial 875 finished with value: 36.755959883747984 and parameters: {'x': 1.2311900955843031, 'y': 0.9100030779093462}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,146] Trial 876 finished with value: 0.6733827623522854 and parameters: {'x': 0.9832957839755491, 'y': 1.0489134980209702}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,163] Trial 877 finished with value: 13.736815924708186 and parameters: {'x': 0.8775725231710567, 'y': 1.1405633842417473}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,179] Trial 878 finished with value: 1.1774843614427326 and parameters: {'x': 1.0484096313210642, 'y': 1.2075666695996166}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,196] Trial 879 finished with value: 17.419395289453803 and parameters: {'x': 0.955267826631819, 'y': 1.3298781392863455}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,213] Trial 880 finished with value: 17.270157217127462 and parameters: {'x': 1.15338377639124, 'y': 0.9150035068743332}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,230] Trial 881 finished with value: 13.355118380393568 and parameters: {'x': 0.8268210190167851, 'y': 1.0486689877812296}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,246] Trial 882 finished with value: 1.1259256345648518 and parameters: {'x': 1.01585841027805, 'y': 0.9258705177957595}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,262] Trial 883 finished with value: 2.3710515009098283 and parameters: {'x': 0.9260790537849252, 'y': 1.011427069010465}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,279] Trial 884 finished with value: 50.48890506447762 and parameters: {'x': 1.0455226288099029, 'y': 1.8036584370770776}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,296] Trial 885 finished with value: 1.8450580482836374 and parameters: {'x': 1.1174714925042004, 'y': 1.113418536591619}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,314] Trial 886 finished with value: 2.8692353583416828 and parameters: {'x': 0.8512231665828617, 'y': 0.8933144211078803}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,331] Trial 887 finished with value: 0.6545220824799636 and parameters: {'x': 0.9538101070371285, 'y': 0.9905242951428702}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,348] Trial 888 finished with value: 306.05557620273544 and parameters: {'x': 1.191353053665231, 'y': -0.33001766329434823}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,365] Trial 889 finished with value: 0.07611765855099602 and parameters: {'x': 1.0783592896876215, 'y': 1.1893120145950777}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,381] Trial 890 finished with value: 64.1498186917577 and parameters: {'x': 1.325260356483549, 'y': 0.9560399073170687}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,398] Trial 891 finished with value: 454.35021029198356 and parameters: {'x': 0.8936377275898647, 'y': -1.3329343007970875}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,416] Trial 892 finished with value: 1.0335396872793252 and parameters: {'x': 0.9829933645402659, 'y': 1.067924882989651}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,433] Trial 893 finished with value: 5.3764720246418 and parameters: {'x': 0.8128087644122078, 'y': 0.8917734583909448}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,450] Trial 894 finished with value: 0.23597135031824806 and parameters: {'x': 1.0167696241916753, 'y': 0.9852725408113888}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,467] Trial 895 finished with value: 266.22640720710376 and parameters: {'x': 0.9028705962891433, 'y': -0.8164403674714931}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,485] Trial 896 finished with value: 0.8416524825948639 and parameters: {'x': 1.1657522434934178, 'y': 1.2687464431584299}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,502] Trial 897 finished with value: 0.43332604647805734 and parameters: {'x': 1.0809613042807804, 'y': 1.1031496050004388}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,519] Trial 898 finished with value: 0.0832191638845998 and parameters: {'x': 0.9477559300932175, 'y': 0.8698705925944128}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,537] Trial 899 finished with value: 12.57679374698765 and parameters: {'x': 0.828606292235739, 'y': 1.0408117343624024}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,554] Trial 900 finished with value: 0.4945801858551594 and parameters: {'x': 1.0135937496938974, 'y': 0.9570590334973065}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,571] Trial 901 finished with value: 11.812485098666519 and parameters: {'x': 1.1057710787417163, 'y': 0.8791995113827494}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,588] Trial 902 finished with value: 9.76824918591314 and parameters: {'x': 0.9209684399988615, 'y': 1.1606249132750246}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,606] Trial 903 finished with value: 27.01955415875149 and parameters: {'x': 1.2437498965535618, 'y': 1.027682256186744}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,623] Trial 904 finished with value: 0.1460539473720335 and parameters: {'x': 0.9919973612481696, 'y': 0.9458501393692258}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,641] Trial 905 finished with value: 13.576372427540313 and parameters: {'x': 0.8579902353490311, 'y': 1.104334772997355}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,659] Trial 906 finished with value: 7.895426345813061 and parameters: {'x': 1.0675326549049902, 'y': 0.8587191219825876}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,676] Trial 907 finished with value: 0.7817735239505997 and parameters: {'x': 0.9581992562821429, 'y': 1.0064649079061485}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,694] Trial 908 finished with value: 32.32110100410708 and parameters: {'x': 0.7929942728248502, 'y': 1.1969794210189213}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,711] Trial 909 finished with value: 54.6570654674409 and parameters: {'x': -0.28153256876998334, 'y': 0.807372802136778}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,729] Trial 910 finished with value: 16.450909750820053 and parameters: {'x': 1.1447012979248656, 'y': 0.9050020516877163}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,746] Trial 911 finished with value: 6.911196301738627 and parameters: {'x': 0.8985604157412955, 'y': 1.0701065822652929}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,764] Trial 912 finished with value: 0.20223241007538492 and parameters: {'x': 1.0187625433948215, 'y': 0.9929460195661411}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,781] Trial 913 finished with value: 11.25436822823172 and parameters: {'x': 0.9776296278434229, 'y': 1.2912275385862242}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,799] Trial 914 finished with value: 7.471339974571688 and parameters: {'x': 1.080755982378569, 'y': 0.8948152940920128}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,816] Trial 915 finished with value: 16.76262274771687 and parameters: {'x': 0.852955224528579, 'y': 1.136690293479385}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,834] Trial 916 finished with value: 19.380444244745547 and parameters: {'x': 1.179759492471268, 'y': 0.9519673583731438}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,851] Trial 917 finished with value: 0.20173256476150478 and parameters: {'x': 0.9316939576548409, 'y': 0.8236614186499687}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,868] Trial 918 finished with value: 1649.8808542355937 and parameters: {'x': 1.902202705856738, 'y': -0.4424953199496564}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,886] Trial 919 finished with value: 19.02130940434015 and parameters: {'x': 0.793796273567682, 'y': 1.0657590483915202}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,902] Trial 920 finished with value: 0.43017512606725106 and parameters: {'x': 1.0377220524666568, 'y': 1.0113878879895926}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,920] Trial 921 finished with value: 106.28919849287057 and parameters: {'x': 1.5022222492061652, 'y': 1.226929139033977}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,938] Trial 922 finished with value: 0.7871165115545277 and parameters: {'x': 0.9007658618416965, 'y': 0.899542004043455}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,956] Trial 923 finished with value: 12.573690678658762 and parameters: {'x': 1.0774050242349453, 'y': 0.8062920780401961}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,973] Trial 924 finished with value: 2.5816287428157683 and parameters: {'x': 0.9760760557132302, 'y': 1.113381131126297}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:37,990] Trial 925 finished with value: 29.204019295627706 and parameters: {'x': 1.2202985375942033, 'y': 0.9491703016379013}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,008] Trial 926 finished with value: 20.55791311511473 and parameters: {'x': 0.7721680697377808, 'y': 1.0490791021287906}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,025] Trial 927 finished with value: 19.84968281533402 and parameters: {'x': 1.1313910691732847, 'y': 0.8347097074755934}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,042] Trial 928 finished with value: 600.9168100455433 and parameters: {'x': -1.8377064607879694, 'y': 0.9422846855981413}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,061] Trial 929 finished with value: 19.713033915633233 and parameters: {'x': 0.8684041910288381, 'y': 1.1979244020708986}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,080] Trial 930 finished with value: 0.03440861918561461 and parameters: {'x': 1.00277527947314, 'y': 1.0241057453209608}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,098] Trial 931 finished with value: 0.007811497217544363 and parameters: {'x': 0.9306055478791897, 'y': 0.8715001738748713}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,115] Trial 932 finished with value: 0.5112310362396082 and parameters: {'x': 0.8500006146088164, 'y': 0.7924103589946302}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,132] Trial 933 finished with value: 7.08121268223203 and parameters: {'x': 1.064143335773222, 'y': 0.866372876751081}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,150] Trial 934 finished with value: 25.235631313213574 and parameters: {'x': 0.9224198146687318, 'y': 1.3531491926513388}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,167] Trial 935 finished with value: 1.2953988375068692 and parameters: {'x': 1.008717494411067, 'y': 1.131323234312571}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,184] Trial 936 finished with value: 2.3891119953502145 and parameters: {'x': 0.8038383602567094, 'y': 0.799473838546459}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,201] Trial 937 finished with value: 9.703078643837689 and parameters: {'x': 1.134459923264724, 'y': 0.9757920043908922}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,219] Trial 938 finished with value: 62.23410907054774 and parameters: {'x': 1.2887368562644452, 'y': 0.8724852778135632}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,238] Trial 939 finished with value: 6.781629355661759 and parameters: {'x': 0.9048265669668396, 'y': 1.0789527611751764}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,256] Trial 940 finished with value: 613.4456793793265 and parameters: {'x': 0.9912121238168115, 'y': -1.4942819278212123}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,274] Trial 941 finished with value: 16.281153090188045 and parameters: {'x': 0.763136280773982, 'y': 0.9851802683813998}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,293] Trial 942 finished with value: 16.98668722344718 and parameters: {'x': 1.0861258689105067, 'y': 0.7676103107748421}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,312] Trial 943 finished with value: 19.420207527009616 and parameters: {'x': 0.8503502265510613, 'y': 1.163524985377452}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,331] Trial 944 finished with value: 31.997383238497026 and parameters: {'x': 1.2036073513153824, 'y': 0.8833749168588808}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,349] Trial 945 finished with value: 0.6508408252192176 and parameters: {'x': 0.9688120523470276, 'y': 1.019211192053659}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,367] Trial 946 finished with value: 2.544301979004398 and parameters: {'x': 1.0477560816079834, 'y': 0.9383556297126961}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,386] Trial 947 finished with value: 16.44127832464369 and parameters: {'x': 0.9275382372623847, 'y': 1.2657408914850021}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,404] Trial 948 finished with value: 21.40499788879945 and parameters: {'x': 1.1207244073367297, 'y': 0.7935253755159598}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,422] Trial 949 finished with value: 10.179228349599317 and parameters: {'x': 0.8723370476738578, 'y': 1.0797654397380243}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,440] Trial 950 finished with value: 4.879781402283587 and parameters: {'x': 1.0403185518790286, 'y': 0.8613972141129169}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,458] Trial 951 finished with value: 0.04502274476072744 and parameters: {'x': 0.9696107227555698, 'y': 0.9611447719173367}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,477] Trial 952 finished with value: 60.98932656656389 and parameters: {'x': 1.3713145912286615, 'y': 1.1004303010522407}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,494] Trial 953 finished with value: 21.627885644279534 and parameters: {'x': 0.7446447845110793, 'y': 1.0188521768849037}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,512] Trial 954 finished with value: 2.5901730979492674 and parameters: {'x': 0.8642237218831501, 'y': 0.9072490313435561}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,530] Trial 955 finished with value: 3.373900702758219 and parameters: {'x': 1.021123328295909, 'y': 1.2263625145113473}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,548] Trial 956 finished with value: 32.04910799711552 and parameters: {'x': 1.1584660875147796, 'y': 0.7761461896557524}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,565] Trial 957 finished with value: 45.31959800697562 and parameters: {'x': -0.7101508982751535, 'y': 1.1554285886887148}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,583] Trial 958 finished with value: 1.2266374737847576 and parameters: {'x': 0.9312424021369109, 'y': 0.9777524434954206}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,600] Trial 959 finished with value: 10.267230736558442 and parameters: {'x': 1.0867558586672523, 'y': 0.8607305668507059}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,619] Trial 960 finished with value: 14.301686584712586 and parameters: {'x': 0.8063285583558488, 'y': 1.027845210729812}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,636] Trial 961 finished with value: 0.12817683379511052 and parameters: {'x': 0.9795069378639086, 'y': 0.9236907485363368}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,654] Trial 962 finished with value: 18.94785229197662 and parameters: {'x': 1.2327727904923782, 'y': 1.0850602687287818}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,672] Trial 963 finished with value: 0.632816274405456 and parameters: {'x': 0.9226021024713195, 'y': 0.7720223059415501}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,689] Trial 964 finished with value: 290.9663027407732 and parameters: {'x': -1.5929516455526538, 'y': 0.8515444579412489}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,707] Trial 965 finished with value: 1.2994672333655646 and parameters: {'x': 1.0434437614239256, 'y': 0.974863519775755}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,724] Trial 966 finished with value: 14.736771261450086 and parameters: {'x': 0.8633485265146678, 'y': 1.1290124074075192}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,742] Trial 967 finished with value: 5.075005156466716 and parameters: {'x': 1.1225864554550344, 'y': 1.0352564076786752}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,760] Trial 968 finished with value: 0.2705027544594762 and parameters: {'x': 0.9766964398488303, 'y': 0.9019782895404718}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,778] Trial 969 finished with value: 42.6749291856341 and parameters: {'x': 0.7517167320669647, 'y': 1.2178665608278723}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,796] Trial 970 finished with value: 6.834815031715491 and parameters: {'x': 1.0298977855742553, 'y': 0.7992717531909683}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,813] Trial 971 finished with value: 3.032275578252192 and parameters: {'x': 0.8820311932315656, 'y': 0.9517132751593542}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,830] Trial 972 finished with value: 7.0610603111464485 and parameters: {'x': 1.1524063011316261, 'y': 1.0627511457088008}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,847] Trial 973 finished with value: 0.36177052165867335 and parameters: {'x': 0.9537175758365652, 'y': 0.8496081842129357}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,865] Trial 974 finished with value: 43.43958856724771 and parameters: {'x': 0.8131172198853416, 'y': 1.3199817698060428}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,882] Trial 975 finished with value: 4.254635051443817 and parameters: {'x': 1.0889585339746257, 'y': 0.9797549397626101}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,899] Trial 976 finished with value: 1.2338306818799394 and parameters: {'x': 1.0174017469997667, 'y': 1.146170614241396}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,917] Trial 977 finished with value: 55.734247697933014 and parameters: {'x': -0.18152825862566824, 'y': 0.7700972615742534}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,934] Trial 978 finished with value: 0.9072973198714737 and parameters: {'x': 0.9063595965463659, 'y': 0.9162784770160619}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,951] Trial 979 finished with value: 19.640865312817294 and parameters: {'x': 1.2256229019281322, 'y': 1.0595460343782959}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,969] Trial 980 finished with value: 0.2431758063046512 and parameters: {'x': 0.9491978622994203, 'y': 0.8519261027839261}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:38,986] Trial 981 finished with value: 3.1743811385027403 and parameters: {'x': 1.0809017624739476, 'y': 0.9903644623037282}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,003] Trial 982 finished with value: 23.84093938594057 and parameters: {'x': 0.8321616658532268, 'y': 1.1804763337265576}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,021] Trial 983 finished with value: 0.9451937952071878 and parameters: {'x': 1.006284677688516, 'y': 0.9153898061640007}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,039] Trial 984 finished with value: 28.621379421258723 and parameters: {'x': 0.7442033189168878, 'y': 1.0882162317325472}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,057] Trial 985 finished with value: 526.313752665882 and parameters: {'x': 1.1585178549887862, 'y': -0.9519345134832267}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,075] Trial 986 finished with value: 0.22014431854026556 and parameters: {'x': 0.8966695014331879, 'y': 0.7582486160348789}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,092] Trial 987 finished with value: 0.9003453298834725 and parameters: {'x': 1.051714463771384, 'y': 1.0113578150424074}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,109] Trial 988 finished with value: 0.39677598317634927 and parameters: {'x': 0.9551203883146944, 'y': 0.8494248834678395}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,127] Trial 989 finished with value: 6.507245317023792 and parameters: {'x': 0.8277365093643958, 'y': 0.9396584478999452}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,145] Trial 990 finished with value: 49.723920145587435 and parameters: {'x': 1.09194084052354, 'y': 1.897426757939357}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,163] Trial 991 finished with value: 2.40740905222225 and parameters: {'x': 0.993307622224312, 'y': 1.1418168647773528}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,180] Trial 992 finished with value: 39.901405477264895 and parameters: {'x': 1.2949437107093769, 'y': 1.045892576291487}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,199] Trial 993 finished with value: 21.528741754213897 and parameters: {'x': 0.8927493853039146, 'y': 1.2608682460503169}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,218] Trial 994 finished with value: 30.17260585612963 and parameters: {'x': 1.1525748286503488, 'y': 0.7793447094839866}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,236] Trial 995 finished with value: 2.278393861568642 and parameters: {'x': 1.0153281074932696, 'y': 0.879955454031676}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,254] Trial 996 finished with value: 14.408253514197892 and parameters: {'x': 0.7927593238667421, 'y': 1.0074832399192202}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,272] Trial 997 finished with value: 0.5704735723681502 and parameters: {'x': 0.9268064414544249, 'y': 0.9341443959369862}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,291] Trial 998 finished with value: 0.4335322842896366 and parameters: {'x': 1.0770658699758067, 'y': 1.094680282215527}. Best is trial 838 with value: 0.00012804617707374996.\n",
      "[I 2024-02-01 04:42:39,310] Trial 999 finished with value: 9.933417271875873 and parameters: {'x': 0.7069803254542296, 'y': 0.8136293503615614}. Best is trial 838 with value: 0.00012804617707374996.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# while the logging from Optuna is very interesting,\n",
    "#                                  ^^^^\n",
    "# it is also very verbose!\n",
    "#            ^^^^^^^^^^^^\n",
    "# uncomment the line below to see it in all its glory\n",
    "#\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0165f97d-69bb-4be2-bc55-b577f6e3b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.9923757174556217, 'y': 0.9856457254219398}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110333d2-8616-4f71-8d24-902c76235463",
   "metadata": {},
   "source": [
    "Right, let's use `transformers.Trainer.hyperparameter_search` and Optuna to do a hyperparameter search for the best values (given a range) of:\n",
    "\n",
    "1. `num_train_epochs`\n",
    "2. `alpha`\n",
    "3. `temperature`\n",
    "\n",
    "We set up a function `hp_space` which returns a `dict` with the target hyperparameter names as keys, with `suggest_*` values. This function is what we pass to `hyperparameter_search` as the [`hp_space`](optuna.logging.set_verbosity(optuna.logging.WARNING)) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea88ba72-25dd-4631-ab13-fb5edbeb92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 2, 20)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8350b-43a1-4f93-8df9-ec3fd2a99988",
   "metadata": {},
   "source": [
    "#### For reproducing the results of `hyperparameter_search`\n",
    "\n",
    "In order to reproduce the results of `hyperparameter_search`, we pass in an additional keyword argument which will automagically be passed to the default Optuna backend as an argument to [`optuna.create_study`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html#optuna-study-create-study). \n",
    "\n",
    "From this [blogpost Optunaã§randomã®seedã‚’å›ºå®šã™ã‚‹æ–¹æ³•](https://qiita.com/phorizon20/items/1b795beb202c2dc378ed), something like this is possible for setting the seed that Optuna will use:\n",
    "\n",
    "    study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5ebab0e-3726-4d78-9d35-00fe39739961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-01 04:42:39,356] A new study created in memory with name: no-name-91501551-3ed5-4aea-aa26-16e37b2c46ab\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2862' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2862/2862 11:38, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.209209</td>\n",
       "      <td>0.583871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.099307</td>\n",
       "      <td>0.821290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.875806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.049595</td>\n",
       "      <td>0.907097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.907097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>0.914194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.917419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.921935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>0.922903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-0/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 04:54:18,393] Trial 0 finished with value: 0.9229032258064516 and parameters: {'num_train_epochs': 9, 'alpha': 0.751045085465288, 'temperature': 8.432597321474045}. Best is trial 0 with value: 0.9229032258064516.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1908' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1908/1908 07:34, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277097</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.124345</td>\n",
       "      <td>0.817419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.079072</td>\n",
       "      <td>0.873548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.060340</td>\n",
       "      <td>0.897419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.052026</td>\n",
       "      <td>0.902581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.905806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-1/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 05:01:53,470] Trial 1 finished with value: 0.9058064516129032 and parameters: {'num_train_epochs': 6, 'alpha': 0.4090011273795646, 'temperature': 3.4782450741530457}. Best is trial 0 with value: 0.9229032258064516.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1908' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1908/1908 07:33, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365046</td>\n",
       "      <td>0.625161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.823226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.082723</td>\n",
       "      <td>0.875806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.061436</td>\n",
       "      <td>0.903871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.910323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.050211</td>\n",
       "      <td>0.913871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-2/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 05:09:28,228] Trial 2 finished with value: 0.9138709677419354 and parameters: {'num_train_epochs': 6, 'alpha': 0.4138809245395255, 'temperature': 2.3270127935496596}. Best is trial 0 with value: 0.9229032258064516.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2862' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2862/2862 11:37, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193894</td>\n",
       "      <td>0.572903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.094275</td>\n",
       "      <td>0.811613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.064328</td>\n",
       "      <td>0.871290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.049092</td>\n",
       "      <td>0.905484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.041398</td>\n",
       "      <td>0.908065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.036268</td>\n",
       "      <td>0.913548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.033244</td>\n",
       "      <td>0.915806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.031803</td>\n",
       "      <td>0.920968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.031119</td>\n",
       "      <td>0.921290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-3/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 05:21:06,826] Trial 3 finished with value: 0.9212903225806451 and parameters: {'num_train_epochs': 9, 'alpha': 0.600103406406427, 'temperature': 18.5470789401325}. Best is trial 0 with value: 0.9229032258064516.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2226' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2226/2226 09:06, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.199906</td>\n",
       "      <td>0.568710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.099006</td>\n",
       "      <td>0.803226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.068649</td>\n",
       "      <td>0.866452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.053981</td>\n",
       "      <td>0.898710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.046372</td>\n",
       "      <td>0.904194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.042511</td>\n",
       "      <td>0.906774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.041180</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-4/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 05:30:14,861] Trial 4 finished with value: 0.9087096774193548 and parameters: {'num_train_epochs': 7, 'alpha': 0.555179672553343, 'temperature': 14.915369008812359}. Best is trial 0 with value: 0.9229032258064516.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='1908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/1908 00:53 < 04:27, 5.94 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.201611</td>\n",
       "      <td>0.561935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 05:31:15,098] Trial 5 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='637' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 637/1590 02:24 < 03:37, 4.38 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.258295</td>\n",
       "      <td>0.591935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.124571</td>\n",
       "      <td>0.807419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-6/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-6/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-6/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 05:33:47,270] Trial 6 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/1590 00:53 < 03:36, 5.88 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202373</td>\n",
       "      <td>0.550968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 05:34:48,004] Trial 7 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='637' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 637/1590 02:24 < 03:37, 4.38 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.275773</td>\n",
       "      <td>0.600968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.129618</td>\n",
       "      <td>0.809032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-8/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 05:37:20,159] Trial 8 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/3180 00:53 < 08:05, 5.89 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192195</td>\n",
       "      <td>0.576129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 05:38:20,763] Trial 9 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.586129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.098023</td>\n",
       "      <td>0.823871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.064892</td>\n",
       "      <td>0.876452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.048098</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.040034</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.916452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>0.919032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.924516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.028160</td>\n",
       "      <td>0.926129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.926129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-10/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 05:51:29,900] Trial 10 finished with value: 0.9261290322580645 and parameters: {'num_train_epochs': 10, 'alpha': 0.3948114252292986, 'temperature': 8.39196173495558}. Best is trial 10 with value: 0.9261290322580645.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2862' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2862/2862 11:36, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319171</td>\n",
       "      <td>0.630645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>0.834516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.069785</td>\n",
       "      <td>0.886774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.914516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.041034</td>\n",
       "      <td>0.918710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.035827</td>\n",
       "      <td>0.923226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.924516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>0.927742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.031145</td>\n",
       "      <td>0.927742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-11/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 06:03:07,614] Trial 11 finished with value: 0.927741935483871 and parameters: {'num_train_epochs': 9, 'alpha': 0.2305689274490612, 'temperature': 2.6309065979268995}. Best is trial 11 with value: 0.927741935483871.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.233483</td>\n",
       "      <td>0.609032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.104714</td>\n",
       "      <td>0.827419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.066487</td>\n",
       "      <td>0.881935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.048050</td>\n",
       "      <td>0.908710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.912258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.921290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.029156</td>\n",
       "      <td>0.926452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>0.927419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.027455</td>\n",
       "      <td>0.928065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-12/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 06:16:17,086] Trial 12 finished with value: 0.9280645161290323 and parameters: {'num_train_epochs': 10, 'alpha': 0.5858821400787321, 'temperature': 4.917005721212045}. Best is trial 12 with value: 0.9280645161290323.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/2862 00:53 < 07:08, 5.93 it/s, Epoch 1/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.201224</td>\n",
       "      <td>0.576452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:17:17,368] Trial 13 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2226\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/2226 00:53 < 05:24, 5.88 it/s, Epoch 1/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203813</td>\n",
       "      <td>0.569677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:18:18,148] Trial 14 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/1590 00:53 < 03:35, 5.90 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.204437</td>\n",
       "      <td>0.552581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:19:18,641] Trial 15 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.238547</td>\n",
       "      <td>0.610645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.105879</td>\n",
       "      <td>0.829032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.882258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.909677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.039610</td>\n",
       "      <td>0.913226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.919677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.030634</td>\n",
       "      <td>0.921290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.029086</td>\n",
       "      <td>0.927097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>0.928710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>0.928065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/run-16/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[I 2024-02-01 06:32:29,669] Trial 16 finished with value: 0.9280645161290323 and parameters: {'num_train_epochs': 10, 'alpha': 0.7230101100179304, 'temperature': 4.602598234145571}. Best is trial 12 with value: 0.9280645161290323.\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/1590 00:53 < 03:34, 5.91 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.246514</td>\n",
       "      <td>0.588065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:33:30,238] Trial 17 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='2862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/2862 00:54 < 07:13, 5.87 it/s, Epoch 1/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.208468</td>\n",
       "      <td>0.585161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:34:31,079] Trial 18 pruned. \n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 319/3180 00:53 < 08:04, 5.91 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.213973</td>\n",
       "      <td>0.590645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "[I 2024-02-01 06:35:31,556] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 30min 16s, sys: 1min 32s, total: 1h 31min 48s\n",
      "Wall time: 1h 52min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_run = distilbert_trainer.hyperparameter_search(\n",
    "    n_trials=20,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space,\n",
    "    sampler=optuna.samplers.RandomSampler(seed=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0888eed-5024-4d50-a5ea-ef4982626bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='12', objective=0.9280645161290323, hyperparameters={'num_train_epochs': 10, 'alpha': 0.5858821400787321, 'temperature': 4.917005721212045})\n"
     ]
    }
   ],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "738b70ea-fd1e-45d2-ad48-0e7f24c28578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/a_naughty_alpaca/dev/github/transformers-gcp/distilbert-base-uncased-distilled-clinc is already a clone of https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3180/3180 13:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.002947</td>\n",
       "      <td>0.690968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>1.058458</td>\n",
       "      <td>0.862581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.905806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.937800</td>\n",
       "      <td>0.407230</td>\n",
       "      <td>0.934839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.327355</td>\n",
       "      <td>0.938710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.295066</td>\n",
       "      <td>0.942581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.273408</td>\n",
       "      <td>0.943871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.262521</td>\n",
       "      <td>0.945161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.256618</td>\n",
       "      <td>0.945161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.256465</td>\n",
       "      <td>0.944516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-1000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-1500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-1500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-2000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-2500\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-2500/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc/checkpoint-3000\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/checkpoint-3000/special_tokens_map.json\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3180, training_loss=0.6848866198797646, metrics={'train_runtime': 786.282, 'train_samples_per_second': 193.951, 'train_steps_per_second': 4.044, 'total_flos': 826980744547356.0, 'train_loss': 0.6848866198797646, 'epoch': 10.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(student_training_args, k, v)\n",
    "\n",
    "distilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\n",
    "student_training_args.output_dir = distilled_ckpt\n",
    "\n",
    "distil_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc[\"train\"],\n",
    "    eval_dataset=clinc_enc[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer\n",
    ")\n",
    "\n",
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71361c83-e4dc-42a5-bfec-f8ee66667dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-distilled-clinc\n",
      "Configuration saved in distilbert-base-uncased-distilled-clinc/config.json\n",
      "Model weights saved in distilbert-base-uncased-distilled-clinc/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-distilled-clinc/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-distilled-clinc/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900b0a06e87e4507a5ba1999470a54b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc\n",
      "   b820ac1..a07aa91  main -> main\n",
      "\n",
      "To https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc\n",
      "   a07aa91..422575f  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't forget to add the knowledge-distillation hyperparameters to your model card! (see best_run)\n"
     ]
    }
   ],
   "source": [
    "distil_trainer.push_to_hub(\"Distilled student model training completed\")\n",
    "\n",
    "print(\"don't forget to add the knowledge-distillation hyperparameters to your model card! (see best_run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c58fc-9801-4fb1-b279-9d1321fe7f1a",
   "metadata": {},
   "source": [
    "### Benchmarking Our Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45535ffb-dd79-4f2f-91f6-a1aa8aeffedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/a_naughty_alpaca/.cache/huggingface/transformers/tmpzqtdiv7g\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97228296775b45ff960eceea00483060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin in cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.c49a33998a0ce5fd91087350c29aa2e491c1a96c1bb519b84c219bf510f6342f\n",
      "creating metadata file for /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.c49a33998a0ce5fd91087350c29aa2e491c1a96c1bb519b84c219bf510f6342f\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.c49a33998a0ce5fd91087350c29aa2e491c1a96c1bb519b84c219bf510f6342f\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-distilled-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d6a6960a277dee54cae995412a5d1d91c3c01a7205ee9a021c5acf723c1009ac.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d765a80fb92038143cb9ca38cacd10d0ef078b1c6be4bf707d68bc482fca65ed.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/830798532de151eb7a28a667da288d249368317cbced033a2a391b4e43660895.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/95607bd84ec8aee89fe4703d88bbb0a4801beac0f3d0d02b83e17841172ec344.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 255.88\n",
      "Average latency (ms) - 11.83 +\\- 0.20\n",
      "Accuracy on test set - 0.875\n"
     ]
    }
   ],
   "source": [
    "distilled_ckpt = \"buruzaemon/distilbert-base-uncased-distilled-clinc\"\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=distilled_ckpt)\n",
    "optim_type = \"Distillation\"\n",
    "\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad1d0e79-6f9d-409f-b75a-4b19ac196d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69af08a-6a04-42f4-a6b0-24643e9c2853",
   "metadata": {},
   "source": [
    "## Making Models Faster with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b34fe-cbfb-475d-91fd-34f302e630da",
   "metadata": {},
   "source": [
    "#### The range of model weight values is actually pretty limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a51eeec-bd96-4aae-b6e7-8bf2654dee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_dict = pipe.model.state_dict()\n",
    "\n",
    "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
    "\n",
    "plt.hist(\n",
    "    weights.flatten().numpy(),\n",
    "    bins=250,\n",
    "    range=(-0.3, 0.3),\n",
    "    edgecolor=\"C0\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf83e0e-a475-471f-b69f-746ad441c409",
   "metadata": {},
   "source": [
    "_Discretization_ means taking the floating-point values $f$ in a tensor; map their range $\\left[ f_{max}, f_{min} \\right]$ into a smaller one $\\left[ q_{max}, q_{min} \\right]$ of fixed-point number $q$; then linearly distributing all values in between (the affine bit).\n",
    "\n",
    "\\begin{align}\n",
    "  f = \\left( \\frac{f_{max} - f_{min}}{q_{max} - q_{min}} \\right) \\left(q - Z\\right)\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "$\\left( \\frac{f_{max} - f_{min}}{q_{max} - q_{min}} \\right)$ is a scaling factor \n",
    "\n",
    "\n",
    "and _zero point_ $Z$, of the same type as $q$, is the quantized value of floating-point value $f = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bb0f809-7c94-4adf-a377-67352d6200c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_point = 0\n",
    "scale = (weights.max() - weights.min()) / (127 - (-128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb06ad-8cf8-442a-a110-606afd68cf67",
   "metadata": {},
   "source": [
    "#### Quantization the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a374a2db-936c-4346-9211-cbd55d869d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,  -8,   0,  ...,  -6,  -3,   9],\n",
       "        [  9,   2,   1,  ...,  -4,   7,   0],\n",
       "        [ -9,  -6,   5,  ...,   1,   6,  -3],\n",
       "        ...,\n",
       "        [  6,   0,  13,  ...,   0,   6,  -1],\n",
       "        [  0,  -2, -12,  ...,  12,  -7, -13],\n",
       "        [-13,  -1,  -9,  ...,   8,   2,  -1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights / scale + zero_point).clamp(-128, 127).round().char()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d861966-10f2-435f-aefc-867196abfe9e",
   "metadata": {},
   "source": [
    "#### Quantization the PyTorch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "691922d2-13e3-4ab4-8a4e-b891c3fea581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,  -8,   0,  ...,  -6,  -3,   9],\n",
       "        [  9,   2,   1,  ...,  -4,   7,   0],\n",
       "        [ -9,  -6,   5,  ...,   1,   6,  -3],\n",
       "        ...,\n",
       "        [  6,   0,  13,  ...,   0,   6,  -1],\n",
       "        [  0,  -2, -12,  ...,  12,  -7, -13],\n",
       "        [-13,  -1,  -9,  ...,   8,   2,  -1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import quantize_per_tensor\n",
    "\n",
    "dtype = torch.qint8\n",
    "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
    "quantized_weights.int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99a8aff2-98e7-4fec-ade5-e11454183fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ms Â± 11.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "weights @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30278aee-9146-43bc-a5c0-ea5fa591dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.quantized import QFunctional\n",
    "\n",
    "q_fn = QFunctional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5258e466-426d-4cea-9349-7baf9d26a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 Âµs Â± 4.54 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "q_fn.mul(quantized_weights, quantized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3194aad8-025d-4417-ae05-0b7b4482628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_60703/2099325647.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.999755879241598"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7ed1a75-bde4-448f-8560-5517af1cb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/vocab.txt from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d6a6960a277dee54cae995412a5d1d91c3c01a7205ee9a021c5acf723c1009ac.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/d765a80fb92038143cb9ca38cacd10d0ef078b1c6be4bf707d68bc482fca65ed.848c414913cfee271695b8761d3e947fb18a724fbad549de63228b20e5f2d615\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/special_tokens_map.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/830798532de151eb7a28a667da288d249368317cbced033a2a391b4e43660895.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/tokenizer_config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/95607bd84ec8aee89fe4703d88bbb0a4801beac0f3d0d02b83e17841172ec344.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "loading configuration file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/config.json from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/440ff9e1a37c55160f5b9f63273c460e8a11b91427d5137024ac04391627b080.332a0a2671a37b2b28094f55b6982c2256246ecec6ece34c3e29448b159520ae\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"buruzaemon/distilbert-base-uncased-distilled-clinc\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"restaurant_reviews\",\n",
      "    \"1\": \"nutrition_info\",\n",
      "    \"2\": \"account_blocked\",\n",
      "    \"3\": \"oil_change_how\",\n",
      "    \"4\": \"time\",\n",
      "    \"5\": \"weather\",\n",
      "    \"6\": \"redeem_rewards\",\n",
      "    \"7\": \"interest_rate\",\n",
      "    \"8\": \"gas_type\",\n",
      "    \"9\": \"accept_reservations\",\n",
      "    \"10\": \"smart_home\",\n",
      "    \"11\": \"user_name\",\n",
      "    \"12\": \"report_lost_card\",\n",
      "    \"13\": \"repeat\",\n",
      "    \"14\": \"whisper_mode\",\n",
      "    \"15\": \"what_are_your_hobbies\",\n",
      "    \"16\": \"order\",\n",
      "    \"17\": \"jump_start\",\n",
      "    \"18\": \"schedule_meeting\",\n",
      "    \"19\": \"meeting_schedule\",\n",
      "    \"20\": \"freeze_account\",\n",
      "    \"21\": \"what_song\",\n",
      "    \"22\": \"meaning_of_life\",\n",
      "    \"23\": \"restaurant_reservation\",\n",
      "    \"24\": \"traffic\",\n",
      "    \"25\": \"make_call\",\n",
      "    \"26\": \"text\",\n",
      "    \"27\": \"bill_balance\",\n",
      "    \"28\": \"improve_credit_score\",\n",
      "    \"29\": \"change_language\",\n",
      "    \"30\": \"no\",\n",
      "    \"31\": \"measurement_conversion\",\n",
      "    \"32\": \"timer\",\n",
      "    \"33\": \"flip_coin\",\n",
      "    \"34\": \"do_you_have_pets\",\n",
      "    \"35\": \"balance\",\n",
      "    \"36\": \"tell_joke\",\n",
      "    \"37\": \"last_maintenance\",\n",
      "    \"38\": \"exchange_rate\",\n",
      "    \"39\": \"uber\",\n",
      "    \"40\": \"car_rental\",\n",
      "    \"41\": \"credit_limit\",\n",
      "    \"42\": \"oos\",\n",
      "    \"43\": \"shopping_list\",\n",
      "    \"44\": \"expiration_date\",\n",
      "    \"45\": \"routing\",\n",
      "    \"46\": \"meal_suggestion\",\n",
      "    \"47\": \"tire_change\",\n",
      "    \"48\": \"todo_list\",\n",
      "    \"49\": \"card_declined\",\n",
      "    \"50\": \"rewards_balance\",\n",
      "    \"51\": \"change_accent\",\n",
      "    \"52\": \"vaccines\",\n",
      "    \"53\": \"reminder_update\",\n",
      "    \"54\": \"food_last\",\n",
      "    \"55\": \"change_ai_name\",\n",
      "    \"56\": \"bill_due\",\n",
      "    \"57\": \"who_do_you_work_for\",\n",
      "    \"58\": \"share_location\",\n",
      "    \"59\": \"international_visa\",\n",
      "    \"60\": \"calendar\",\n",
      "    \"61\": \"translate\",\n",
      "    \"62\": \"carry_on\",\n",
      "    \"63\": \"book_flight\",\n",
      "    \"64\": \"insurance_change\",\n",
      "    \"65\": \"todo_list_update\",\n",
      "    \"66\": \"timezone\",\n",
      "    \"67\": \"cancel_reservation\",\n",
      "    \"68\": \"transactions\",\n",
      "    \"69\": \"credit_score\",\n",
      "    \"70\": \"report_fraud\",\n",
      "    \"71\": \"spending_history\",\n",
      "    \"72\": \"directions\",\n",
      "    \"73\": \"spelling\",\n",
      "    \"74\": \"insurance\",\n",
      "    \"75\": \"what_is_your_name\",\n",
      "    \"76\": \"reminder\",\n",
      "    \"77\": \"where_are_you_from\",\n",
      "    \"78\": \"distance\",\n",
      "    \"79\": \"payday\",\n",
      "    \"80\": \"flight_status\",\n",
      "    \"81\": \"find_phone\",\n",
      "    \"82\": \"greeting\",\n",
      "    \"83\": \"alarm\",\n",
      "    \"84\": \"order_status\",\n",
      "    \"85\": \"confirm_reservation\",\n",
      "    \"86\": \"cook_time\",\n",
      "    \"87\": \"damaged_card\",\n",
      "    \"88\": \"reset_settings\",\n",
      "    \"89\": \"pin_change\",\n",
      "    \"90\": \"replacement_card_duration\",\n",
      "    \"91\": \"new_card\",\n",
      "    \"92\": \"roll_dice\",\n",
      "    \"93\": \"income\",\n",
      "    \"94\": \"taxes\",\n",
      "    \"95\": \"date\",\n",
      "    \"96\": \"who_made_you\",\n",
      "    \"97\": \"pto_request\",\n",
      "    \"98\": \"tire_pressure\",\n",
      "    \"99\": \"how_old_are_you\",\n",
      "    \"100\": \"rollover_401k\",\n",
      "    \"101\": \"pto_request_status\",\n",
      "    \"102\": \"how_busy\",\n",
      "    \"103\": \"application_status\",\n",
      "    \"104\": \"recipe\",\n",
      "    \"105\": \"calendar_update\",\n",
      "    \"106\": \"play_music\",\n",
      "    \"107\": \"yes\",\n",
      "    \"108\": \"direct_deposit\",\n",
      "    \"109\": \"credit_limit_change\",\n",
      "    \"110\": \"gas\",\n",
      "    \"111\": \"pay_bill\",\n",
      "    \"112\": \"ingredients_list\",\n",
      "    \"113\": \"lost_luggage\",\n",
      "    \"114\": \"goodbye\",\n",
      "    \"115\": \"what_can_i_ask_you\",\n",
      "    \"116\": \"book_hotel\",\n",
      "    \"117\": \"are_you_a_bot\",\n",
      "    \"118\": \"next_song\",\n",
      "    \"119\": \"change_speed\",\n",
      "    \"120\": \"plug_type\",\n",
      "    \"121\": \"maybe\",\n",
      "    \"122\": \"w2\",\n",
      "    \"123\": \"oil_change_when\",\n",
      "    \"124\": \"thank_you\",\n",
      "    \"125\": \"shopping_list_update\",\n",
      "    \"126\": \"pto_balance\",\n",
      "    \"127\": \"order_checks\",\n",
      "    \"128\": \"travel_alert\",\n",
      "    \"129\": \"fun_fact\",\n",
      "    \"130\": \"sync_device\",\n",
      "    \"131\": \"schedule_maintenance\",\n",
      "    \"132\": \"apr\",\n",
      "    \"133\": \"transfer\",\n",
      "    \"134\": \"ingredient_substitution\",\n",
      "    \"135\": \"calories\",\n",
      "    \"136\": \"current_location\",\n",
      "    \"137\": \"international_fees\",\n",
      "    \"138\": \"calculator\",\n",
      "    \"139\": \"definition\",\n",
      "    \"140\": \"next_holiday\",\n",
      "    \"141\": \"update_playlist\",\n",
      "    \"142\": \"mpg\",\n",
      "    \"143\": \"min_payment\",\n",
      "    \"144\": \"change_user_name\",\n",
      "    \"145\": \"restaurant_suggestion\",\n",
      "    \"146\": \"travel_notification\",\n",
      "    \"147\": \"cancel\",\n",
      "    \"148\": \"pto_used\",\n",
      "    \"149\": \"travel_suggestion\",\n",
      "    \"150\": \"change_volume\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"accept_reservations\": 9,\n",
      "    \"account_blocked\": 2,\n",
      "    \"alarm\": 83,\n",
      "    \"application_status\": 103,\n",
      "    \"apr\": 132,\n",
      "    \"are_you_a_bot\": 117,\n",
      "    \"balance\": 35,\n",
      "    \"bill_balance\": 27,\n",
      "    \"bill_due\": 56,\n",
      "    \"book_flight\": 63,\n",
      "    \"book_hotel\": 116,\n",
      "    \"calculator\": 138,\n",
      "    \"calendar\": 60,\n",
      "    \"calendar_update\": 105,\n",
      "    \"calories\": 135,\n",
      "    \"cancel\": 147,\n",
      "    \"cancel_reservation\": 67,\n",
      "    \"car_rental\": 40,\n",
      "    \"card_declined\": 49,\n",
      "    \"carry_on\": 62,\n",
      "    \"change_accent\": 51,\n",
      "    \"change_ai_name\": 55,\n",
      "    \"change_language\": 29,\n",
      "    \"change_speed\": 119,\n",
      "    \"change_user_name\": 144,\n",
      "    \"change_volume\": 150,\n",
      "    \"confirm_reservation\": 85,\n",
      "    \"cook_time\": 86,\n",
      "    \"credit_limit\": 41,\n",
      "    \"credit_limit_change\": 109,\n",
      "    \"credit_score\": 69,\n",
      "    \"current_location\": 136,\n",
      "    \"damaged_card\": 87,\n",
      "    \"date\": 95,\n",
      "    \"definition\": 139,\n",
      "    \"direct_deposit\": 108,\n",
      "    \"directions\": 72,\n",
      "    \"distance\": 78,\n",
      "    \"do_you_have_pets\": 34,\n",
      "    \"exchange_rate\": 38,\n",
      "    \"expiration_date\": 44,\n",
      "    \"find_phone\": 81,\n",
      "    \"flight_status\": 80,\n",
      "    \"flip_coin\": 33,\n",
      "    \"food_last\": 54,\n",
      "    \"freeze_account\": 20,\n",
      "    \"fun_fact\": 129,\n",
      "    \"gas\": 110,\n",
      "    \"gas_type\": 8,\n",
      "    \"goodbye\": 114,\n",
      "    \"greeting\": 82,\n",
      "    \"how_busy\": 102,\n",
      "    \"how_old_are_you\": 99,\n",
      "    \"improve_credit_score\": 28,\n",
      "    \"income\": 93,\n",
      "    \"ingredient_substitution\": 134,\n",
      "    \"ingredients_list\": 112,\n",
      "    \"insurance\": 74,\n",
      "    \"insurance_change\": 64,\n",
      "    \"interest_rate\": 7,\n",
      "    \"international_fees\": 137,\n",
      "    \"international_visa\": 59,\n",
      "    \"jump_start\": 17,\n",
      "    \"last_maintenance\": 37,\n",
      "    \"lost_luggage\": 113,\n",
      "    \"make_call\": 25,\n",
      "    \"maybe\": 121,\n",
      "    \"meal_suggestion\": 46,\n",
      "    \"meaning_of_life\": 22,\n",
      "    \"measurement_conversion\": 31,\n",
      "    \"meeting_schedule\": 19,\n",
      "    \"min_payment\": 143,\n",
      "    \"mpg\": 142,\n",
      "    \"new_card\": 91,\n",
      "    \"next_holiday\": 140,\n",
      "    \"next_song\": 118,\n",
      "    \"no\": 30,\n",
      "    \"nutrition_info\": 1,\n",
      "    \"oil_change_how\": 3,\n",
      "    \"oil_change_when\": 123,\n",
      "    \"oos\": 42,\n",
      "    \"order\": 16,\n",
      "    \"order_checks\": 127,\n",
      "    \"order_status\": 84,\n",
      "    \"pay_bill\": 111,\n",
      "    \"payday\": 79,\n",
      "    \"pin_change\": 89,\n",
      "    \"play_music\": 106,\n",
      "    \"plug_type\": 120,\n",
      "    \"pto_balance\": 126,\n",
      "    \"pto_request\": 97,\n",
      "    \"pto_request_status\": 101,\n",
      "    \"pto_used\": 148,\n",
      "    \"recipe\": 104,\n",
      "    \"redeem_rewards\": 6,\n",
      "    \"reminder\": 76,\n",
      "    \"reminder_update\": 53,\n",
      "    \"repeat\": 13,\n",
      "    \"replacement_card_duration\": 90,\n",
      "    \"report_fraud\": 70,\n",
      "    \"report_lost_card\": 12,\n",
      "    \"reset_settings\": 88,\n",
      "    \"restaurant_reservation\": 23,\n",
      "    \"restaurant_reviews\": 0,\n",
      "    \"restaurant_suggestion\": 145,\n",
      "    \"rewards_balance\": 50,\n",
      "    \"roll_dice\": 92,\n",
      "    \"rollover_401k\": 100,\n",
      "    \"routing\": 45,\n",
      "    \"schedule_maintenance\": 131,\n",
      "    \"schedule_meeting\": 18,\n",
      "    \"share_location\": 58,\n",
      "    \"shopping_list\": 43,\n",
      "    \"shopping_list_update\": 125,\n",
      "    \"smart_home\": 10,\n",
      "    \"spelling\": 73,\n",
      "    \"spending_history\": 71,\n",
      "    \"sync_device\": 130,\n",
      "    \"taxes\": 94,\n",
      "    \"tell_joke\": 36,\n",
      "    \"text\": 26,\n",
      "    \"thank_you\": 124,\n",
      "    \"time\": 4,\n",
      "    \"timer\": 32,\n",
      "    \"timezone\": 66,\n",
      "    \"tire_change\": 47,\n",
      "    \"tire_pressure\": 98,\n",
      "    \"todo_list\": 48,\n",
      "    \"todo_list_update\": 65,\n",
      "    \"traffic\": 24,\n",
      "    \"transactions\": 68,\n",
      "    \"transfer\": 133,\n",
      "    \"translate\": 61,\n",
      "    \"travel_alert\": 128,\n",
      "    \"travel_notification\": 146,\n",
      "    \"travel_suggestion\": 149,\n",
      "    \"uber\": 39,\n",
      "    \"update_playlist\": 141,\n",
      "    \"user_name\": 11,\n",
      "    \"vaccines\": 52,\n",
      "    \"w2\": 122,\n",
      "    \"weather\": 5,\n",
      "    \"what_are_your_hobbies\": 15,\n",
      "    \"what_can_i_ask_you\": 115,\n",
      "    \"what_is_your_name\": 75,\n",
      "    \"what_song\": 21,\n",
      "    \"where_are_you_from\": 77,\n",
      "    \"whisper_mode\": 14,\n",
      "    \"who_do_you_work_for\": 57,\n",
      "    \"who_made_you\": 96,\n",
      "    \"yes\": 107\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/buruzaemon/distilbert-base-uncased-distilled-clinc/resolve/main/pytorch_model.bin from cache at /home/a_naughty_alpaca/.cache/huggingface/transformers/276e8f877903dae9bee5d2f28dcf703a974e430630b3491539dac90347fb0d9e.c49a33998a0ce5fd91087350c29aa2e491c1a96c1bb519b84c219bf510f6342f\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at buruzaemon/distilbert-base-uncased-distilled-clinc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import random\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# take our last, best model...\n",
    "model_ckpt = \"buruzaemon/distilbert-base-uncased-distilled-clinc\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = (\n",
    "    AutoModelForSequenceClassification\n",
    "        .from_pretrained(model_ckpt)\n",
    "        #.to(\"cpu\")\n",
    ")\n",
    "\n",
    "model_quantized = quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97395aa2-0875-40cf-85b8-5d1df464b1f1",
   "metadata": {},
   "source": [
    "### Benchmarking Our Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87e9afea-3edc-4905-bf09-9bab941bd8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 132.39\n",
      "Average latency (ms) - 8.68 +\\- 0.18\n",
      "Accuracy on test set - 0.885\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_quantized,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "optim_type = \"Distillation + quantization\"\n",
    "\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bfafac9-0da4-4044-a54b-3e91692f1c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAG2CAYAAABoEokhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABeGElEQVR4nO3dd3hUZf7//9dkkkwmnYRUShIkdKTLgoiI9LKAooBKdZVdZQFdEVBQQAFREQR+Lq4rRQELCogsIL2JIh0pUjT0xFBCOimT8/sjX+ZjDCUJyUwIz8d1zQVz5j73/Z4MQ+Y197nPMRmGYQgAAAAAHMTF2QUAAAAAuLsQQgAAAAA4FCEEAAAAgEMRQgAAAAA4FCEEAAAAgEMRQgAAAAA4FCEEAAAAgEMRQgAAAAA4FCEEAAAAgEMRQgAAAAA4lFNDSHJysoYPH66IiAhZrVY1b95cO3futD9uGIbGjRun8PBwWa1WtWrVSocOHXJixQAAAABul1NDyN/+9jetXbtWn376qX7++We1a9dObdq00blz5yRJb7/9tt577z3NmjVLO3fuVGhoqNq2bavk5GRnlg0AAADgNpgMwzCcMXB6erp8fHz0zTffqHPnzvbt9evXV5cuXfTGG28oPDxcw4cP18iRIyVJGRkZCgkJ0ZQpUzR48GBnlA0AAADgNrk6a+Ds7GzZbDZ5eHjk2W61WrVt2zbFxMQoLi5O7dq1sz9msVj04IMPavv27TcMIRkZGcrIyLDfz8nJ0eXLlxUYGCiTyVQyTwYAABQrwzCUnJys8PBwubiwhBUoa5wWQnx8fNSsWTO98cYbqlmzpkJCQvTZZ59px44dio6OVlxcnCQpJCQkz34hISE6derUDfudPHmyxo8fX6K1AwAAxzhz5owqVqzo7DIAFDOnhRBJ+vTTTzVo0CBVqFBBZrNZDRs21BNPPKE9e/bY2/x59sIwjJvOaIwePVovvvii/X5iYqIqV66sM2fOyNfXt/ifBAAAKHZJSUmqVKmSfHx8nF0KgBLg1BByzz33aPPmzUpNTVVSUpLCwsLUq1cvRUVFKTQ0VJIUFxensLAw+z7x8fH5Zkf+yGKxyGKx5Nvu6+tLCAEA4A7DodRA2VQqDrL08vJSWFiYEhIS9N1336lbt272ILJ27Vp7u8zMTG3evFnNmzd3YrUAAAAAbodTZ0K+++47GYah6tWr68SJExoxYoSqV6+ugQMHymQyafjw4Zo0aZKio6MVHR2tSZMmydPTU0888YQzywYAAABwG5waQhITEzV69GidPXtWAQEBevTRRzVx4kS5ublJkl5++WWlp6frueeeU0JCgpo2bao1a9ZwfCgAAABwB3PadUIcJSkpSX5+fkpMTGRNCAAAdwh+fwNlW6lYEwIAAADg7kEIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQhBAAAAAADkUIAQAAAOBQrs4uACiLbCmpSt+/T7YriZJhyOznJ+u9dWX283N2aQAAAE5HCAGKUebZc8qOPS+T1VOJS5cpK/53SZJrQKBcPK2SyUWuAeXkHhnp3EIBAACciBACFAPDMJS2c5cSv/1WsmUroF8/ud9TRR61akomk3JS0+Ti46PLn34q2XLk26mjvO6/XyaTydmlAwAAOBwhBCgGV/fv15Wvv1b2hQuy3HOPXLx9FPSPf+Rpk33hgsw+vso4dkxXli6VzGZ5N2vmpIoBAACch4XpQDHIunBRysmRZ8OGKv/cP+ResUK+Nq5BQSr/j7/L8y9/kcnkIlt8vBMqBQAAcD5mQoBi4NPmYVkiI2QuX16u5crdsJ3Zx0flej2urPPnZYmOdmCFAAAApQchBLhNlxcuVE5auvy6dL5pALnG7OMjo3x5Xf7kEykrWwFPD5LJhUlJAABw9+CTD3AbjKwsZZw4ofQ9e5R1/nyB98v+/Xel79mrjFOnlJOSUoIVAgAAlD6EEOA2GFlZMmw2ydVVJotHgfczeXhIrmbJli0jM7MEKwQAACh9CCHAbTC5uclkNkvZ2TIyrhZ4P+PqVSnbJpldZXJ3L8EKAQAASh/WhAC3weTmJkvVqnKvUFFu4eEF3s81JETWRg2lzCy5eHuXYIUAAAClDyEEuE0BTz4pwzCUeeKEshMSbrk43ZacrOyLFxXQty8L0gEAwF2JT0BAMUhet14X//OREhYuUnZCwg3b2ZKTlfDFF7r00UdK+vZbB1YIAABQejATAhQDt6DykotJaXv2yJaYqID+/fNdsDD7wgVdmjtPGceOysXPX+bQUCdVCwAA4FyEEKAYeNSrJ//MLCV++61yUlOUk5KsC//+t1zc3CWzi3JSU+XTrp1syUlyDQ6Rb6eO8rrvPmeXDQAA4BSEEKAYmEwmed3XRG7h4cqOPS8j26bMX39TVvzvkiTXgEB5NUuWf49H5BpQTu6Rkc4tGAAAwIkIIUAxcq9YQe4VK8iWkiq/Ht1lu5IoGYbMfn6yVK0qs5+fs0sEAABwOkIIUALM3l7yvv9+Z5cBAABQKnF2LAAAAAAORQgBAAAA4FCEEAAAAAAO5dQQkp2drTFjxigqKkpWq1VVqlTRhAkTlJOTY2+TkpKiIUOGqGLFirJarapZs6b+/e9/O7FqAAAAALfDqQvTp0yZotmzZ2v+/PmqXbu2du3apYEDB8rPz0/Dhg2TJL3wwgvauHGjFixYoMjISK1Zs0bPPfecwsPD1a1bN2eWDwAAAKAInDoT8sMPP6hbt27q3LmzIiMj1bNnT7Vr1067du3K06Z///5q1aqVIiMj9eyzz6pevXp52gAAAAC4czg1hLRo0ULr16/XsWPHJEn79+/Xtm3b1KlTpzxtli9frnPnzskwDG3cuFHHjh1T+/btr9tnRkaGkpKS8twAAAAAlB5OPRxr5MiRSkxMVI0aNWQ2m2Wz2TRx4kT16dPH3mbGjBl65plnVLFiRbm6usrFxUX//e9/1aJFi+v2OXnyZI0fP95RTwEAAABAITl1JuSLL77QggULtGjRIu3Zs0fz58/Xu+++q/nz59vbzJgxQz/++KOWL1+u3bt3a+rUqXruuee0bt266/Y5evRoJSYm2m9nzpxx1NMBAAAAUAAmwzAMZw1eqVIljRo1Ss8//7x925tvvqkFCxbol19+UXp6uvz8/LR06VJ17tzZ3uZvf/ubzp49q9WrV99yjKSkJPn5+SkxMVG+vr4l8jwAAEDx4vc3ULY5dSYkLS1NLi55SzCbzfZT9GZlZSkrK+umbQAAAADcWZy6JqRr166aOHGiKleurNq1a2vv3r167733NGjQIEmSr6+vHnzwQY0YMUJWq1URERHavHmzPvnkE7333nvOLB0AAABAETn1cKzk5GSNHTtWS5cuVXx8vMLDw9WnTx+99tprcnd3lyTFxcVp9OjRWrNmjS5fvqyIiAg9++yzeuGFF2QymW45BtO5gHOkZqXq8tXLsuXYZHYxK8AjQF5uXs4uC8Adgt/fQNnm1BDiCPwnBjjOpfRLOnTpkH6+8LMupl9Uena6cowcuZhcZHW1qry1vOoG1VXtwNoKtAY6u1wApRi/v4GyzamHYwEoG9Kz07X17FZtP79dVzKuyOpqlbebt4I9g2U2mWUzbErPTtfZlLM6fuW4NpzeoObhzfVAxQdkdbU6u3wAAOBghBAAtyUuNU5Lji/RsYRjKmcpp6r+VeViynsyCTe5ycPVQ+U8yinHyNGl9EtaGbNSJ66c0CPRjyjUK9RJ1QMAAGdw6tmxANzZ4lLjtPDIQh1POK5I30gFeQblCyB/5mJyUZBnkCJ9I3U84bgWHlmouNQ4B1UMAABKA0IIgCJJz07X18e+1pnkM6riX0XuZvdC7e9udlcV/yo6m3xWS44vUXp2eglVCgAAShtCCIAi2Xp2q45fyZ0BMZvMRerDbDIrwjdCxxKOaevZrcVcIQAAKK0IIQAK7VL6JW0/v13lLOUKPQPyZ+5md5WzlNP289t1Kf1SMVUIAABKM0IIgEI7dOmQrmRcKbbT7AZaA3Ul44oOXTpULP0BAIDSjRACoNB+vvCzrK7WWy5CL6hr1xE5ePFgsfQHAABKN0IIgEJJzUrVxfSL8nbzLtZ+vd28dSHtglKzUou1XwAAUPoQQgAUyuWrl5WenV7sFxm0ulqVnp2uy1cvF2u/AACg9CGEACgUW45NOUZOkc+IdSMuJhflGDmy5diKtV8AAFD6EEIAFIrZxSwXk4tsRvGGhRwjRy4mF5ldijfcAACA0ocQAqBQAjwC7IdOFadrh3gFeAQUa78AAKD0IYQAKBQvNy+Vt5ZXSlZKsfabkpWiIM8gebl5FWu/AACg9CGEACi0ukF1lZ6drhwjp1j6yzFylJ6drjrl6xRLfwAAoHQjhAAotNqBteVv8S+2K5xfSr8kf4u/agfWLpb+AABA6UYIAVBogdZANQ9vroSMBGXaMm+rr0xbphIyEtQ8vHmxXYEdAACUboQQAEXyQMUHVK1cNZ1KOlXkM2XZDJtOJZ1StXLV9EDFB4q5QgAAUFoRQgAUidXVqkeiH1FFn4r67cpvhZ4RybRl6rcrv6miT0U9Ev1IsV/8EAAAlF6EEABFFuoVqidrPqnoctGKSYrRhbQLt1ysnmPk6ELaBZ1MOqnoctF6suaTCvUKdVDFAACgNDAZhmE4u4iSlJSUJD8/PyUmJsrX19fZ5QBlUnp2urae3art57frSsYVWV2t8nbzltXVar8Senp2ulKyUpSenS5/i7+ahzfXAxUfYAYEwHXx+xso21ydXQCAO5/V1ap2ke3UKKSRDl06pIMXD+pC2gXFp8Xbr4RudbWqkk8l1SlfR7UDa7MIHQCAuxghBECxCbQGqmXFlmpZsaVSs1J1+epl2XJsMruYFeARwIUIAQCAJEIIgBLi5eZF6AAAANfFwnQAAAAADsVMCAAAd4GMbJvikzKUfDVbOYYhF5NJPh6uCva1yOJqdnZ5AO4yhBAAAMqo5KtZ+vlcog6cTdTZhDSlZmQrIyvHHkIsbi7ysriqYjlP3VvRT3Ur+MnHw83ZZQO4CxBCAAAoY1IzsrXpaLx2xFxWfFKG3Mwm+Xi4qby3RR6uZplMkmFIV7NtSs2w6dC5RO07fUXBvhY1jQpQq+rB8rLwEQFAyeF/GAAAygjDMHT092R9u/+8TsSnyN/qrnuCveTqkn8JqMkkebq7ytPdVUE+FmXn5OhicqaW7z+vw7FJ6lovXDVCuT4HgJJBCAEAoAwwDEPbTlzUN3vPKT0rR/cEecvNXPDzz7i6uCjUz0OB3u46eTFN/93ym7o3qKj7qwbKZDKVYOUA7kacHQsAgDvctQDy1e6zcnExqWpw4QLIH7mZXVQ12FsuLiYt3n1G35+4VMzVAgAhBACAO97R35P1zd5zsri6KMzPWix9hvlZZXF10bJ95/RLXFKx9AkA1xBCAAC4g6VmZOvb/eeVnpVTbAHkmjA/q9Izbfp2/3mlZmQXa98A7m6EEAAA7mCbjsbrRHyKIgI9S6T/iEBPnYhP0aaj8SXSP4C7EyEEAIA7VPLVLO2IuSx/q3uR14DcipvZRf5Wd+2Iuazkq1klMgaAuw8hBACAO9TP5xIVn5Sh8j7uJTpOeR93xSdl6OA51oYAKB6EEAAA7lAHzibKzWy67nVAipOri4vczCbtP3ulRMcBcPcghAAAcAfKyLbpbEKafDzcHDKej4ebziWkKyPb5pDxAJRthBAAAO5A8UkZSs3IlpfF7JDxPN3NSsnIVnxShkPGA1C2EUIAALgDJV/NVkZWjjxcHRNCPNzMysiyKfkqp+oFcPsIIQAA3IFyDEM5hiGTyTHjuZiknP83LgDcLkIIAAB3IBeTSS4mkxyVCXKM3A8NLo5KPQDKNEIIAAB3IB8PV1ncXHTVQQvFr2bZZHEzy8fD1SHjASjbCCEAANyBgn0t8rK4KjXDMSEkLdMmb4urgn0tDhkPQNlGCAEA4A5kcTWrYjlPh13FPPlqliqUs8rioIXwAMo2QggAAHeoeyv6KctmKDsnp0THyc7JUZbNUL2K/iU6DoC7ByEEAIA7VN0Kfgr2tehicmaJjnMxOVPBvhbVqeBbouMAuHsQQgAAuEP5eLipaVSArqRnKstWMrMhWbYcXUnPVNOoAIddnR1A2UcIAQDgDtaqerCqBnvr1KW0Eun/1KU0VQ32VqvqwSXSP4C7EyEEAIA7mJfFVV3rhcvq5qLYxPRi7Ts2MV1Wd7O61guXl4VT8wIoPoQQAADucNVDfNStQQVlZOcUWxCJTUxXRnaOutevoBqhrAUBULwIIQAA3OFMJpNaVC2vxxpVUo4hnYhPKfIakSxbjk7EpyjHkB5rVEn3Vw0s5moBQGJuFQCAMsBkMqlFdHmV93HXt/vP60R8ivyt7irv4y5Xl1t/55idk6OLyZm6kp6pqsHe6lovnBkQACWGEAIAQBlSI9RXlcp5atPReO2Iuaxf41PlZjbJx8NNnu5mebiZ5WKScgzpapZNaZk2JV/NUpbNULCvRX+tFq5W1YNZAwKgRPE/DAAAZYyXxVWd7w1Xy2pBOnguSfvPXtG5hHRdSs1URpZNOco9HtviZpa3xVW1K/ipXkV/1angy2l4ATgEIQQAgDLKx8NNze4JVLN7ApWRbVN8UoaSr2YrxzDkYjLJx8NVwb4WWVzNzi4VwF2GEAIAwF3A4mpWpQBPZ5cBAJI4OxYAAAAAByOEAAAAAHAoDscCAAB3LJvNpqysLGeXAUCSm5ubzOaCrTErVAgxDEObN2/W1q1bdfLkSaWlpSkoKEgNGjRQmzZtVKlSpSIVDAAAUBiGYSguLk5XrlxxdikA/sDf31+hoaEymUw3bVegEJKenq5p06bpgw8+0KVLl1SvXj1VqFBBVqtVJ06c0LJly/TMM8+oXbt2eu211/SXv/ylWJ4EAADA9VwLIMHBwfL09LzlBx4AJcswDKWlpSk+Pl6SFBYWdtP2BQoh1apVU9OmTTV79my1b99ebm75zyF+6tQpLVq0SL169dKYMWP0zDPPFKF8AACAm7PZbPYAEhgY6OxyAPw/VqtVkhQfH6/g4OCbHppVoIXpq1at0ldffaUuXbpcN4BIUkREhEaPHq3jx4+rVatWBSo0OztbY8aMUVRUlKxWq6pUqaIJEyYoJycnT7sjR47or3/9q/z8/OTj46O//OUvOn36dIHGAAAAZcu1NSCenpxyGChtrr0vb7VWq0AzIXXq1CnwwO7u7oqOji5Q2ylTpmj27NmaP3++ateurV27dmngwIHy8/PTsGHDJEm//vqrWrRooaefflrjx4+Xn5+fjhw5Ig8PjwLXBAAAyh4OwQJKn4K+L4t8dqzs7Gx9+OGH2rRpk2w2m+6//349//zzhQoHP/zwg7p166bOnTtLkiIjI/XZZ59p165d9javvvqqOnXqpLffftu+rUqVKkUtGwAAAICTFfk6IUOHDtXSpUv10EMP6cEHH9SiRYs0cODAQvXRokULrV+/XseOHZMk7d+/X9u2bVOnTp0kSTk5Ofrf//6natWqqX379goODlbTpk21bNmyG/aZkZGhpKSkPDcAAADc3IABA9S9e3en1jBu3DjVr1/ffr801ISSUeAQsnTp0jz316xZo++++07PPfechg0bpoULF2rVqlWFGnzkyJHq06ePatSoITc3NzVo0EDDhw9Xnz59JOUuaklJSdFbb72lDh06aM2aNerRo4ceeeQRbd68+bp9Tp48WX5+fvYbpw0GAAClwYABA2Qymey3wMBAdejQQQcOHMjT7o9t/nj7/PPPJUmbNm3K10/r1q31/fffS8o9suRGfZhMpgKv3S0N3n//fc2bN8/ZZaAEFDiEfPzxx+revbvOnTsnSWrYsKH+/ve/a/Xq1fr222/18ssvq0mTJoUa/IsvvtCCBQu0aNEi7dmzR/Pnz9e7776r+fPnS5J9gXq3bt30wgsvqH79+ho1apS6dOmi2bNnX7fP0aNHKzEx0X47c+ZMoWoCAAAoKR06dFBsbKxiY2O1fv16ubq6qkuXLvnazZ07197u2u3PMwJHjx5VbGysNm3apKCgIHXu3Fnx8fHauXOnfZ+vv/46T9vY2FgtWbLEEU+1WPj5+cnf39/ZZaAEFDiErFixQr1791arVq00c+ZM/ec//5Gvr69effVVjR07VpUqVdKiRYsKNfiIESM0atQo9e7dW3Xr1lXfvn31wgsvaPLkyZKk8uXLy9XVVbVq1cqzX82aNW94diyLxSJfX988NwAAgOtJSM1UzMVUJaRmOmQ8i8Wi0NBQhYaGqn79+ho5cqTOnDmjCxcu5Gl37YJvf7z9ed1tcHCwQkNDVbduXY0ZM0aJiYnasWOHgoKC7PsEBATkafvHbTcyfvx4BQcHy9fXV4MHD1Zm5v/9bFavXq0WLVrI399fgYGB6tKli3799Vf745mZmRoyZIjCwsLk4eGhyMhI++c6SUpMTNSzzz5r779169bav3//DWv58+FYrVq10tChQ/Xyyy8rICBAoaGhGjduXJ59CjsGnKNQC9N79+6tDh06aMSIEWrfvr0+/PBDTZ06tciDp6WlycUlbw4ym832GRB3d3c1adJER48ezdPm2LFjioiIKPK4AADg7nY1y6YVB85r18kEpWVmy9PdVY0jy6nLveHycLvxtQ2KU0pKihYuXKiqVave1vVO0tLSNHfuXEm64aUUCmr9+vXy8PDQxo0bdfLkSQ0cOFDly5fXxIkTJUmpqal68cUXVbduXaWmpuq1115Tjx49tG/fPrm4uGjGjBlavny5vvzyS1WuXFlnzpyxH5ViGIY6d+6sgIAArVy5Un5+fvrwww/18MMP69ixY7cMR9fMnz9fL774onbs2KEffvhBAwYM0P3336+2bdsW2xgoeYU+O5a/v78++ugjbdmyRX379lWHDh00YcIE+8VJCqNr166aOHGiKleurNq1a2vv3r167733NGjQIHubESNGqFevXmrZsqUeeugh++FfmzZtKvR4AAAAkrTiwHmtPfy7Ar0sCve3Kik9W2sP/y5J6tmo5NaTrlixQt7e3pJyP9CHhYVpxYoV+b6U7dOnT74LvR04cCDPGUIrVqwoKTeEGIahRo0a6eGHH76t+tzd3TVnzhx5enqqdu3amjBhgkaMGKE33nhDLi4uevTRR/O0//jjjxUcHKzDhw+rTp06On36tKKjo9WiRQuZTKY8Xxpv3LhRP//8s+Lj42WxWCRJ7777rpYtW6avvvpKzz77bIFqvPfee/X6669LkqKjozVr1iytX79ebdu2LbYxUPIKfDjWmTNn1KtXL9WtW1dPPvmkoqOjtXv3blmtVtWvX7/Qi9IlaebMmerZs6eee+451axZUy+99JIGDx6sN954w96mR48emj17tt5++23VrVtX//3vf/X111+rRYsWhR4PAAAgITVTu04mKNDLoiAfiyyuZgX5WBToZdHukwklemjWQw89pH379mnfvn3asWOH2rVrp44dO+rUqVN52k2bNs3e7trtzyfb2bp1q/bs2aPPPvtMERERmjdv3m3PhNSrVy/PRSCbNWumlJQU+2zGr7/+qieeeEJVqlSRr6+voqKiJMl+mPyAAQO0b98+Va9eXUOHDtWaNWvsfe3evVspKSkKDAyUt7e3/RYTE5PnkK5buffee/PcDwsLU3x8fLGOgZJX4JmQfv36KSQkRO+8846+++47DR48WMuXL9eECRPUp08fDR48WHPnztWXX35Z4MF9fHw0ffp0TZ8+/abtBg0alGd2BMAdJiNZSr0o5dgkF7PkVV6y+Di7KgB3qSvpWUrLzFa4f96jOHytrjp/JV1X0rNUzsu9RMb28vJS1apV7fcbNWokPz8/ffTRR3rzzTft20NDQ/O0u56oqCj5+/urWrVqunr1qnr06KGDBw/aZwCK07UL0HXt2lWVKlXSRx99pPDwcOXk5KhOnTr2dSMNGzZUTEyMVq1apXXr1unxxx9XmzZt9NVXXyknJ0dhYWHXPZqlMIvP/xy0TCaT/VD+4hoDJa/AIWTXrl3at2+f7rnnHrVv396efKXcheJbtmzRf/7znxIpEsAdKOWCdH6PdHa3lPK7lJUmGTbJZJbcPCXvEKliIym8oeQd5OxqAdxF/K1u8nR3VVJ6toJ8/u+Qp6T0bHm5u8rfenuzCYVhMpnk4uKi9PT02+qnb9++mjBhgj744AO98MILRe5n//79Sk9Ptx9m/+OPP8rb21sVK1bUpUuXdOTIEX344Yd64IEHJEnbtm3L14evr6969eqlXr16qWfPnurQoYMuX76shg0bKi4uTq6uroqMjCxyjTfjiDFQPAocQho2bKjXXntN/fv317p161S3bt18bTjODoAy06Tja6RfN0hplyQ3r9xZD5+w3FmQHJuUmSolnJTiD0m//E+6p7UU3U5y97xl9wBwu8p5uatxZDn7GhBfa24guZSaoba1QkpsFkTKvahyXFycJCkhIUGzZs1SSkqKunbtmqfdlStX7O2u8fHxkZeX13X7dXFx0fDhw/Xmm29q8ODBeQ6pKozMzEw9/fTTGjNmjE6dOqXXX39dQ4YMkYuLi8qVK6fAwED95z//UVhYmE6fPq1Ro0bl2X/atGkKCwtT/fr15eLiosWLFys0NFT+/v5q06aNmjVrpu7du2vKlCmqXr26zp8/r5UrV6p79+5q3LhxkWr+I0eMgeJR4DUhn3zyiTIyMvTCCy/o3Llz+vDDD0uyLgB3osRz0vfvSz8vlmSSgmtJAVG5h1+5WSWze+6fXuVztwfXym338+Lc/RLPOfsZALhLdLk3XG1rhcgwDJ2/ki7DMNS2Voi63BteouOuXr1aYWFhCgsLU9OmTbVz504tXrw43wUEBw4caG937TZz5syb9j1o0CBlZWVp1qxZRa7v4YcfVnR0tFq2bKnHH39cXbt2tZ8C18XFRZ9//rl2796tOnXq6IUXXtA777yTZ39vb29NmTJFjRs3VpMmTXTy5EmtXLlSLi4uMplMWrlypVq2bKlBgwapWrVq6t27t06ePKmQkJAi1/xHjhgDxcNkGIbh7CJKUlJSkvz8/JSYmMg1Q4CSlHhO2jFbSoiRAqpKroU4Jjk7Q7p8QioXJTX9u+RXoeTqBHBHuNnv76tXryomJkZRUVH5rp1RWAmpmbqSniV/q1uJzoAAd4uCvj8LNBOSmppaqMEL2x7AHS4zTdrziXQ5Ripfo3ABRMptX75GboDZ80lufwDgAOW83BVV3osAAjhYgUJI1apVNWnSJJ0/f/6GbQzD0Nq1a9WxY0fNmDGj2AoEcAc4vkb6/aAUWDV33UdRuJhzZ1B+P5jbHwAAKLMKtDB906ZNGjNmjMaPH6/69eurcePGCg8Pl4eHhxISEnT48GH98MMPcnNz0+jRo1mgDtxNUi7kLkL3LF/4GZA/c7Xk9vPrBinifs6aBQBAGVWgEFK9enUtXrxYZ8+e1eLFi7VlyxZt375d6enpKl++vBo0aKCPPvpInTp1ynfFTwBl3Pk9uWfBCq5VPP15B0vxh3P7rda+ePoEAAClSoFP0StJFStW1AsvvHBb558GUMac3Z17Gl5TMX0BYXLJ7e8cIQQAgLKKaQsARZeRnHshwuK++rnFR0qOy+0fAACUOYQQAEWXejH3Suju1794VpG5e+X2m3qxePsFAAClAiEEQNHl2CTDVvQzYt2IySW33xxb8fYLAABKBUIIgKJzMUsmc/GHBSMnt9/iDjcAAKBUIIQAKDqv8pKbp5RZzBcozUzN7derfPH2CwB3CJPJpGXLlhV5/3Hjxql+/fr2+wMGDFD37t1vuy6guBQ6hERGRmrChAk6ffp0SdQD4E5i8ZG8Q4p/AXlGsuQTWvwL3gHAyQYMGCCTySSTySQ3NzeFhISobdu2mjNnjnJycuztYmNj1bFjxwL1eb3A8tJLL2n9+vUFqsNkMikwMFAdOnTQgQMH8vV9vdvnn38uKfdacn/up3Xr1vr+++8l5X5uvFEfJpNJrVq1KtBzRNlT6BDyr3/9S998842qVKmitm3b6vPPP1dGRkZJ1AbgTlCxkZSVmnsIVXEwcnL7q9CwePoDgFKmQ4cOio2N1cmTJ7Vq1So99NBDGjZsmLp06aLs7GxJUmhoqCyWol8A1tvbW4GBgQWqIzY2VuvXr5erq6u6dOmSr93cuXPt7a7d/jyrcvToUcXGxmrTpk0KCgpS586dFR8fr507d9r3+frrr/O0jY2N1ZIlS4r8HHFnK3QI+ec//6ndu3dr9+7dqlWrloYOHaqwsDANGTJEe/bsKYkaAZRm4Q0lz0ApJb54+kuJz+0vnBACwAHSLkuXfs3900EsFotCQ0NVoUIFNWzYUK+88oq++eYbrVq1SvPmzZOUd3YjMzNTQ4YMUVhYmDw8PBQZGanJkydLyp1pkKQePXrIZDLZ7//5cKyb1REaGqr69etr5MiROnPmjC5cuJCnnb+/v73dtZuHh0eeNsHBwQoNDVXdunU1ZswYJSYmaseOHQoKCrLvExAQkKftH7fh7lPkNSH16tXT+++/r3Pnzun111/Xf//7XzVp0kT16tXTnDlzZBhGcdYJoLTyDpLuaS2lXZSyb3NWNDsjt597Wuf2CwAlJStd2rdIWjdO2jgp9899i3K3O0Hr1q1Vr169684MzJgxQ8uXL9eXX36po0ePasGCBfawsXPnTkn/N1tx7X5hpaSkaOHChapateotZ1BuJi0tTXPnzpUkubm5FbkflH2FumL6H2VlZWnp0qWaO3eu1q5dq7/85S96+umndf78eb366qtat26dFi1aVJy1AiitottJ8Uek+ENS+RpFO6tVjk26fEIKqZPbHwCUpENLpV/+J3kFS34VpatJufclqf4TTimpRo0a+dZkSNLp06cVHR2tFi1ayGQyKSIiwv5YUFDuFzbXZisKY8WKFfL29pYkpaamKiwsTCtWrJCLS97vqPv06SOzOe//6wcOHFCVKlXs9ytWrCgpN4QYhqFGjRrp4YcfLlQ9uLsUOoTs2bNHc+fO1WeffSaz2ay+fftq2rRpqlGjhr1Nu3bt1LJly2ItFEAp5u4pNewn7ZgtXfxFCqgquRbiWObsjNwAUi4qtx93z5KrFQDSLkunf8wNIN7Budu8/9/hRad/lKp1kDwdf5iQYRgymUz5tg8YMEBt27ZV9erV1aFDB3Xp0kXt2t3+lzUPPfSQ/v3vf0uSLl++rA8++EAdO3bUTz/9lCfoTJs2TW3atMmzb6VKlfLc37p1q7y8vLR3716NHDlS8+bNYyYEN1XoENKkSRO1bdtW//73v9W9e/fr/gOrVauWevfuXSwFArhD+FWQmv5d2vOJ9PtBybN87i93002O+jRycteApF3MnQFp2C+3HwAoSekJuacC96uYd7uHr5R4NvdxJ4SQI0eOKCoqKt/2hg0bKiYmRqtWrdK6dev0+OOPq02bNvrqq69uazwvLy9VrVrVfr9Ro0by8/PTRx99pDfffNO+PTQ0NE+764mKipK/v7+qVaumq1evqkePHjp48OBtLa5H2VboNSG//fabVq9erccee+yGCdfLy8t+PCCAu4hfBen+YVLdxyQZUvxh6XKMlHox9zjr7IzcP1Mv5m6PP5zbru5jufsRQAA4grWc5O6VewjWH11Nyt1uLefwkjZs2KCff/5Zjz766HUf9/X1Va9evfTRRx/piy++0Ndff63Ll3MX07u5uclmu/2LxppMJrm4uCg9/fbWxfTt21c5OTn64IMPbrsmlF2FngmJj49XXFycmjZtmmf7jh07ZDab1bhx42IrDsAdyN1Tqt1dirhfOr9HOrdHSo6TkmMlw5Z7JXQ3TykgKvc0vOENWYQOwLE8A6TKf/m/NSAevrkBJDVeqtG5xGdBMjIyFBcXJ5vNpt9//12rV6/W5MmT1aVLF/Xr1y9f+2nTpiksLEz169eXi4uLFi9erNDQUPn7+0vKPUPW+vXrdf/998tisahcuYKFqGt1SFJCQoJmzZqllJQUde3aNU+7K1eu2Ntd4+PjIy8vr+v26+LiouHDh+vNN9/U4MGD5enJIbbIr9Ah5Pnnn9fLL7+cL4ScO3dOU6ZM0Y4dO4qtOAB3MO8gqVr73FtGcu7sR44td9G6V3kuRAjAuWr3yP3z9I+5h2C5e+UGkGvbS9Dq1asVFhYmV1dXlStXTvXq1dOMGTPUv3//fIvCpdxrfkyZMkXHjx+X2WxWkyZNtHLlSnvbqVOn6sUXX9RHH32kChUq6OTJk4WqQ8oNFTVq1NDixYvzXUBw4MCB+fadPHmyRo0adcO+Bw0apNdff12zZs3Syy+/XKB6cHcxGYU8l663t3e+MyJIUkxMjO69914lJxfzlZNvU1JSkvz8/JSYmChfX19nlwMAAArgZr+/r169qpiYGEVFReW7XkWhpV3OXQNiLeeUdSBAWVPQ92eh14RYLBb9/vvv+bbHxsbK1bXIZ/wFAABwPM8AKfAeAgjgYIUOIW3bttXo0aOVmJho33blyhW98soratu2bbEWBwAAAKDsKfTUxdSpU9WyZUtFRESoQYMGkqR9+/YpJCREn376abEXCAAAAKBsKXQIqVChgg4cOKCFCxdq//79slqtGjhwoPr06cNFaQAAAADcUpEWcXh5eenZZ58t7loAAAAA3AWKvJL88OHDOn36tDIzM/Ns/+tf/3rbRQEAAAAouwodQn777Tf16NFDP//8s0wmk66d4ddkMklSsVyxEwAAAEDZVeizYw0bNkxRUVH6/fff5enpqUOHDmnLli1q3LixNm3aVAIlAgAAAChLCj0T8sMPP2jDhg0KCgqSi4uLXFxc1KJFC02ePFlDhw7V3r17S6JOAAAAAGVEoWdCbDabvL29JUnly5fX+fPnJUkRERE6evRo8VYHAABwFzKZTFq2bFmR9x83bpzq169vvz9gwAB1797dfr9Vq1YaPnx4kfsv7n5w9yl0CKlTp44OHDggSWratKnefvttff/995owYYKqVKlS7AUCAACUFQMGDJDJZJLJZJKbm5tCQkLUtm1bzZkzRzk5OfZ2sbGx6tixY4H6vF5geemll7R+/fpiq3vTpk0ymUy6cuVKnu1LlizRG2+8UWzj4O5R6BAyZswY+5vkzTff1KlTp/TAAw9o5cqVmjFjRrEXCAAAUJZ06NBBsbGxOnnypFatWqWHHnpIw4YNU5cuXZSdnS1JCg0NlcViKfIY3t7eCgwMLK6SbyggIEA+Pj4lPg7KnkKHkPbt2+uRRx6RJFWpUkWHDx/WxYsXFR8fr9atWxd7gQAAACXlytUrOpV0SleuXnHYmBaLRaGhoapQoYIaNmyoV155Rd98841WrVqlefPmSco7u5GZmakhQ4YoLCxMHh4eioyM1OTJkyVJkZGRkqQePXrIZDLZ7//5cKxbWbBggRo3biwfHx+FhobqiSeeUHx8vCTp5MmTeuihhyRJ5cqVk8lk0oABAyTlPxwrISFB/fr1U7ly5eTp6amOHTvq+PHj9sfnzZsnf39/fffdd6pZs6a8vb3toQx3l0KFkOzsbLm6uurgwYN5tgcEBNhP0QsAAFDaXc2+qm9OfKPpe6br/9v7/2n6nun65sQ3upp91Sn1tG7dWvXq1dOSJUvyPTZjxgwtX75cX375pY4ePaoFCxbYw8bOnTslSXPnzlVsbKz9fmFlZmbqjTfe0P79+7Vs2TLFxMTYg0alSpX09ddfS5KOHj2q2NhYvf/++9ftZ8CAAdq1a5eWL1+uH374QYZhqFOnTsrKyrK3SUtL07vvvqtPP/1UW7Zs0enTp/XSSy8VqW7cuQp1dixXV1dFRERwLRAAAHBH++7kd9pweoPKW8srzDtMKZkp2nB6gySpW9VuTqmpRo0a9nW3f3T69GlFR0erRYsWMplMioiIsD8WFBQkSfL391doaGiRxx40aJD971WqVNGMGTN03333KSUlRd7e3goICJAkBQcHy9/f/7p9HD9+XMuXL9f333+v5s2bS5IWLlyoSpUqadmyZXrsscckSVlZWZo9e7buueceSdKQIUM0YcKEIteOO1OR1oSMHj1aly9fLol6AAAAStSVq1e0N36vylvLK9AaKIvZokBroMpby2tv/F6HHpr1R4ZhXPfIkgEDBmjfvn2qXr26hg4dqjVr1hT72Hv37lW3bt0UEREhHx8ftWrVSlJuACqoI0eOyNXVVU2bNrVvCwwMVPXq1XXkyBH7Nk9PT3sAkaSwsDD7oV+4exT6OiEzZszQiRMnFB4eroiICHl5eeV5fM+ePcVWHAAAQHFLzExUWlaawrzD8mz3dvdWXGqcEjMT5e/h7/C6jhw5oqioqHzbGzZsqJiYGK1atUrr1q3T448/rjZt2uirr74qlnFTU1PVrl07tWvXTgsWLFBQUJBOnz6t9u3bKzMzs8D9GIZxw+1/DFdubm55HjeZTDfcF2VXoUPIH88xDQAAcKfxc/eTp5unUjJTZLH+3xmoUjJTZHW1ys/dz+E1bdiwQT///LNeeOGF6z7u6+urXr16qVevXurZs6c6dOigy5cvKyAgQG5ubrd1qPwvv/yiixcv6q233lKlSpUkSbt27crTxt3dXZJuOk6tWrWUnZ2tHTt22A/HunTpko4dO6aaNWsWuT6UTYUOIa+//npJ1AEAAOAQ/h7+ahDcwL4GxNvdWymZKbqYflGtK7cu8VmQjIwMxcXFyWaz6ffff9fq1as1efJkdenSRf369cvXftq0aQoLC1P9+vXl4uKixYsXKzQ01L42IzIyUuvXr9f9998vi8WicuXKFaqeypUry93dXTNnztTf//53HTx4MN+1PyIiImQymbRixQp16tRJVqvVfvHqa6Kjo9WtWzc988wz+vDDD+Xj46NRo0apQoUK6tbNOetsUHoVek0IAADAna59ZHu1rtxaNsOmuNQ42QybWldurfaR7Ut87NWrVyssLEyRkZHq0KGDNm7cqBkzZuibb76R2WzO197b21tTpkxR48aN1aRJE508eVIrV66Ui0vux7ipU6dq7dq1qlSpkho0aFDoeoKCgjRv3jwtXrxYtWrV0ltvvaV33303T5sKFSpo/PjxGjVqlEJCQjRkyJDr9jV37lw1atRIXbp0UbNmzWQYhlauXJnvECzAZBTyIDwXF5ebno63tJ05KykpSX5+fkpMTJSvr6+zywEAAAVws9/fV69eVUxMjKKiouTh4XFb41y5ekWJmYnyc/dzyjoQoKwp6Puz0IdjLV26NM/9rKws7d27V/Pnz9f48eMLXykAAICT+Hv4Ez4AJyh0CLneMX09e/ZU7dq19cUXX+jpp58ulsIAAAAAlE3FtiakadOmWrduXXF1BwAAAKCMKpYQkp6erpkzZ6pixYrF0R0AAACAMqzQh2OVK1cuz8J0wzCUnJwsT09PLViwoFiLAwAAAFD2FDqETJs2LU8IcXFxUVBQkJo2bVro81IDAAAAuPsUOoQMGDCgBMoAAAAAcLco9JqQuXPnavHixfm2L168WPPnzy+WogAAAACUXYUOIW+99ZbKly+fb3twcLAmTZpULEUBAAAAKLsKHUJOnTqlqKiofNsjIiJ0+vTpYikKAADgbmYymbRs2bIi7z9u3DjVr1/ffn/AgAHq3r27/X6rVq00fPjwIvdf3P3c7f78+pSETZs2yWQy6cqVKyU6TkEVOoQEBwfrwIED+bbv379fgYGBxVIUAABAWTNgwACZTCaZTCa5ubkpJCREbdu21Zw5c5STk5OnbWxsrDp27Figfq8XWF566SWtX7++uEq/4QfYJUuW6I033ii2ccq6kydPymQyad++fXm2v//++5o3b16xjXO9cNi8eXPFxsbKz8+v2Ma5HYUOIb1799bQoUO1ceNG2Ww22Ww2bdiwQcOGDVPv3r1LokYAAIAyoUOHDoqNjdXJkye1atUqPfTQQxo2bJi6dOmi7Oxse7vQ0FBZLJYij+Pt7e2QL4cDAgLk4+NT4uP80aZNmxQZGenQMUuan5+f/P39S3QMd3d3hYaG5jnLrTMVOoS8+eabatq0qR5++GFZrVZZrVa1a9dOrVu3Zk0IAAC4oxiGoZzUVBlZWQ4Zz2KxKDQ0VBUqVFDDhg31yiuv6JtvvtGqVavyfBP+x9mNzMxMDRkyRGFhYfLw8FBkZKQmT54sSfYP4z169JDJZLLf//PhWLeyYMECNW7cWD4+PgoNDdUTTzyh+Ph4Sbnf3j/00EOS/u96cdfOlvrnb9wTEhLUr18/lStXTp6enurYsaOOHz9uf3zevHny9/fXd999p5o1a8rb29sezErSTz/9pAYNGsjDw0ONGzfW0qVL88xIXKvrj5YtW5bnA/uvv/6qbt26KSQkRN7e3mrSpInWrVuXZ5/IyEhNmjRJgwYNko+PjypXrqz//Oc/9sevLWlo0KCBTCaTWrVqJSnv4VjXZkv+fLvW9tKlS+rTp48qVqwoT09P1a1bV5999pl9jAEDBmjz5s16//337fuePHnyurNZX3/9tWrXri2LxaLIyEhNnTq1UM/ndhQ6hLi7u+uLL77Q0aNHtXDhQi1ZskS//vqr5syZI3d392IpCgAAoKRlHD+uC7Nm6fcpbyt++vtK2rBBRmamw+to3bq16tWrpyVLllz38RkzZmj58uX68ssvdfToUS1YsMAeNnbu3Ckp9+ylsbGx9vuFlZmZqTfeeEP79+/XsmXLFBMTYw8alSpV0tdffy1JOnr0qGJjY/X+++9ft58BAwZo165dWr58uX744QcZhqFOnTop6w8hLy0tTe+++64+/fRTbdmyRadPn9ZLL71UpLoLIjU1VV26dFH16tW1e/dujRs3rkjjpaSkqFOnTlq3bp327t2r9u3bq2vXrvnWRE+dOlWNGzfW3r179dxzz+kf//iHfvnlF0m5YUiS1q1bp9jY2Ou+5pUqVVJsbKz9tnfvXgUGBqply5aSpKtXr6pRo0ZasWKFDh48qGeffVZ9+/bVjh07JOUe2tWsWTM988wz9j4qVaqUb5zdu3fr8ccfV+/evfXzzz9r3LhxGjt2bL7Dwm72fG5Hoa8Tck10dLSio6NvuwAAAABHy4qL0+XPPpeLxV3mwEDJJKWsXSfjaob8OhVsLUZxqlGjxnXX3ErS6dOnFR0drRYtWshkMikiIsL+WFBQkCTJ399foaGhRR5/0KBB9r9XqVJFM2bM0H333aeUlBR5e3srICBAUu7a4BsdNnT8+HEtX75c33//vZo3by5JWrhwoSpVqqRly5bpsccekyRlZWVp9uzZuueeeyRJQ4YM0YQJE4pc+60sXLhQNptNc+bMkaenp2rXrq2zZ8/qH//4R6H6qVevnurVq2e//+abb2rp0qVavny5hgwZYt/eqVMnPffcc5KkkSNHatq0adq0aZNq1Khhf70CAwNv+HqZzWb7Y1evXlX37t3VrFkzjRs3TpJUoUKFPCHqn//8p1avXq3FixeradOm8vPzk7u7uzw9PW/6b+K9997Tww8/rLFjx0qSqlWrpsOHD+udd97Jc13Amz2f21HomZCePXvqrbfeyrf9nXfesf/jAgAAKM3Sdu6S2ddX5Z58SsHDhiro73+XW+VKyjh2VLbkZIfXYxjGDY/VHzBggPbt26fq1atr6NChWrNmTbGPv3fvXnXr1k0RERHy8fGxH/pTmDOfHjlyRK6urmratKl9W2BgoKpXr64jR47Yt3l6etoDiCSFhYXZD/26EW9vb/utY8eOOn36dL5tN6urXr168vT0tG9r1qxZgZ/XNampqXr55ZdVq1Yt+fv7y9vbW7/88ku+n9G9995r/7vJZFJoaOgtn9+NPP3000pOTtaiRYvk4pL7sd1ms2nixIm69957FRgYKG9vb61Zs6bQZ6k9cuSI7r///jzb7r//fh0/flw2m61Ens8fFXomZPPmzXr99dfzbe/QoYPefffd2y4IAACgpF09clhGVpbcK1aQJJnc3WWpco9Stm6R7dIlmR282PrIkSPXvQSCJDVs2FAxMTFatWqV1q1bp8cff1xt2rTRV199VSxjp6amql27dmrXrp0WLFigoKAgnT59Wu3bt1dmIQ5PMwzjhtv/GLDc3NzyPG4ymW647zV/PJvUjh07NHLkSG3atMm+zWq1FrquP3JxccnXLutP64RGjBih7777Tu+++66qVq0qq9Wqnj175vsZXe/5/fnsZwXx5ptvavXq1frpp5/yLP6fOnWqpk2bpunTp6tu3bry8vLS8OHDC/VaSdcPvtf7WRXX8/mzQs+EpKSkXHfth5ubm5KSkgrVV3Z2tsaMGaOoqChZrVZVqVJFEyZMuOETGzx4sEwmk6ZPn17YsgEAAOw8ataSi5e3Ms+ekyQZmZnK+O1XuZYvn3t4lgNt2LBBP//8sx599NEbtvH19VWvXr300Ucf6YsvvtDXX3+ty5cvS8r9DPbHb64L65dfftHFixf11ltv6YEHHlCNGjXyfdN97bPfzcapVauWsrOz7WsTpNxF1MeOHVPNmjWLXJ8kVa1a1X6rUKGCXF1d8227WV379+9Xenq6fduPP/6Yp01QUJCSk5OVmppq3/bn0+hu3bpVAwYMUI8ePVS3bl2Fhobq5MmThXoeBfk5SrkLxidMmKAvv/wyz6zRtTq6deump556SvXq1VOVKlXyLP6/Ns6txqhVq5a2bduWZ9v27dtVrVo1mc3mgj6lIit0CKlTp46++OKLfNs///xz1apVq1B9TZkyRbNnz9asWbN05MgRvf3223rnnXc0c+bMfG2XLVumHTt2KDw8vLAlAwAA5OHZpLFsSUlKWLhA8e/P0IXZs5V1+ows1aqX6CxIRkaG4uLidO7cOe3Zs0eTJk1St27d1KVLF/Xr1++6+0ybNk2ff/65fvnlFx07dkyLFy9WaGiofW1GZGSk1q9fr7i4OCUkJBS6psqVK8vd3V0zZ87Ub7/9puXLl+e79kdERIRMJpNWrFihCxcuKCUlJV8/0dHR6tatm5555hlt27ZN+/fv11NPPaUKFSqoW7duha6ruDzxxBNycXHR008/rcOHD2vlypX5jt5p2rSpPD099corr+jEiRNatGhRvgXaVatW1ZIlS7Rv3z7t379fTzzxRKFnBIKDg2W1WrV69Wr9/vvvSkxMzNfm4MGD6tevn0aOHKnatWsrLi5OcXFx9tBZtWpVrV27Vtu3b9eRI0c0ePBgxcXF5ekjMjJSO3bs0MmTJ3Xx4sXr1vmvf/1L69ev1xtvvKFjx45p/vz5mjVrVomeJOCPCh1Cxo4dqzfeeEP9+/fX/PnzNX/+fPXr108TJ060L2wpqB9++EHdunVT586dFRkZqZ49e6pdu3batWtXnnbnzp3TkCFDtHDhwnxTQgAAAIXlFhqqgD69ZfL0lO3SJRkZmfJu20a+bR4u0XFXr16tsLAwRUZGqkOHDtq4caNmzJihb7755obfPnt7e2vKlClq3LixmjRpopMnT2rlypX2NQJTp07V2rVrValSJTVo0KDQNQUFBWnevHlavHixatWqpbfeeivfh/QKFSpo/PjxGjVqlEJCQvIsxP6juXPnqlGjRurSpYuaNWsmwzC0cuVKp35+8/b21rfffqvDhw+rQYMGevXVVzVlypQ8bQICArRgwQKtXLnSfsrbawvBr5k2bZrKlSun5s2bq2vXrmrfvr0aNmxYqFpcXV01Y8YMffjhhwoPD79uONu1a5fS0tL05ptvKiwszH575JFHJOV+Fm/YsKHat2+vVq1aKTQ0NN/V1l966SWZzWbVqlXLfnjdnzVs2FBffvmlPv/8c9WpU0evvfaaJkyYkGdRekkyGQU5UO5P/ve//2nSpEnat2+frFar7r33Xr3++ut68MEHC9XPW2+9pdmzZ2vNmjWqVq2a9u/fr3bt2mn69Onq06ePJCknJ0dt2rRRt27dNGzYMEVGRmr48OH5rgJ5TUZGhjIyMuz3k5KSVKlSJSUmJsrX17ewTxUAADhBUlKS/Pz8rvv7++rVq4qJiVFUVJQ8PDxuaxzDMGSkpcnk7i4TX3TeNU6ePKmoqCjt3bu3UNdTwa0V9P1ZpFP0du7cWZ07d863fd++fYV6IUeOHKnExETVqFFDZrPZvtr/WgCRcg/ZcnV11dChQwvU5+TJkzV+/PgC1wAAAO5eJpNJJi8vZ5cB3HUKfTjWnyUmJuqDDz5Qw4YN1ahRo0Lt+8UXX2jBggVatGiR9uzZo/nz5+vdd9/V/PnzJeVeROX999/XvHnzCnyJ+dGjRysxMdF+O3PmTKGfEwAAAICSU+SLFW7YsEEff/yxli5dqoiICD366KP6+OOPC9XHiBEjNGrUKPXu3VuSVLduXZ06dUqTJ09W//79tXXrVsXHx6ty5cr2fWw2m/71r39p+vTp1z0jgcVikcViKerTAgAAQBkXGRlZoFP3ouQUKoScPXtW8+bN05w5c5SamqrHH39cWVlZ+vrrrwt9ZixJSktLsy+qusZsNttX8Pft21dt2rTJ83j79u3Vt29fDRw4sNDjAQAAAHC+AoeQTp06adu2berSpYtmzpypDh06yGw2a/bs2UUevGvXrpo4caIqV66s2rVra+/evXrvvfc0aNAgSblX2Qz807m63dzcFBoaqurVqxd5XAAAcOfjm2yg9Cno+7LAIWTNmjUaOnSo/vGPfyg6OrrIhf3RzJkzNXbsWD333HOKj49XeHi4Bg8erNdee61Y+gcAAGXPtdO9pqWl3fRK2QAcLy0tTVL+K63/WYFDyNatWzVnzhw1btxYNWrUUN++fdWrV6/bKtLHx0fTp08v1BXQC3tlSgAAULaYzWb5+/vbr+rt6elZ4BPYACgZhmEoLS1N8fHx8vf3v+VV1wt9nZC0tDR9/vnnmjNnjn766SfZbDb7IVQ+JXiF0aK62XnGAQBA6XSr39+GYSguLk5XrlxxfHEAbsjf31+hoaG3/GKgSBcrvObo0aP6+OOP9emnn+rKlStq27atli9fXtTuSgQhBACAO09Bf3/bbDZlZWU5sDIAN+Lm5nbLGZBrbiuEXGOz2fTtt99qzpw5hBAAAHDb+P0NlG3FEkJKM/4TAwDgzsPvb6Bsu+0rpgMAAABAYRBCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQxFCAAAAADgUIQQAAACAQzk1hGRnZ2vMmDGKioqS1WpVlSpVNGHCBOXk5EiSsrKyNHLkSNWtW1deXl4KDw9Xv379dP78eWeWDQAAAOA2uDpz8ClTpmj27NmaP3++ateurV27dmngwIHy8/PTsGHDlJaWpj179mjs2LGqV6+eEhISNHz4cP31r3/Vrl27nFk6AAAAgCIyGYZhOGvwLl26KCQkRB9//LF926OPPipPT099+umn191n586duu+++3Tq1ClVrlz5lmMkJSXJz89PiYmJ8vX1LbbaAQBAyeH3N1C2OfVwrBYtWmj9+vU6duyYJGn//v3atm2bOnXqdMN9EhMTZTKZ5O/vf93HMzIylJSUlOcGAAAAoPRw6uFYI0eOVGJiomrUqCGz2SybzaaJEyeqT58+121/9epVjRo1Sk888cQNvxWZPHmyxo8fX5JlAwAAALgNTp0J+eKLL7RgwQItWrRIe/bs0fz58/Xuu+9q/vz5+dpmZWWpd+/eysnJ0QcffHDDPkePHq3ExET77cyZMyX5FAAAAAAUklNnQkaMGKFRo0apd+/ekqS6devq1KlTmjx5svr3729vl5WVpccff1wxMTHasGHDTY8NtVgsslgsJV47AAAAgKJxaghJS0uTi0veyRiz2Ww/Ra/0fwHk+PHj2rhxowIDAx1dJgAAAIBi5NQQ0rVrV02cOFGVK1dW7dq1tXfvXr333nsaNGiQpNzriPTs2VN79uzRihUrZLPZFBcXJ0kKCAiQu7u7M8sHAAAAUAROPUVvcnKyxo4dq6VLlyo+Pl7h4eHq06ePXnvtNbm7u+vkyZOKioq67r4bN25Uq1atbjkGp/gDAODOw+9voGxzaghxBP4TAwDgzsPvb6Bsc+rZsQAAAADcfQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoQghAAAAAByKEAIAAADAoZwaQrKzszVmzBhFRUXJarWqSpUqmjBhgnJycuxtDMPQuHHjFB4eLqvVqlatWunQoUNOrBoAAADA7XBqCJkyZYpmz56tWbNm6ciRI3r77bf1zjvvaObMmfY2b7/9tt577z3NmjVLO3fuVGhoqNq2bavk5GQnVg4AAACgqJwaQn744Qd169ZNnTt3VmRkpHr27Kl27dpp165dknJnQaZPn65XX31VjzzyiOrUqaP58+crLS1NixYtcmbpAAAAAIrI1ZmDt2jRQrNnz9axY8dUrVo17d+/X9u2bdP06dMlSTExMYqLi1O7du3s+1gsFj344IPavn27Bg8enK/PjIwMZWRk2O8nJiZKkpKSkkr2yQAAgGJz7fe2YRhOrgRASXBqCBk5cqQSExNVo0YNmc1m2Ww2TZw4UX369JEkxcXFSZJCQkLy7BcSEqJTp05dt8/Jkydr/Pjx+bZXqlSpmKsHAAAl7dKlS/Lz83N2GQCKmVNDyBdffKEFCxZo0aJFql27tvbt26fhw4crPDxc/fv3t7czmUx59jMMI9+2a0aPHq0XX3zRfj8nJ0eXL19WYGDgDfdBwSQlJalSpUo6c+aMfH19nV0O/oDXpvTitSndeH1Kr8TERFWuXFkBAQHOLgVACXBqCBkxYoRGjRql3r17S5Lq1q2rU6dOafLkyerfv79CQ0Ml5c6IhIWF2feLj4/PNztyjcVikcViybPN39+/ZJ7AXcrX15df1qUUr03pxWtTuvH6lF4uLlxNACiLnPrOTktLy/efi9lstp+iNyoqSqGhoVq7dq398czMTG3evFnNmzd3aK0AAAAAiodTZ0K6du2qiRMnqnLlyqpdu7b27t2r9957T4MGDZKUexjW8OHDNWnSJEVHRys6OlqTJk2Sp6ennnjiCWeWDgAAAKCInBpCZs6cqbFjx+q5555TfHy8wsPDNXjwYL322mv2Ni+//LLS09P13HPPKSEhQU2bNtWaNWvk4+PjxMrvThaLRa+//nq+w93gfLw2pRevTenG61N68doAZZvJ4Nx3AAAAAByI1V4AAAAAHIoQAgAAAMChCCEAAAAAHIoQAgAAAMChCCG4qXHjxslkMuW5XbuIJBxvy5Yt6tq1q8LDw2UymbRs2bI8jxuGoXHjxik8PFxWq1WtWrXSoUOHnFPsXeZWr82AAQPyvZf+8pe/OKfYu8zkyZPVpEkT+fj4KDg4WN27d9fRo0fztOG94xwFeW147wBlEyEEt1S7dm3Fxsbabz///LOzS7prpaamql69epo1a9Z1H3/77bf13nvvadasWdq5c6dCQ0PVtm1bJScnO7jSu8+tXhtJ6tChQ5730sqVKx1Y4d1r8+bNev755/Xjjz9q7dq1ys7OVrt27ZSammpvw3vHOQry2ki8d4CyyKnXCcGdwdXVldmPUqJjx47q2LHjdR8zDEPTp0/Xq6++qkceeUSSNH/+fIWEhGjRokUaPHiwI0u969zstbnGYrHwXnKC1atX57k/d+5cBQcHa/fu3WrZsiXvHSe61WtzDe8doOxhJgS3dPz4cYWHhysqKkq9e/fWb7/95uyScB0xMTGKi4tTu3bt7NssFosefPBBbd++3YmV4ZpNmzYpODhY1apV0zPPPKP4+Hhnl3RXSkxMlCQFBARI4r1Tmvz5tbmG9w5Q9hBCcFNNmzbVJ598ou+++04fffSR4uLi1Lx5c126dMnZpeFP4uLiJEkhISF5toeEhNgfg/N07NhRCxcu1IYNGzR16lTt3LlTrVu3VkZGhrNLu6sYhqEXX3xRLVq0UJ06dSTx3iktrvfaSLx3gLKKw7FwU388vKRu3bpq1qyZ7rnnHs2fP18vvviiEyvDjZhMpjz3DcPItw2O16tXL/vf69Spo8aNGysiIkL/+9//7IcAoeQNGTJEBw4c0LZt2/I9xnvHuW702vDeAcomZkJQKF5eXqpbt66OHz/u7FLwJ9eOl/7zN7fx8fH5vuGF84WFhSkiIoL3kgP985//1PLly7Vx40ZVrFjRvp33jvPd6LW5Ht47QNlACEGhZGRk6MiRIwoLC3N2KfiTqKgohYaGau3atfZtmZmZ2rx5s5o3b+7EynA9ly5d0pkzZ3gvOYBhGBoyZIiWLFmiDRs2KCoqKs/jvHec51avzfXw3gHKBg7Hwk299NJL6tq1qypXrqz4+Hi9+eabSkpKUv/+/Z1d2l0pJSVFJ06csN+PiYnRvn37FBAQoMqVK2v48OGaNGmSoqOjFR0drUmTJsnT01NPPPGEE6u+O9zstQkICNC4ceP06KOPKiwsTCdPntQrr7yi8uXLq0ePHk6s+u7w/PPPa9GiRfrmm2/k4+Njn/Hw8/OT1WqVyWTiveMkt3ptUlJSeO8AZZUB3ESvXr2MsLAww83NzQgPDzceeeQR49ChQ84u6661ceNGQ1K+W//+/Q3DMIycnBzj9ddfN0JDQw2LxWK0bNnS+Pnnn51b9F3iZq9NWlqa0a5dOyMoKMhwc3MzKleubPTv3984ffq0s8u+K1zvdZFkzJ07196G945z3Oq14b0DlF0mwzAMR4YeAAAAAHc31oQAAAAAcChCCAAAAACHIoQAAAAAcChCCAAAAACHIoQAAAAAcChCCAAAAACHIoQAAAAAcChCCIBSZdOmTTKZTLpy5YqzSykxly5dUnBwsE6ePFliY7z00ksaOnRoifUPAMDtIIQAd4Dt27fLbDarQ4cOzi6lVGrVqpWGDx/u7DIKbPLkyeratasiIyNLbIyXX35Zc+fOVUxMTImNAQBAURFCgDvAnDlz9M9//lPbtm3T6dOnS3Qsm82mnJycEh3jbpaenq6PP/5Yf/vb30p0nODgYLVr106zZ88u0XEAACgKQghQyqWmpurLL7/UP/7xD3Xp0kXz5s2zP9asWTONGjUqT/sLFy7Izc1NGzdulCRlZmbq5ZdfVoUKFeTl5aWmTZtq06ZN9vbz5s2Tv7+/VqxYoVq1aslisejUqVPauXOn2rZtq/Lly8vPz08PPvig9uzZk2esX375RS1atJCHh4dq1aqldevWyWQyadmyZfY2586dU69evVSuXDkFBgaqW7duhToM6dKlS+rTp48qVqwoT09P1a1bV5999pn98QEDBmjz5s16//33ZTKZZDKZ7P0fPnxYnTp1kre3t0JCQtS3b19dvHjRvm+rVq00dOhQvfzyywoICFBoaKjGjRuXZ/wrV67o2WefVUhIiDw8PFSnTh2tWLFCqamp8vX11VdffZWn/bfffisvLy8lJydf9/msWrVKrq6uatasmX3btUPQvvvuOzVo0EBWq1WtW7dWfHy8Vq1apZo1a8rX11d9+vRRWlqafb+vvvpKdevWldVqVWBgoNq0aaPU1FT743/961/z/KwAACgtCCFAKffFF1+oevXqql69up566inNnTtXhmFIkp588kl99tln9vvX2oeEhOjBBx+UJA0cOFDff/+9Pv/8cx04cECPPfaYOnTooOPHj9v3SUtL0+TJk/Xf//5Xhw4dUnBwsJKTk9W/f39t3bpVP/74o6Kjo9WpUyf7h+ucnBx1795dnp6e2rFjh/7zn//o1VdfzVN7WlqaHnroIXl7e2vLli3atm2bvL291aFDB2VmZhbo+V+9elWNGjXSihUrdPDgQT377LPq27evduzYIUl6//331axZMz3zzDOKjY1VbGysKlWqpNjYWD344IOqX7++du3apdWrV+v333/X448/nqf/+fPny8vLSzt27NDbb7+tCRMmaO3atfbn2LFjR23fvl0LFizQ4cOH9dZbb8lsNsvLy0u9e/fW3Llz8/Q3d+5c9ezZUz4+Ptd9Plu2bFHjxo2v+9i4ceM0a9Ysbd++XWfOnNHjjz+u6dOna9GiRfrf//6ntWvXaubMmZKk2NhY9enTR4MGDdKRI0e0adMmPfLII3n+Ldx33306c+aMTp06VaCfNQAADmMAKNWaN29uTJ8+3TAMw8jKyjLKly9vrF271jAMw4iPjzdcXV2NLVu22Ns3a9bMGDFihGEYhnHixAnDZDIZ586dy9Pnww8/bIwePdowDMOYO3euIcnYt2/fTevIzs42fHx8jG+//dYwDMNYtWqV4erqasTGxtrbrF271pBkLF261DAMw/j444+N6tWrGzk5OfY2GRkZhtVqNb777rvrjrNx40ZDkpGQkHDDWjp16mT861//st9/8MEHjWHDhuVpM3bsWKNdu3Z5tp05c8aQZBw9etS+X4sWLfK0adKkiTFy5EjDMAzju+++M1xcXOzt/2zHjh2G2Wy2/3wvXLhguLm5GZs2bbph7d26dTMGDRp03ee8bt06+7bJkycbkoxff/3Vvm3w4MFG+/btDcMwjN27dxuSjJMnT95wrMTEREPSTesBAMAZmAkBSrGjR4/qp59+Uu/evSVJrq6u6tWrl+bMmSNJCgoKUtu2bbVw4UJJUkxMjH744Qc9+eSTkqQ9e/bIMAxVq1ZN3t7e9tvmzZv166+/2sdxd3fXvffem2fs+Ph4/f3vf1e1atXk5+cnPz8/paSk2NekHD16VJUqVVJoaKh9n/vuuy9PH7t379aJEyfk4+NjHzsgIEBXr17NM/7N2Gw2TZw4Uffee68CAwPl7e2tNWvW3HJtzO7du7Vx48Y8z7tGjRqSlGfsPz/vsLAwxcfHS5L27dunihUrqlq1atcd47777lPt2rX1ySefSJI+/fRTVa5cWS1btrxhXenp6fLw8LjuY3+sJSQkRJ6enqpSpUqebddqq1evnh5++GHVrVtXjz32mD766CMlJCTk6c9qtUpSnkO4AAAoDVydXQCAG/v444+VnZ2tChUq2LcZhiE3NzclJCSoXLlyevLJJzVs2DDNnDlTixYtUu3atVWvXj1JuYcTmc1m7d69W2azOU/f3t7e9r9brVaZTKY8jw8YMEAXLlzQ9OnTFRERIYvFombNmtkPozIMI98+f5aTk6NGjRrZQ9IfBQUFFehnMHXqVE2bNk3Tp09X3bp15eXlpeHDh9/ycK6cnBx17dpVU6ZMyfdYWFiY/e9ubm55HjOZTPaF+dc+xN/M3/72N82aNUujRo3S3LlzNXDgwJv+XMqXL58vLFyvFpPJdNPazGaz1q5dq+3bt2vNmjWaOXOmXn31Ve3YsUNRUVGSpMuXL0sq+M8aAABHYSYEKKWys7P1ySefaOrUqdq3b5/9tn//fkVERNg/2Hfv3l1Xr17V6tWrtWjRIj311FP2Pho0aCCbzab4+HhVrVo1z+2PMxjXs3XrVg0dOlSdOnVS7dq1ZbFY8izqrlGjhk6fPq3ff//dvm3nzp15+mjYsKGOHz+u4ODgfOP7+fkV6OewdetWdevWTU899ZTq1aunKlWq5FnPIuXO5NhstnxjHzp0SJGRkfnG9vLyKtDY9957r86ePatjx47dsM1TTz2l06dPa8aMGTp06JD69+9/0z4bNGigw4cPF2j8WzGZTLr//vs1fvx47d27V+7u7lq6dKn98YMHD8rNzU21a9culvEAACguhBCglFqxYoUSEhL09NNPq06dOnluPXv21McffyxJ8vLyUrdu3TR27FgdOXJETzzxhL2PatWq6cknn1S/fv20ZMkSxcTEaOfOnZoyZYpWrlx50/GrVq2qTz/9VEeOHNGOHTv05JNP5pkZaNu2re655x71799fBw4c0Pfff29fmH5tJuDJJ59U+fLl1a1bN23dulUxMTHavHmzhg0bprNnzxbo51C1alX7N/5HjhzR4MGDFRcXl6dNZGSkduzYoZMnT+rixYvKycnR888/r8uXL6tPnz766aef9Ntvv2nNmjUaNGhQvsByIw8++KBatmypRx99VGvXrlVMTIxWrVql1atX29uUK1dOjzzyiEaMGKF27dqpYsWKN+2zffv2OnTo0A1nQwpqx44dmjRpknbt2qXTp09ryZIlunDhgmrWrGlvs3XrVj3wwAMFmtEBAMCRCCFAKfXxxx+rTZs2150xePTRR7Vv3z77KXOffPJJ7d+/Xw888IAqV66cp+3cuXPVr18//etf/1L16tX117/+VTt27FClSpVuOv6cOXOUkJCgBg0aqG/fvho6dKiCg4Ptj5vNZi1btkwpKSlq0qSJ/va3v2nMmDGSZF/z4OnpqS1btqhy5cp65JFHVLNmTQ0aNEjp6eny9fUt0M9h7Nixatiwodq3b69WrVopNDRU3bt3z9PmpZdektlsVq1atRQUFKTTp08rPDxc33//vWw2m9q3b686depo2LBh8vPzk4tLwf/r+/rrr9WkSRP16dNHtWrV0ssvv5wvxDz99NPKzMzUoEGDbtlf3bp11bhxY3355ZcFruF6fH19tWXLFnXq1EnVqlXTmDFjNHXqVHXs2NHe5rPPPtMzzzxzW+MAAFASTIbxh/M5AsBt+P7779WiRQudOHFC99xzj7PLcZiFCxdq2LBhOn/+vNzd3W/ZfuXKlXrppZd08ODBQgWiwvjf//6nESNG6MCBA3J1ZfkfAKB04TcTgCJbunSpvL29FR0drRMnTmjYsGG6//7775oAkpaWppiYGE2ePFmDBw8uUACRpE6dOun48eM6d+7cLWekiio1NVVz584lgAAASiVmQgAU2SeffKI33nhDZ86cUfny5dWmTRtNnTpVgYGBzi7NIcaNG6eJEyeqZcuW+uabb/KccQwAANwYIQQAAACAQ7EwHQAAAIBDEUIAAAAAOBQhBAAAAIBDEUIAAAAAOBQhBAAAAIBDEUIAAAAAOBQhBAAAAIBDEUIAAAAAOBQhBAAAAIBD/f8wqTFfgCEqdgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d3a9378-a4d6-4241-aa99-de0a1fdc26bd",
   "metadata": {},
   "source": [
    "# sadly, this will not work!\n",
    "model_quantized.push_to_hub(\"buruzaemon/distilbert-base-uncased-distilled-quantized-clinc\")\n",
    "\n",
    "# ... it'll result in something like this...\n",
    "...\n",
    "\n",
    "File /opt/conda/envs/transformers-py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\n",
    "    115 if check_use_auth_token:\n",
    "    116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n",
    "--> 118 return fn(*args, **kwargs)\n",
    "\n",
    "TypeError: create_repo() got an unexpected keyword argument 'organization'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccc74738-db34-4130-9172-dfd715b580ea",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfFolder, whoami, create_repo, delete_repo\n",
    "\n",
    "print(whoami())\n",
    "\n",
    "print(HfFolder().get_token())\n",
    "\n",
    "create_repo(\"test_bug_temporary\", token=HfFolder().get_token()) # explicitly paste your token\n",
    "\n",
    "delete_repo(\"distilbert-base-uncased-distilled-quantized-clinc\", token=HfFolder().get_token()) # explicitly paste your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebee64-2d2f-477e-acc3-07f9a0dc7100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3066fc-113d-49f0-9cc0-384860861d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849fe1d2-a18f-4c57-b417-39191bf211ca",
   "metadata": {},
   "source": [
    "## Optimizing Inference with ONNX and the ONXX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bebe3b-7d5e-4095-9536-a17826f29a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eab0d0-cb16-426e-b0b2-d0355bb5978c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd6760-afcd-44f6-b356-2860914e1fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b104c-d8ca-4221-9764-b746ab5f2163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f1dbe-8d11-4f73-b90b-b83642c3f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a6b30-7589-4303-9f4c-b9d3b74c9e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e4c91-1f4b-4fe6-b9c0-678352ae1a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce6348-7558-4c98-9cb7-3b65db496dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c3e5b-722b-4467-973f-fe6aa1c2cef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

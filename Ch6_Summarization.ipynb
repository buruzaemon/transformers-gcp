{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d68c6fb-2413-4152-b307-92bdf4b04ed3",
   "metadata": {},
   "source": [
    "# Chapter 6: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70209eef-cc54-41f2-9bab-26e74787c108",
   "metadata": {},
   "source": [
    "## The CNN/DailyMail Dataset\n",
    "\n",
    "* ~300k pairs of news articles and their corresponding summaries\n",
    "* summaries are _abstractive_\n",
    "* [`cnn_dailymail` dataset viewer at HF](https://huggingface.co/datasets/viewer/?dataset=cnn_dailymail&config=3.0.0)\n",
    "* also see the [Dataset card for `cnn_dailymail` at HF](https://huggingface.co/datasets/cnn_dailymail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ccb622-53ab-4e04-8a6b-9d1ae03f7fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/kashiwapoodle/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b4038b009446c7b3eeea2faa2304fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"cnn_dailymail\",\n",
    "    version=\"3.0.0\"\n",
    ")\n",
    "\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb50477f-1a18-433b-bc9e-65faeb37efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article (excerpt of 500 char, total length: 4051):\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 char, total length: {len(sample['article'])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f\"\\nSummary (length: {len(sample['highlights'])}):\")\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9f18a-93f4-462c-831b-d13b07fd296e",
   "metadata": {},
   "source": [
    "## Text Summarization Pipelines\n",
    "\n",
    "This section require `nltk`, so be sure to download/install that before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93b7478-b211-415a-a99f-626fc901b6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72812316-c54e-456b-9f19-c846bdb1b5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kashiwapoodle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The U.S. are a country.', 'The U.N. is an organization.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb203714-c049-4603-a9db-217f780c7798",
   "metadata": {},
   "source": [
    "## Summarization Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a32187-82a4-406c-895b-76bc1df8cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920dbf83-9966-4fcd-8113-3449392be7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': 'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\\nHere, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial.\\nMIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5cfb-bf75-401e-87fb-86bfde2b2b19",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "\n",
    "See the [`gpt2-xl` model details on HF](https://huggingface.co/gpt2-xl#model-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a51a05-1203-4a6f-8cb3-f9beda6ad59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(\n",
    "    gpt2_query,\n",
    "    max_length=512,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9183ed-1e4f-4aaa-a6e4-0a82fc0991bb",
   "metadata": {},
   "source": [
    "## T5\n",
    "\n",
    "See the [`tf-large` model details on HF](https://huggingface.co/t5-large#model-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf546c3-18ac-4b56-8a53-c311f1c9379f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b071a-43af-45c6-85f0-2d3b28b08653",
   "metadata": {},
   "source": [
    "## BART\n",
    "\n",
    "See the [`facebook/bart-large-cnn` model card on HF](https://huggingface.co/facebook/bart-large-cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c17681b-9507-487c-8d04-1529c5cf957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb37477-65af-475f-bec2-2d59e9893832",
   "metadata": {},
   "source": [
    "## PEGASUS\n",
    "\n",
    "See the [`google/pegasus-cnn-dailymail` model card on HF](https://huggingface.co/google/pegasus-cnn_dailymail).\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This model has a dependency on the `protobuf` library, so you will need to install that as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c6ddc6-87f3-442b-8ce4-58a44a304296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\"<n>\", \"\\n\").replace(\" .\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75e1a2-2315-4a03-b233-c217a8255748",
   "metadata": {},
   "source": [
    "## Comparing Different Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6801908a-f117-4c6e-9568-dc83b56a5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n",
      "\n",
      "BASELINE\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\n",
      "Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial.\n",
      "MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"\n",
      "\n",
      "GPT2\n",
      "- No shoes.\n",
      "- No bed.\n",
      "- No mattress.\n",
      "- Some inmates sleeping on the floor.\n",
      "- Lacks security.\n",
      "- Insufficient funds to provide mental health care.\n",
      "\n",
      "T5\n",
      "mentally ill inmates are housed on the ninth floor of a florida jail .\n",
      "most face drug charges or charges of assaulting an officer .\n",
      "judge says arrests often result from confrontations with police .\n",
      "one-third of all people in Miami-dade county jails are mental ill .\n",
      "\n",
      "BART\n",
      "Mentally ill inmates are housed on the \"forgotten floor\" of Miami-Dade jail.\n",
      "Most often, they face drug charges or charges of assaulting an officer.\n",
      "Judge Steven Leifman says the arrests often result from confrontations with police.\n",
      "He says about one-third of all people in the county jails are mentally ill.\n",
      "\n",
      "PEGASUS\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "The ninth floor is where they're held until they're ready to appear in court.\n",
      "Most often, they face drug charges or charges of assaulting an officer.\n",
      "They end up on the ninth floor severely mentally disturbed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc93e76-3e86-496c-863d-327c70428ae1",
   "metadata": {},
   "source": [
    "## Measuring the Quality of Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29769274-bfdf-4d6c-b4a5-108d15bd98fe",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "> _The closer a machine translation is to a professional human translation, the better it is._\n",
    "\n",
    "* The BLEU metric is based primarily on [_precision_](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_valueshttps://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)<p/>\n",
    "* It only really pays attention to how many n-grams in the _references_ (hopefully human-generated examples of good translations) show up in the translation (generated text).</p>\n",
    "* It takes the geometric mean of precision calculated with respect to several n-grams, usually 1-grams through 4-grams. <p/><p/><span style=\"padding-left:1.5em\">e.g., $p_{n} = \\frac{\\sum_{\\text{n-gram } \\in \\text{ reference}} \\underset{clip}{\\text{Count(n-gram)}}}{ \\sum_{\\text{n-gram } \\in \\text{ translation}} \\text{Count(n-gram)}}$</span><br/><br/><span style=\"padding-left:1.5em\">and so $\\text{BLEU-4} \\sim \\sqrt[4]{p_{1} \\cdot p_{2} \\cdot p_{3} \\cdot p_{4}}$</span><p/>\n",
    "* It also penalizes shorter translations by scaling the above-mentioned geometric mean of the n-grams with a brevity penalty ranging from `0.0` to `1.0`.<br/><span style=\"padding-left:1.5em\">e.g., $\\text{BP} = \\begin{cases} 1 & \\text{ if } c \\gt r \\\\ e^{1 - \\frac{r}{c}} & \\text{ if } c \\leq r \\end{cases}$</span><br/>where $r$ is the effective reference corpus length and $c$ is the length of the candidate translation.<p/>\n",
    "* Putting it all together, we have: <p/><p/><span style=\"padding-left:1.5em\">$\\text{BLEU-N} = \\text{BP} \\times \\left( \\prod_{n=1}^{N} p_{n} \\right)^{\\frac{1}{N}} $</span><p/>\n",
    "* Plain-vanilla BLEU assumes that the translation and reference sentences are already tokenized, with the tokenization corresponding to single words. But different models may use different tokenization schemes, so that is why `sacrebleu` is currently preferred over `bleu`. For that reason, it is also the case that `bleu`/`sacrebleu` might not work very well with non-English languages, where tokenization may be happening at the morpheme-level.\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the BLEU metric? video on Youtube](https://www.youtube.com/watch?v=M05L1DhFqcw).\n",
    "* [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)... only 8 pages!\n",
    "* [Rachel Tatman's blogpost _Evaluating Text Output in NLP: BLEU at your own risk_](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)... ?\n",
    "\n",
    "\n",
    "Regarding `datasets` and the `load_metric` API...\n",
    "\n",
    "> <pre>FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate</pre>\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `sacrebleu` library, so you will need to `pip install` that as well!</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b7ffa92-27a1-4034-9e46-e75fc94be87d",
   "metadata": {},
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a04504e-bea6-44af-9be9-e7d77b11e847",
   "metadata": {
    "tags": []
   },
   "source": [
    "# the old API, \n",
    "# somewhat different from what is shown by Lewis in the video\n",
    "old_bleu = load_metric(\"bleu\")\n",
    "\n",
    "old_bleu.add(\n",
    "    prediction=[\"I\", \"have\", \"thirty\", \"six\", \"years\"], \n",
    "    reference=[[\n",
    "        \"I\", \"am\", \"thirty\", \"six\", \"years\", \"old\"], \n",
    "        [\"I\", \"am\", \"thirty\", \"six\"]\n",
    "    ]\n",
    ")\n",
    "old_bleu.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4adb0bdc-ecfc-4d02-9065-34d1bb1915f6",
   "metadata": {},
   "source": [
    "# the old API, but using sacrebleu\n",
    "# as is shown in the book...\n",
    "sacrebleu = load_metric(\"sacrebleu\")\n",
    "\n",
    "sacrebleu.add(\n",
    "    prediction=\"I have thirty six years\",\n",
    "    reference=[\"I am thirty six years old\", \"I am thirty six\"]\n",
    ")\n",
    "\n",
    "sacrebleu.compute(\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02fa7d-03e0-4ab5-b159-6f21c952f30d",
   "metadata": {},
   "source": [
    "So we will go ahead and use the [`evaluate`](https://huggingface.co/evaluate-metric) API here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d053328a-9d81-4978-a507-9513827252f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67a74f7-6b97-4672-898f-c77095717b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.8, 0.5, 0.3333333333333333, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.25,\n",
       " 'translation_length': 5,\n",
       " 'reference_length': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "predictions = [\"I have thirty six years\"]\n",
    "references = [[\"I am thirty six years old\", \"I am thirty six\"]]\n",
    "\n",
    "new_bleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "414025d3-43ee-42b8-ae00-1cade860b0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 1, 0],\n",
       " 'totals': [5, 4, 3, 2],\n",
       " 'precisions': [80.0, 50.0, 33.333333333333336, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 5,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"I have thirty six years\"]\n",
    "references = [[\"I am thirty six years old\", \"I am thirty six\"]]\n",
    "\n",
    "new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cae496-fd0b-425a-a046-d9224945c723",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLEU (`sacrebleu`, actually) via the `evaluate` API \n",
    "\n",
    "#### Inputs\n",
    "\n",
    "* `predictions`: list of translations to score\n",
    "* `references`: list of lists of references\n",
    "* `smooth_method`: defaults to `exp` exponential decay; choose from `none`, `floor`, `add-k`, or `exp`\n",
    "* `smooth_value`: `float`\n",
    "* `tokenize`: tokenization method!\n",
    "* `lowercase`: enable/disable case-insensitivity; defaults to `False`\n",
    "* `force`: assume input is actually detokenized; defaults to `False`\n",
    "* `use_effective_order`: flag to stop inclusion of n-gram orders for which precision is `0`, so use `True` for sentence-level BLEU computations; defaults to `False`\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "* `score`: BLEU score, ranging from `0.0` to `100.0`, inclusive\n",
    "* `counts`: Counts\n",
    "* `totals`: Totals\n",
    "* `precisions`: Precisions\n",
    "* `bp`: Brevity penalty\n",
    "* `sys_len`: predictions length\n",
    "* `ref_len`: reference length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d9f7279-235a-47ff-bef9-b3dc1e313ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[2, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[6, 5, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[33.33, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             value\n",
       "score                          0.0\n",
       "counts                [2, 0, 0, 0]\n",
       "totals                [6, 5, 4, 3]\n",
       "precisions  [33.33, 0.0, 0.0, 0.0]\n",
       "bp                             1.0\n",
       "sys_len                          6\n",
       "ref_len                          6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "predictions = [\"the the the the the the\"]\n",
    "references = [[\"the cat is on the mat\"]]\n",
    "\n",
    "results = new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79cd930-26fb-4efc-a5d7-a512e770dc66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>57.893007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[5, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[5, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[100.0, 75.0, 66.67, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 value\n",
       "score                        57.893007\n",
       "counts                    [5, 3, 2, 1]\n",
       "totals                    [5, 4, 3, 2]\n",
       "precisions  [100.0, 75.0, 66.67, 50.0]\n",
       "bp                            0.818731\n",
       "sys_len                              5\n",
       "ref_len                              6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"the cat is on mat\"]\n",
    "references = [[\"the cat is on the mat\"]]\n",
    "\n",
    "results = new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d8693-3177-4c63-9f8f-784168baa6c9",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "> The ROUGE score was specifically developed for applications like summarization where high [_recall_](https://en.wikipedia.org/wiki/Sensitivity_and_specificityhttps://en.wikipedia.org/wiki/Sensitivity_and_specificity) is more important than just precision.\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the ROUGE metric? video on Youtube](https://www.youtube.com/watch?v=TMshhnrEXlg)\n",
    "* [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)... again, only 8 pages!\n",
    "\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `absl-py` and `rouge_score` libraries, so you will need to `pip install rouge_score` as well!</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "184bf6e9-bb17-4fe5-bf32-898cc4a306aa",
   "metadata": {},
   "source": [
    "# the old API\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49fdfe2b-cd2a-43f4-8637-956d1fefd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's use the rouge metric implementation in evaluate!\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4159c04f-fffd-4223-8ff2-b86cc4b75891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce789df8-4a97-4ba6-bbff-633ee89efa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.415842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.365079  0.145161  0.206349   0.285714\n",
       "gpt2      0.114286  0.029412  0.114286   0.114286\n",
       "t5        0.382979  0.130435  0.255319   0.382979\n",
       "bart      0.475248  0.222222  0.316832   0.415842\n",
       "pegasus   0.326531  0.208333  0.285714   0.326531"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_name in summaries:\n",
    "    results = rouge.compute(\n",
    "        predictions=[summaries[model_name]],\n",
    "        references=[reference]\n",
    "    )\n",
    "    records.append(results)\n",
    "    \n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56d311-16ca-4b5e-b652-db02667b6451",
   "metadata": {},
   "source": [
    "## Evaluating PEGASUS on the CNN/DailyMail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c63b76f3-4177-4f4e-bc2b-ae5d1b3f13e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(\n",
    "    dataset,\n",
    "    metric,\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\"\n",
    "):\n",
    "    summaries = [\n",
    "        three_sentence_summary(text) \n",
    "        for text in dataset[column_text]]\n",
    "    score = metric.compute(\n",
    "        predictions=summaries,\n",
    "        references=dataset[column_summary]\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b2ed178-9819-452f-85b3-6266581f486b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-888acb9a2eb72e89.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.389276</td>\n",
       "      <td>0.171296</td>\n",
       "      <td>0.245061</td>\n",
       "      <td>0.354239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.389276  0.171296  0.245061   0.354239"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "score = evaluate_summaries_baseline(\n",
    "    test_sampled,\n",
    "    rouge\n",
    ")\n",
    "#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(\n",
    "    score, \n",
    "    orient=\"index\", \n",
    "    columns=[\"baseline\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32ea34-9e2e-46b7-9486-4d473803adc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333372-6bf7-4094-b78c-67648456dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311a96e-408a-4f89-92f0-e10f9e20c2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3479f9-cdf4-4ca8-951d-b5991dc298d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

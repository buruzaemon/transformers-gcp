{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d68c6fb-2413-4152-b307-92bdf4b04ed3",
   "metadata": {},
   "source": [
    "# Chapter 6: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70209eef-cc54-41f2-9bab-26e74787c108",
   "metadata": {},
   "source": [
    "## The CNN/DailyMail Dataset\n",
    "\n",
    "* ~300k pairs of news articles and their corresponding summaries\n",
    "* summaries are _abstractive_\n",
    "* [`cnn_dailymail` dataset viewer at HF](https://huggingface.co/datasets/viewer/?dataset=cnn_dailymail&config=3.0.0)\n",
    "* also see the [Dataset card for `cnn_dailymail` at HF](https://huggingface.co/datasets/cnn_dailymail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccb622-53ab-4e04-8a6b-9d1ae03f7fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"cnn_dailymail\",\n",
    "    version=\"3.0.0\"\n",
    ")\n",
    "\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50477f-1a18-433b-bc9e-65faeb37efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 char, total length: {len(sample['article'])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f\"\\nSummary (length: {len(sample['highlights'])}):\")\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9f18a-93f4-462c-831b-d13b07fd296e",
   "metadata": {},
   "source": [
    "## Text Summarization Pipelines\n",
    "\n",
    "This section require `nltk`, so be sure to download/install that before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b7478-b211-415a-a99f-626fc901b6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72812316-c54e-456b-9f19-c846bdb1b5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb203714-c049-4603-a9db-217f780c7798",
   "metadata": {},
   "source": [
    "## Summarization Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a32187-82a4-406c-895b-76bc1df8cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dbf83-9966-4fcd-8113-3449392be7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5cfb-bf75-401e-87fb-86bfde2b2b19",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "\n",
    "See the [`gpt2-xl` model details on HF](https://huggingface.co/gpt2-xl#model-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a51a05-1203-4a6f-8cb3-f9beda6ad59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(\n",
    "    gpt2_query,\n",
    "    max_length=512,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9183ed-1e4f-4aaa-a6e4-0a82fc0991bb",
   "metadata": {},
   "source": [
    "## T5\n",
    "\n",
    "See the [`tf-large` model details on HF](https://huggingface.co/t5-large#model-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf546c3-18ac-4b56-8a53-c311f1c9379f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b071a-43af-45c6-85f0-2d3b28b08653",
   "metadata": {},
   "source": [
    "## BART\n",
    "\n",
    "See the [`facebook/bart-large-cnn` model card on HF](https://huggingface.co/facebook/bart-large-cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17681b-9507-487c-8d04-1529c5cf957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb37477-65af-475f-bec2-2d59e9893832",
   "metadata": {},
   "source": [
    "## PEGASUS\n",
    "\n",
    "See the [`google/pegasus-cnn-dailymail` model card on HF](https://huggingface.co/google/pegasus-cnn_dailymail).\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This model has a dependency on the `protobuf` library, so you will need to install that as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6ddc6-87f3-442b-8ce4-58a44a304296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\"<n>\", \"\\n\").replace(\" .\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75e1a2-2315-4a03-b233-c217a8255748",
   "metadata": {},
   "source": [
    "## Comparing Different Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801908a-f117-4c6e-9568-dc83b56a5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc93e76-3e86-496c-863d-327c70428ae1",
   "metadata": {},
   "source": [
    "## Measuring the Quality of Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29769274-bfdf-4d6c-b4a5-108d15bd98fe",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "The BLEU metric is based primarily on [_precision_](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_valueshttps://en.wikipedia.org/wiki/Positive_and_negative_predictive_values), and thus it only really pays attention to how many n-grams in the references (hopefully human-generated examples of good translations) show up in the translation (generated text).\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the BLEU metric? video on Youtube](https://www.youtube.com/watch?v=M05L1DhFqcw).\n",
    "* [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)... only 8 pages!\n",
    "* [Rachel Tatman's blogpost _Evaluating Text Output in NLP: BLEU at your own risk_](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)... ?\n",
    "\n",
    "\n",
    "Regarding `datasets` and the `load_metric` API...\n",
    "\n",
    "> <pre>FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate</pre>\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `sacrebleu` library, so you will need to install that as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe444a-4f3a-4010-8940-0ee185727f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how it is done in the book...\n",
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a74f7-6b97-4672-898f-c77095717b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "foo_metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f7279-235a-47ff-bef9-b3dc1e313ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bleu_metric.add(\n",
    "    prediction=\"the the the the the the\",\n",
    "    reference=[\"the cat is on the mat\"]\n",
    ")\n",
    "\n",
    "results=bleu_metric.compute(\n",
    "    smooth_method=\"floor\", \n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b593ef-d94f-41ab-9acc-24e7250fef7b",
   "metadata": {},
   "source": [
    "TODO! explain those keys on the left..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cd930-26fb-4efc-a5d7-a512e770dc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu_metric.add(\n",
    "    prediction=\"the cat is on mat\",\n",
    "    reference=[\"the cat is on the mat\"]\n",
    ")\n",
    "\n",
    "results=bleu_metric.compute(\n",
    "    smooth_method=\"floor\", \n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d8693-3177-4c63-9f8f-784168baa6c9",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "> The ROUGE score was specifically developed for applications like summarization where high [_recall_](https://en.wikipedia.org/wiki/Sensitivity_and_specificityhttps://en.wikipedia.org/wiki/Sensitivity_and_specificity) is more important than just precision.\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the ROUGE metric? video on Youtube](https://www.youtube.com/watch?v=TMshhnrEXlg)\n",
    "* [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)... again, only 8 pages!\n",
    "\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `absl-py` and `rouge_score` libraries, so you will need to install them as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83085d6-8f9f-4c7a-a637-59da5a0d1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2797de-91fa-480c-ab9a-eb1c1e5a774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce789df8-4a97-4ba6-bbff-633ee89efa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

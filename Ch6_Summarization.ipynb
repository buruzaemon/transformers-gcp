{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d68c6fb-2413-4152-b307-92bdf4b04ed3",
   "metadata": {},
   "source": [
    "# Chapter 6: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70209eef-cc54-41f2-9bab-26e74787c108",
   "metadata": {},
   "source": [
    "## The CNN/DailyMail Dataset\n",
    "\n",
    "* ~300k pairs of news articles and their corresponding summaries\n",
    "* summaries are _abstractive_\n",
    "* [`cnn_dailymail` dataset viewer at HF](https://huggingface.co/datasets/viewer/?dataset=cnn_dailymail&config=3.0.0)\n",
    "* also see the [Dataset card for `cnn_dailymail` at HF](https://huggingface.co/datasets/cnn_dailymail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccb622-53ab-4e04-8a6b-9d1ae03f7fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"cnn_dailymail\",\n",
    "    version=\"3.0.0\"\n",
    ")\n",
    "\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13459566-2d73-436f-b38e-f25b25f574df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 char, total length: {len(sample['article'])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f\"\\nSummary (length: {len(sample['highlights'])}):\")\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68f2d2-67df-4a60-98ce-5617cd347987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9f18a-93f4-462c-831b-d13b07fd296e",
   "metadata": {},
   "source": [
    "## Text Summarization Pipelines\n",
    "\n",
    "This section require `nltk`, so be sure to download/install that before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b7478-b211-415a-a99f-626fc901b6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72812316-c54e-456b-9f19-c846bdb1b5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb203714-c049-4603-a9db-217f780c7798",
   "metadata": {},
   "source": [
    "## Summarization Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a32187-82a4-406c-895b-76bc1df8cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dbf83-9966-4fcd-8113-3449392be7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5cfb-bf75-401e-87fb-86bfde2b2b19",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "\n",
    "> Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI. \n",
    "\n",
    "* Read [\"Release Strategies and the Social Impacts of Language Models\" by Solaiman, Brundage, Clark, Askell, Herbert-Voss, Wu, Radford, Krueger, Kim, Kreps, McCain, Newhouse, Blazakis, McGuffie, and Wang on arXiv](https://arxiv.org/ftp/arxiv/papers/1908/1908.09203.pdf).\n",
    "* See the [`gpt2-xl` model details on HF](https://huggingface.co/gpt2-xl#model-details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a51a05-1203-4a6f-8cb3-f9beda6ad59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(\n",
    "    gpt2_query,\n",
    "    max_length=512,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9183ed-1e4f-4aaa-a6e4-0a82fc0991bb",
   "metadata": {},
   "source": [
    "## T5\n",
    "\n",
    "Text-To-Text Transfer Transformer...\n",
    "\n",
    "> Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. \n",
    "\n",
    "* Read [\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" by Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu on arXiv](https://arxiv.org/pdf/1910.10683).\n",
    "* See the [`tf-large` model details on HF](https://huggingface.co/t5-large#model-details).\n",
    "* [`google-research/text-to-text-transfer-transformer`](https://github.com/google-research/text-to-text-transfer-transformer) on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf546c3-18ac-4b56-8a53-c311f1c9379f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b071a-43af-45c6-85f0-2d3b28b08653",
   "metadata": {},
   "source": [
    "## BART\n",
    "\n",
    "> ... BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. \n",
    "\n",
    "* Read [\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" by Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer on arXiv](https://arxiv.org/pdf/1910.13461.pdf).\n",
    "* See the [`facebook/bart-large-cnn` model card on HF](https://huggingface.co/facebook/bart-large-cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17681b-9507-487c-8d04-1529c5cf957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb37477-65af-475f-bec2-2d59e9893832",
   "metadata": {},
   "source": [
    "## PEGASUS\n",
    "\n",
    "> Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. \n",
    "\n",
    "* Read [\"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" by Zhang, Zhao, Saleh and Liu on arXiv](https://arxiv.org/pdf/1912.08777.pdf).\n",
    "* See the model card for [`google/pegasus-cnn_dailymail`](https://huggingface.co/google/pegasus-cnn_dailymail) on HF\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This model has a dependency on the `protobuf` library, so you will need to `pip install` that as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6ddc6-87f3-442b-8ce4-58a44a304296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\"<n>\", \"\\n\").replace(\" .\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75e1a2-2315-4a03-b233-c217a8255748",
   "metadata": {},
   "source": [
    "## Comparing Different Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801908a-f117-4c6e-9568-dc83b56a5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc93e76-3e86-496c-863d-327c70428ae1",
   "metadata": {},
   "source": [
    "## Measuring the Quality of Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29769274-bfdf-4d6c-b4a5-108d15bd98fe",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "> _The closer a machine translation is to a professional human translation, the better it is._\n",
    "\n",
    "* The BLEU metric is based primarily on [_precision_](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_valueshttps://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)<p/>\n",
    "* It only really pays attention to how many n-grams in the _references_ (hopefully human-generated examples of good translations) show up in the translation (generated text).</p>\n",
    "* It takes the geometric mean of precision calculated with respect to several n-grams, usually 1-grams through 4-grams. <p/><p/><span style=\"padding-left:1.5em\">e.g., $p_{n} = \\frac{\\sum_{\\text{n-gram } \\in \\text{ reference}} \\underset{clip}{\\text{Count(n-gram)}}}{ \\sum_{\\text{n-gram } \\in \\text{ translation}} \\text{Count(n-gram)}}$</span><br/><br/><span style=\"padding-left:1.5em\">and so $\\text{BLEU-4} \\sim \\sqrt[4]{p_{1} \\cdot p_{2} \\cdot p_{3} \\cdot p_{4}}$</span><p/>\n",
    "* It also penalizes shorter translations by scaling the above-mentioned geometric mean of the n-grams with a brevity penalty ranging from `0.0` to `1.0`.<br/><span style=\"padding-left:1.5em\">e.g., $\\text{BP} = \\begin{cases} 1 & \\text{ if } c \\gt r \\\\ e^{1 - \\frac{r}{c}} & \\text{ if } c \\leq r \\end{cases}$</span><br/>where $r$ is the effective reference corpus length and $c$ is the length of the candidate translation.<p/>\n",
    "* Putting it all together, we have: <p/><p/><span style=\"padding-left:1.5em\">$\\text{BLEU-N} = \\text{BP} \\times \\left( \\prod_{n=1}^{N} p_{n} \\right)^{\\frac{1}{N}} $</span><p/>\n",
    "* Plain-vanilla BLEU assumes that the translation and reference sentences are already tokenized, with the tokenization corresponding to single words. But different models may use different tokenization schemes, so that is why `sacrebleu` is currently preferred over `bleu`. For that reason, it is also the case that `bleu`/`sacrebleu` might not work very well with non-English languages, where tokenization may be happening at the morpheme-level.\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the BLEU metric? video on Youtube](https://www.youtube.com/watch?v=M05L1DhFqcw).\n",
    "* [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)... only 8 pages!\n",
    "* [Rachel Tatman's blogpost _Evaluating Text Output in NLP: BLEU at your own risk_](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)... ?\n",
    "\n",
    "\n",
    "Regarding `datasets` and the `load_metric` API...\n",
    "\n",
    "> <pre>FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate</pre>\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `sacrebleu` library, so you will need to `pip install` that as well!</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b7ffa92-27a1-4034-9e46-e75fc94be87d",
   "metadata": {},
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a04504e-bea6-44af-9be9-e7d77b11e847",
   "metadata": {
    "tags": []
   },
   "source": [
    "# the old API, \n",
    "# somewhat different from what is shown by Lewis in the video\n",
    "old_bleu = load_metric(\"bleu\")\n",
    "\n",
    "old_bleu.add(\n",
    "    prediction=[\"I\", \"have\", \"thirty\", \"six\", \"years\"], \n",
    "    reference=[[\n",
    "        \"I\", \"am\", \"thirty\", \"six\", \"years\", \"old\"], \n",
    "        [\"I\", \"am\", \"thirty\", \"six\"]\n",
    "    ]\n",
    ")\n",
    "old_bleu.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4adb0bdc-ecfc-4d02-9065-34d1bb1915f6",
   "metadata": {},
   "source": [
    "# the old API, but using sacrebleu\n",
    "# as is shown in the book...\n",
    "sacrebleu = load_metric(\"sacrebleu\")\n",
    "\n",
    "sacrebleu.add(\n",
    "    prediction=\"I have thirty six years\",\n",
    "    reference=[\"I am thirty six years old\", \"I am thirty six\"]\n",
    ")\n",
    "\n",
    "sacrebleu.compute(\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02fa7d-03e0-4ab5-b159-6f21c952f30d",
   "metadata": {},
   "source": [
    "So we will go ahead and use the [`evaluate`](https://huggingface.co/evaluate-metric) API here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053328a-9d81-4978-a507-9513827252f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a74f7-6b97-4672-898f-c77095717b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "predictions = [\"I have thirty six years\"]\n",
    "references = [[\"I am thirty six years old\", \"I am thirty six\"]]\n",
    "\n",
    "new_bleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414025d3-43ee-42b8-ae00-1cade860b0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"I have thirty six years\"]\n",
    "references = [[\"I am thirty six years old\", \"I am thirty six\"]]\n",
    "\n",
    "new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cae496-fd0b-425a-a046-d9224945c723",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLEU (`sacrebleu`, actually) via the `evaluate` API \n",
    "\n",
    "#### Inputs\n",
    "\n",
    "* `predictions`: list of translations to score\n",
    "* `references`: list of lists of references\n",
    "* `smooth_method`: defaults to `exp` exponential decay; choose from `none`, `floor`, `add-k`, or `exp`\n",
    "* `smooth_value`: `float`\n",
    "* `tokenize`: tokenization method!\n",
    "* `lowercase`: enable/disable case-insensitivity; defaults to `False`\n",
    "* `force`: assume input is actually detokenized; defaults to `False`\n",
    "* `use_effective_order`: flag to stop inclusion of n-gram orders for which precision is `0`, so use `True` for sentence-level BLEU computations; defaults to `False`\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "* `score`: BLEU score, ranging from `0.0` to `100.0`, inclusive\n",
    "* `counts`: Counts\n",
    "* `totals`: Totals\n",
    "* `precisions`: Precisions\n",
    "* `bp`: Brevity penalty\n",
    "* `sys_len`: predictions length\n",
    "* `ref_len`: reference length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f7279-235a-47ff-bef9-b3dc1e313ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "predictions = [\"the the the the the the\"]\n",
    "references = [[\"the cat is on the mat\"]]\n",
    "\n",
    "results = new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cd930-26fb-4efc-a5d7-a512e770dc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = [\"the cat is on mat\"]\n",
    "references = [[\"the cat is on the mat\"]]\n",
    "\n",
    "results = new_sacrebleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    smooth_method=\"floor\",\n",
    "    smooth_value=0\n",
    ")\n",
    "results[\"precisions\"] = [np.round(p,2) for p in results[\"precisions\"]]\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    results,\n",
    "    orient=\"index\",\n",
    "    columns=[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d8693-3177-4c63-9f8f-784168baa6c9",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "> The ROUGE score was specifically developed for applications like summarization where high [_recall_](https://en.wikipedia.org/wiki/Sensitivity_and_specificityhttps://en.wikipedia.org/wiki/Sensitivity_and_specificity) is more important than just precision.\n",
    "\n",
    "References:\n",
    "* Lewis in the [What is the ROUGE metric? video on Youtube](https://www.youtube.com/watch?v=TMshhnrEXlg)\n",
    "* [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)... again, only 8 pages!\n",
    "\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">This metric implementation has a dependency on the `absl-py` and `rouge_score` libraries, so you will need to `pip install rouge_score` as well!</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "184bf6e9-bb17-4fe5-bf32-898cc4a306aa",
   "metadata": {},
   "source": [
    "# the old API\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdfe2b-cd2a-43f4-8637-956d1fefd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's use the rouge metric implementation in evaluate!\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159c04f-fffd-4223-8ff2-b86cc4b75891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce789df8-4a97-4ba6-bbff-633ee89efa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in summaries:\n",
    "    results = rouge.compute(\n",
    "        predictions=[summaries[model_name]],\n",
    "        references=[reference]\n",
    "    )\n",
    "    records.append(results)\n",
    "    \n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56d311-16ca-4b5e-b652-db02667b6451",
   "metadata": {},
   "source": [
    "## Evaluating PEGASUS on the CNN/DailyMail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b76f3-4177-4f4e-bc2b-ae5d1b3f13e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(\n",
    "    dataset,\n",
    "    metric,\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\"\n",
    "):\n",
    "    summaries = [\n",
    "        three_sentence_summary(text) \n",
    "        for text in dataset[column_text]\n",
    "    ]\n",
    "\n",
    "    score = metric.compute(\n",
    "        predictions=summaries,\n",
    "        references=dataset[column_summary]\n",
    "    )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ed178-9819-452f-85b3-6266581f486b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "score = evaluate_summaries_baseline(\n",
    "    test_sampled,\n",
    "    rouge\n",
    ")\n",
    "#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(\n",
    "    score, \n",
    "    orient=\"index\", \n",
    "    columns=[\"baseline\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32ea34-9e2e-46b7-9486-4d473803adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\" \n",
    "    Iterate and yield successive batch-sized chunks from list_of_elements.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i: i+batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(\n",
    "    dataset,\n",
    "    metric,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    device=device,\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\"\n",
    "):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "    num_batches = len(article_batches)\n",
    "    \n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches),\n",
    "        total=num_batches\n",
    "    ):\n",
    "        inputs = tokenizer(\n",
    "            article_batch,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        summaries = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            length_penalty=0.8,\n",
    "            num_beams=8,\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        decoded_summaries = [\n",
    "            tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            for s in summaries\n",
    "        ]\n",
    "        \n",
    "        decoded_summaries = [\n",
    "            d.replace(\"<n>\", \" \")\n",
    "            for d in decoded_summaries\n",
    "        ]\n",
    "        \n",
    "        metric.add_batch(\n",
    "            predictions=decoded_summaries,\n",
    "            references=target_batch\n",
    "        )\n",
    "    \n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333372-6bf7-4094-b78c-67648456dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "score = evaluate_summaries_pegasus(\n",
    "    test_sampled,\n",
    "    rouge,\n",
    "    model,\n",
    "    tokenizer, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(score, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0b19c-1528-480c-9c16-5b4f0f0d0b28",
   "metadata": {},
   "source": [
    "## Training a Summarization Model\n",
    "\n",
    "> The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes (non-commercial licence: CC BY-NC-ND 4.0).\n",
    "\n",
    "* See the [dataset card for the SAMSum dataset](https://huggingface.co/datasets/samsum#dataset-card-for-samsum-corpus) on HF\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">The SAMSum dataset has a dependency on the `py7zr` library, so you will need to `pip install` that as well!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444de7b-9267-4774-b662-05f2f7d1553b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5acadb9-5cac-4845-8db9-d0802fc7afcf",
   "metadata": {},
   "source": [
    "### Evaluating PEGASUS on SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbc2c7-1511-48cc-94bc-8ae896453c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d9525-721e-47be-a5fd-31822005bf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(\n",
    "    dataset_samsum[\"test\"],\n",
    "    rouge,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    column_text=\"dialogue\",\n",
    "    column_summary=\"summary\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "pd.DataFrame(score, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcba335-ad1b-46a4-b2fe-b15d78207528",
   "metadata": {},
   "source": [
    "### Fine-Tuning PEGASUS\n",
    "\n",
    "Let's compare the dialogue and summary token lengths of the SAMSum dataset with the article and highlights token lengths of the CNN/DailyMail dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4980b-ef80-46d0-bd24-e01cc0c0028a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_len = [\n",
    "    len(tokenizer.encode(s))\n",
    "    for s in dataset_samsum[\"train\"][\"dialogue\"]\n",
    "]\n",
    "s_len = [\n",
    "    len(tokenizer.encode(s))\n",
    "    for s in dataset_samsum[\"train\"][\"summary\"]\n",
    "]\n",
    "\n",
    "color = \"cornflowerblue\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "\n",
    "axes[0].hist(d_len, bins=20, color=color, edgecolor=color)\n",
    "axes[0].set_title(\"SAMSum Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(s_len, bins=20, color=color, edgecolor=color)\n",
    "axes[1].set_title(\"SAMSum Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493430d1-9c2c-4d19-8d66-0305386b39f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep things sane, let's just take a 2000-element sample\n",
    "# so that we can quickly compare CNN/DailyMail vs SAMSum\n",
    "dataset_sample = dataset[\"train\"].shuffle(seed=42)[:2000]\n",
    "\n",
    "a_len = [\n",
    "    len(tokenizer.encode(s))\n",
    "    for s in dataset_sample[\"article\"]\n",
    "]\n",
    "h_len = [\n",
    "    len(tokenizer.encode(s))\n",
    "    for s in dataset_sample[\"highlights\"]\n",
    "]\n",
    "\n",
    "color = \"darkorange\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "\n",
    "axes[0].hist(a_len, bins=20, color=color, edgecolor=color)\n",
    "axes[0].set_title(\"CNN/DailyMail Article Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(h_len, bins=20, color=color, edgecolor=color)\n",
    "axes[1].set_title(\"CNN/DailyMail Highlight Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba72b3-06fa-42de-9896-f6c61567f89e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a04ca-1bc3-452e-be7f-85712b33b47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(\n",
    "        example_batch[\"dialogue\"],\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(\n",
    "            example_batch[\"summary\"],\n",
    "            max_length=128,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(\n",
    "    convert_examples_to_features, \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a7d18-fe97-4256-ac20-e22951a0b4cd",
   "metadata": {},
   "source": [
    "##### Set up a `DataCollator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa937b-6481-47e3-905d-3228868d22ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b164c6d-d23f-4a03-8860-de743eb86cf1",
   "metadata": {},
   "source": [
    "##### Set up `TrainingArguments`\n",
    "\n",
    "See `output_dir`? You may need to visit [Create a new model repository](https://huggingface.co/new) on HF whilst logged in in order to create the model repo before executing the next set of code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273179f-5d1b-45cb-a9ab-79765bf4a24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pegasus-samsum\",\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"adamw_torch\"    # let's get rid of that annoying warning message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd7c5c-6633-42f6-b0f0-f32c042d54cf",
   "metadata": {},
   "source": [
    "Use your Notebook Token here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05bc31-bb8f-4a7e-90f0-2570db1da629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba46f0-bc7a-4ef1-98bf-1e0e3355c72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt[\"train\"],\n",
    "    eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c01f77-5b3e-4029-a72c-959a7a0dbc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "score = evaluate_summaries_pegasus(\n",
    "    dataset_samsum[\"test\"],\n",
    "    rouge,\n",
    "    trainer.model,\n",
    "    tokenizer,\n",
    "    batch_size=2,\n",
    "    column_text=\"dialogue\",\n",
    "    column_summary=\"summary\"\n",
    ")\n",
    "\n",
    "pd.DataFrame(score, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea50bce-22cc-428f-924d-86897fe14fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cd1e9-29de-4363-961b-6e3cffc64e47",
   "metadata": {},
   "source": [
    "## Generating Dialogue Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e513460-0dbe-42cd-a7c7-505db74189bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_kwargs = { \n",
    "    \"length_penalty\": 0.8,\n",
    "    \"num_beams\": 8,\n",
    "    \"max_length\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45b72c-61b2-4203-9f52-2b2edb4bbbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "pipe = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\n",
    "\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2064a4-2eed-4fc2-9a81-009617da92b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_dialogue = \"\"\"\\\n",
    "Thom: Hi guys, have you heard of transformers?\n",
    "Lewis: Yes, I used them recently!\n",
    "Leandro: Indeed, there is a great library by Hugging Face.\n",
    "Thom: I know, I helped build it ;)\n",
    "Lewis: Cool, maybe we should write a book about it. What do you think?\n",
    "Leandro: Great idea, how hard can it be?\n",
    "Thom: I am in!\n",
    "Lewis: Awesome, let's do it together!\n",
    "\"\"\"\n",
    "\n",
    "print(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

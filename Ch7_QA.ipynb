{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc883d8-2d79-4be3-aeb6-3117637f88b2",
   "metadata": {},
   "source": [
    "# Chapter 7: Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74c08f-1b10-4e29-91ea-ea9ab1726e03",
   "metadata": {},
   "source": [
    "## Building a Review-Based QA System\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "> SubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as \"How much does this product weigh?\" is a factual question (i.e., low subjectivity), while \"Is this easy to use?\" is a subjective question (i.e., high subjectivity).\n",
    ">\n",
    "> In short, SubjQA provides a setting to study how well extractive QA systems perform on finding answer that are less factual and to what extent modeling subjectivity can improve the performance of QA systems.\n",
    "\n",
    "Let's download the `subjqa` dataset and poke around a bit.\n",
    "\n",
    "See the [Dataset card fof `subjqa` at HF](https://huggingface.co/datasets/subjqa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ee076-23ff-46ce-b594-490f40c7191d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afa77f-bfe1-4b5c-8b7e-7d176c339f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")\n",
    "subjqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d71b3-b73f-45ed-a0cc-b2a031428309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#print(subjqa[\"train\"][\"answers\"][1])\n",
    "print(json.dumps(\n",
    "    subjqa[\"train\"][1], \n",
    "    indent=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710f341-d76d-43f5-833a-cf2612a58c9a",
   "metadata": {},
   "source": [
    "You see how `answers` has children `text`, `answer_start`, `answer_subj_level`, etc. \n",
    "\n",
    "If you want to explode the children of `answers` into their own columns, then use [`datasets.flatten`](https://huggingface.co/docs/datasets/process#flatten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4201c0c-de3f-4b78-90d1-193288e00654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subjqa[\"validation\"].flatten().features.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da513598-7f62-4c46-88a2-f03d0e83b0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = { \n",
    "    split: dset.to_pandas() \n",
    "    for split, dset \n",
    "    in subjqa.flatten().items() \n",
    "}\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d2e70-dd76-465b-823c-4ac047c94a62",
   "metadata": {},
   "source": [
    "Note that the `subjqa` dataset is quite small, but entirely in keeping with real-world scenarios since labelled data is very hard to find and expensive to create (you should know that!).\n",
    "\n",
    "Now that we have transformed the `dataset` into `pandas.DataFrame`, we can use things like [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) to have a closer look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aede2e-873a-45b9-9495-ac4699e70f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_cols = [\n",
    "    \"title\",\n",
    "    \"question\",\n",
    "    \"answers.text\",\n",
    "    \"answers.answer_start\",\n",
    "    \"context\"\n",
    "]\n",
    "\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b812ced-00ea-4277-b815-6c3b66edd34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0c8e7-246a-4165-a771-08bc1f26631c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = {}\n",
    "\n",
    "question_types = [\n",
    "    \"What\",\n",
    "    \"How\",\n",
    "    \"Is\",\n",
    "    \"Does\",\n",
    "    \"Do\",\n",
    "    \"Was\",\n",
    "    \"Where\",\n",
    "    \"Why\"\n",
    "]\n",
    "\n",
    "for q in question_types:\n",
    "    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
    "\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "plt.title(\"Frequency of Question Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b698a5b-a931-4a74-9775-1c847e87f454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "    for question in (\n",
    "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
    "        .sample(n=3, random_state=42)[\"question\"]\n",
    "    ):\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cef7-ca37-44c1-a916-7387c24fd5c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting Answers from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090962f-9481-4dfe-a994-678c15d9a2b7",
   "metadata": {},
   "source": [
    "On SQuAD2.0:\n",
    "\n",
    "> Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0. \n",
    "\n",
    "Suggested reading:\n",
    "* [SQuAD2.0 - The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "* [Know What You Don't Know: Unanswerable Questions for SQuAD]() by Rajpurkar, Jia, and Liang, 2018\n",
    "* [Question Answering on SQuAD2.0](https://paperswithcode.com/sota/question-answering-on-squad20) on paperswithcode.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422a8e2-3241-49e1-ace4-b581abb1571f",
   "metadata": {},
   "source": [
    "### Tokening text for QA\n",
    "\n",
    "* Training dataset is small, with only 1295 examples.\n",
    "* Since the structure of the labels for QA (predicting the start/end of an answer span) should be the same across datasets, starting from a fine-tuned, large-scale QA model is the sane approach.\n",
    "\n",
    "We will use [`deepset/minilm-uncased-squad2`](https://huggingface.co/deepset/minilm-uncased-squad2#minilm-l12-h384-uncased-for-qa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef0529-b409-4352-8554-bb9c295b7107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891906b-ff0f-4e9f-94f9-866c034f773d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MG/minute, so about 6000 hours depending on file size.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da125ce2-3b4e-411c-84b9-55569ea3579b",
   "metadata": {},
   "source": [
    "`inputs` has `input_ids` and `attention_mask` as expected, but notice how `token_type_id` indicate `0` for question token and `1` for context token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2b55d-4776-4a79-b07e-502a5799525c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    dict(\n",
    "        (k, v.tolist()[0]) \n",
    "        for k,v in inputs.items()\n",
    "    )\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ade2f8-a930-475e-9c2d-72e3e53f94e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a1abb-88d4-480e-a5e9-73b5b2af3d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fec1108-13e9-4761-8b76-891ebef5c74c",
   "metadata": {
    "tags": []
   },
   "source": [
    "dir(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072441d-f357-4dc4-bbf9-8bf2a92c4e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed755315-293b-489f-93ae-c4e3a2a3249f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c042bd-db21-4d50-b12c-53dd0488f50a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec206b27-4f52-4382-9490-4a50a18dc0de",
   "metadata": {},
   "source": [
    "... and the same as above, but now wrapped in an HF `pipeline`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec113c0-d8b9-4846-9634-51470540f28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "pipe(\n",
    "    question=question, \n",
    "    context=context, \n",
    "    topk=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd6557-8ef5-4ae9-8603-89c54766f71b",
   "metadata": {},
   "source": [
    "In the case of a question for which no answer is possble, this model will assign a high start and end score to the `[CLS]` token, mapping the output to the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7fb19-222c-4bd2-97ca-f38ee37badba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe(\n",
    "    question=\"How many roads must a man walk?\",\n",
    "    context=context,\n",
    "    handle_impossible_answer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248379fc-01f0-40cd-a6c8-fd56e668b394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc883d8-2d79-4be3-aeb6-3117637f88b2",
   "metadata": {},
   "source": [
    "# Chapter 7: Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74c08f-1b10-4e29-91ea-ea9ab1726e03",
   "metadata": {},
   "source": [
    "## Building a Review-Based QA System\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "> SubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as \"How much does this product weigh?\" is a factual question (i.e., low subjectivity), while \"Is this easy to use?\" is a subjective question (i.e., high subjectivity).\n",
    ">\n",
    "> In short, SubjQA provides a setting to study how well extractive QA systems perform on finding answer that are less factual and to what extent modeling subjectivity can improve the performance of QA systems.\n",
    "\n",
    "Let's download the `subjqa` dataset and poke around a bit.\n",
    "\n",
    "See the [Dataset card fof `subjqa` at HF](https://huggingface.co/datasets/subjqa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35ee076-23ff-46ce-b594-490f40c7191d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cf602f-7aa5-4837-a0b9-be8d05bfc5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (1295, 15), 'test': (358, 15), 'validation': (255, 15)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")\n",
    "subjqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10d71b3-b73f-45ed-a0cc-b2a031428309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": [\n",
      "    \"Bass is weak as expected\",\n",
      "    \"Bass is weak as expected, even with EQ adjusted up\"\n",
      "  ],\n",
      "  \"answer_start\": [\n",
      "    1302,\n",
      "    1302\n",
      "  ],\n",
      "  \"answer_subj_level\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"ans_subj_score\": [\n",
      "    0.5083333253860474,\n",
      "    0.5083333253860474\n",
      "  ],\n",
      "  \"is_ans_subjective\": [\n",
      "    true,\n",
      "    true\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#print(subjqa[\"train\"][\"answers\"][1])\n",
    "print(json.dumps(\n",
    "    subjqa[\"train\"][\"answers\"][1], \n",
    "    indent=2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f550cd-6361-40fc-9fa0-704a3b10e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"domain\": \"electronics\",\n",
      "  \"nn_mod\": \"harsh\",\n",
      "  \"nn_asp\": \"high\",\n",
      "  \"query_mod\": \"not strong\",\n",
      "  \"query_asp\": \"bass\",\n",
      "  \"q_reviews_id\": \"7c46670208f7bf5497480fbdbb44561a\",\n",
      "  \"question_subj_level\": 1,\n",
      "  \"ques_subj_score\": 0.5,\n",
      "  \"is_ques_subjective\": false,\n",
      "  \"review_id\": \"ce76793f036494eabe07b33a9a67288a\",\n",
      "  \"id\": \"d476830bf9282e2b9033e2bb44bbb995\",\n",
      "  \"title\": \"B00001P4ZH\",\n",
      "  \"context\": \"To anyone who hasn't tried all the various types of headphones, it is important to remember exactly what these are: cheap portable on-ear headphones. They give a totally different sound then in-ears or closed design phones, but for what they are I would say they're good. I currently own six pairs of phones, from stock apple earbuds to Sennheiser HD 518s. Gave my Portapros a run on both my computer's sound card and mp3 player, using 256 kbps mp3s or better. The clarity is good and they're very lightweight. The folding design is simple but effective. The look is certainly retro and unique, although I didn't find it as comfortable as many have claimed. Earpads are *very* thin and made my ears sore after 30 minutes of listening, although this can be remedied to a point by adjusting the \\\"comfort zone\\\" feature (tightening the temple pads while loosening the ear pads). The cord seems to be an average thickness, but I wouldn't get too rough with these. The steel headband adjusts smoothly and easily, just watch out that the slider doesn't catch your hair. Despite the sore ears, the phones are very lightweight overall.Back to the sound: as you would expect, it's good for a portable phone, but hardly earth shattering. At flat EQ the clarity is good, although the highs can sometimes be harsh. Bass is weak as expected, even with EQ adjusted up. To be fair, a portable on-ear would have a tough time comparing to the bass of an in-ear with a good seal or a pair with larger drivers. No sound isolation offered if you're into that sort of thing. Cool 80s phones, though I've certainly owned better portable on-ears (Sony makes excellent phones in this category). Soundstage is very narrow and lacks body. A good value if you can get them for under thirty, otherwise I'd rather invest in a nicer pair of phones. If we're talking about value, they're a good buy compared to new stock apple buds. If you're trying to compare the sound quality of this product to serious headphones, there's really no comparison at all.Update: After 100 hours of burn-in time the sound has not been affected in any appreciable way. Highs are still harsh, and bass is still underwhelming. I sometimes use these as a convenience but they have been largely replaced in my collection.\",\n",
      "  \"question\": \"Is this music song have a goo bass?\",\n",
      "  \"answers\": {\n",
      "    \"text\": [\n",
      "      \"Bass is weak as expected\",\n",
      "      \"Bass is weak as expected, even with EQ adjusted up\"\n",
      "    ],\n",
      "    \"answer_start\": [\n",
      "      1302,\n",
      "      1302\n",
      "    ],\n",
      "    \"answer_subj_level\": [\n",
      "      1,\n",
      "      1\n",
      "    ],\n",
      "    \"ans_subj_score\": [\n",
      "      0.5083333253860474,\n",
      "      0.5083333253860474\n",
      "    ],\n",
      "    \"is_ans_subjective\": [\n",
      "      true,\n",
      "      true\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(\n",
    "    subjqa[\"train\"][1], \n",
    "    indent=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710f341-d76d-43f5-833a-cf2612a58c9a",
   "metadata": {},
   "source": [
    "You see how `answers` has children `text`, `answer_start`, `answer_subj_level`, etc. \n",
    "\n",
    "If you want to explode the children of `answers` into their own columns, then use [`datasets.flatten`](https://huggingface.co/docs/datasets/process#flatten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4201c0c-de3f-4b78-90d1-193288e00654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': {'dtype': 'string', '_type': 'Value'},\n",
       " 'nn_mod': {'dtype': 'string', '_type': 'Value'},\n",
       " 'nn_asp': {'dtype': 'string', '_type': 'Value'},\n",
       " 'query_mod': {'dtype': 'string', '_type': 'Value'},\n",
       " 'query_asp': {'dtype': 'string', '_type': 'Value'},\n",
       " 'q_reviews_id': {'dtype': 'string', '_type': 'Value'},\n",
       " 'question_subj_level': {'dtype': 'int64', '_type': 'Value'},\n",
       " 'ques_subj_score': {'dtype': 'float32', '_type': 'Value'},\n",
       " 'is_ques_subjective': {'dtype': 'bool', '_type': 'Value'},\n",
       " 'review_id': {'dtype': 'string', '_type': 'Value'},\n",
       " 'id': {'dtype': 'string', '_type': 'Value'},\n",
       " 'title': {'dtype': 'string', '_type': 'Value'},\n",
       " 'context': {'dtype': 'string', '_type': 'Value'},\n",
       " 'question': {'dtype': 'string', '_type': 'Value'},\n",
       " 'answers.text': {'feature': {'dtype': 'string', '_type': 'Value'},\n",
       "  '_type': 'Sequence'},\n",
       " 'answers.answer_start': {'feature': {'dtype': 'int32', '_type': 'Value'},\n",
       "  '_type': 'Sequence'},\n",
       " 'answers.answer_subj_level': {'feature': {'dtype': 'int64', '_type': 'Value'},\n",
       "  '_type': 'Sequence'},\n",
       " 'answers.ans_subj_score': {'feature': {'dtype': 'float32', '_type': 'Value'},\n",
       "  '_type': 'Sequence'},\n",
       " 'answers.is_ans_subjective': {'feature': {'dtype': 'bool', '_type': 'Value'},\n",
       "  '_type': 'Sequence'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjqa[\"validation\"].flatten().features.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da513598-7f62-4c46-88a2-f03d0e83b0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1295\n",
      "Number of questions in test: 358\n",
      "Number of questions in validation: 255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = { \n",
    "    split: dset.to_pandas() \n",
    "    for split, dset \n",
    "    in subjqa.flatten().items() \n",
    "}\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d2e70-dd76-465b-823c-4ac047c94a62",
   "metadata": {},
   "source": [
    "Note that the `subjqa` dataset is quite small, but entirely in keeping with real-world scenarios since labelled data is very hard to find and expensive to create (you should know that!).\n",
    "\n",
    "Now that we have transformed the `dataset` into `pandas.DataFrame`, we can use things like [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) to have a closer look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24aede2e-873a-45b9-9495-ac4699e70f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>B005DKZTMG</td>\n",
       "      <td>Does the keyboard lightweight?</td>\n",
       "      <td>[this keyboard is compact]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>B00AAIPT76</td>\n",
       "      <td>How is the battery?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I bought this after the first spare gopro batt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                        question                answers.text  \\\n",
       "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
       "1159  B00AAIPT76             How is the battery?                          []   \n",
       "\n",
       "     answers.answer_start                                            context  \n",
       "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
       "1159                   []  I bought this after the first spare gopro batt...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\n",
    "    \"title\",\n",
    "    \"question\",\n",
    "    \"answers.text\",\n",
    "    \"answers.answer_start\",\n",
    "    \"context\"\n",
    "]\n",
    "\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b812ced-00ea-4277-b815-6c3b66edd34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this keyboard is compact'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb0c8e7-246a-4165-a771-08bc1f26631c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = {}\n",
    "\n",
    "question_types = [\n",
    "    \"What\",\n",
    "    \"How\",\n",
    "    \"Is\",\n",
    "    \"Does\",\n",
    "    \"Do\",\n",
    "    \"Was\",\n",
    "    \"Where\",\n",
    "    \"Why\"\n",
    "]\n",
    "\n",
    "for q in question_types:\n",
    "    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
    "\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "plt.title(\"Frequency of Question Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b698a5b-a931-4a74-9775-1c847e87f454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the camera?\n",
      "How do you like the control?\n",
      "How fast is the charger?\n",
      "What is direction?\n",
      "What is the quality of the construction of the bag?\n",
      "What is your impression of the product?\n",
      "Is this how zoom works?\n",
      "Is sound clear?\n",
      "Is it a wireless keyboard?\n"
     ]
    }
   ],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "    for question in (\n",
    "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
    "        .sample(n=3, random_state=42)[\"question\"]\n",
    "    ):\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cef7-ca37-44c1-a916-7387c24fd5c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting Answers from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090962f-9481-4dfe-a994-678c15d9a2b7",
   "metadata": {},
   "source": [
    "On SQuAD2.0:\n",
    "\n",
    "> Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0. \n",
    "\n",
    "Suggested reading:\n",
    "* [SQuAD2.0 - The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "* [Know What You Don't Know: Unanswerable Questions for SQuAD]() by Rajpurkar, Jia, and Liang, 2018\n",
    "* [Question Answering on SQuAD2.0](https://paperswithcode.com/sota/question-answering-on-squad20) on paperswithcode.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422a8e2-3241-49e1-ace4-b581abb1571f",
   "metadata": {},
   "source": [
    "### Tokening text for QA\n",
    "\n",
    "* Training dataset is small, with only 1295 examples.\n",
    "* Since the structure of the labels for QA (predicting the start/end of an answer span) should be the same across datasets, starting from a fine-tuned, large-scale QA model is the sane approach.\n",
    "\n",
    "We will use [`deepset/minilm-uncased-squad2`](https://huggingface.co/deepset/minilm-uncased-squad2#minilm-l12-h384-uncased-for-qa).\n",
    "\n",
    "> Language model: microsoft/MiniLM-L12-H384-uncased<br/>\n",
    "> Language: English<br/>\n",
    "> Downstream-task: Extractive QA<br/>\n",
    "> Training data: SQuAD 2.0<br/>\n",
    "> Eval data: SQuAD 2.0<br/>\n",
    "> Code: See an example QA pipeline on Haystack<br/>\n",
    "> Infrastructure: 1x Tesla v100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ef0529-b409-4352-8554-bb9c295b7107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d891906b-ff0f-4e9f-94f9-866c034f773d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MG/minute, so about 6000 hours depending on file size.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da125ce2-3b4e-411c-84b9-55569ea3579b",
   "metadata": {},
   "source": [
    "`inputs` has `input_ids` and `attention_mask` as expected, but notice how `token_type_id` indicate `0` for question token and `1` for context token. There should be a total of 28 tokens in the concatenation of `question` and `context`, including the tokens for `[CLS]` and `[SEP]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a2b55d-4776-4a79-b07e-502a5799525c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2172</td>\n",
       "      <td>2189</td>\n",
       "      <td>2064</td>\n",
       "      <td>2023</td>\n",
       "      <td>2907</td>\n",
       "      <td>1029</td>\n",
       "      <td>102</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>2061</td>\n",
       "      <td>2055</td>\n",
       "      <td>25961</td>\n",
       "      <td>2847</td>\n",
       "      <td>5834</td>\n",
       "      <td>2006</td>\n",
       "      <td>5371</td>\n",
       "      <td>2946</td>\n",
       "      <td>1012</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_ids</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0     1     2     3     4     5     6     7    8     9   ...  \\\n",
       "input_ids       101  2129  2172  2189  2064  2023  2907  1029  102  2019  ...   \n",
       "token_type_ids    0     0     0     0     0     0     0     0    0     1  ...   \n",
       "attention_mask    1     1     1     1     1     1     1     1    1     1  ...   \n",
       "\n",
       "                  18    19     20    21    22    23    24    25    26   27  \n",
       "input_ids       2061  2055  25961  2847  5834  2006  5371  2946  1012  102  \n",
       "token_type_ids     1     1      1     1     1     1     1     1     1    1  \n",
       "attention_mask     1     1      1     1     1     1     1     1     1    1  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    dict(\n",
    "        (k, v.tolist()[0]) \n",
    "        for k,v in inputs.items()\n",
    "    )\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ade2f8-a930-475e-9c2d-72e3e53f94e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how much music can this hold? [SEP] an mp3 is about 1 mg / minute, so about 6000 hours depending on file size. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "661a1abb-88d4-480e-a5e9-73b5b2af3d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-1.1286, -4.7415, -5.3876, -5.2379, -5.2723, -5.5013, -4.9711, -6.1821,\n",
      "         -1.1286, -0.0502, -0.3999, -1.9431,  3.2140,  4.1175, -1.1694, -3.8895,\n",
      "         -2.1539, -4.5801, -1.3942,  4.1693,  5.3227, -0.0295, -3.0786, -4.8637,\n",
      "         -2.3779, -3.5348, -3.5455, -1.1286]]), end_logits=tensor([[-1.0579, -5.4757, -5.0125, -5.1458, -5.4216, -5.4915, -5.1583, -4.5826,\n",
      "         -1.0580, -3.8277, -1.1060, -3.8418, -3.2789, -1.2299, -0.9271, -3.3560,\n",
      "          4.1270,  0.2892, -3.2542, -3.1378,  1.0524,  5.8469, -0.2193, -4.8842,\n",
      "         -3.3028, -0.1381,  1.6563, -1.0579]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c072441d-f357-4dc4-bbf9-8bf2a92c4e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the logits for the predictions of start/end indices...\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed755315-293b-489f-93ae-c4e3a2a3249f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 28])\n",
      "Start logits shape: torch.Size([1, 28])\n",
      "End logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0c042bd-db21-4d50-b12c-53dd0488f50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much music can this hold?\n",
      "Answer: 6000 hours\n"
     ]
    }
   ],
   "source": [
    "# get the indices for start and end of the predicted answer span...\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "# decode the answer span...\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec206b27-4f52-4382-9490-4a50a18dc0de",
   "metadata": {},
   "source": [
    "... and the same as above, but now wrapped in an [HF `pipeline` for Question Answering](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ec113c0-d8b9-4846-9634-51470540f28e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/qackery/.cache/torch_extensions/py38_cu112 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Failed to load CUDA kernels. Mra requires custom CUDA kernels. Please verify that compatible versions of PyTorch and CUDA Toolkit are installed: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:323: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.468392014503479, 'start': 38, 'end': 48, 'answer': '6000 hours'},\n",
       " {'score': 0.14781400561332703,\n",
       "  'start': 32,\n",
       "  'end': 48,\n",
       "  'answer': 'about 6000 hours'},\n",
       " {'score': 0.14034169912338257,\n",
       "  'start': 16,\n",
       "  'end': 48,\n",
       "  'answer': '1 MG/minute, so about 6000 hours'},\n",
       " {'score': 0.05686333402991295,\n",
       "  'start': 10,\n",
       "  'end': 48,\n",
       "  'answer': 'about 1 MG/minute, so about 6000 hours'},\n",
       " {'score': 0.025132538750767708,\n",
       "  'start': 16,\n",
       "  'end': 27,\n",
       "  'answer': '1 MG/minute'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "pipe(\n",
    "    question=question,\n",
    "    context=context, \n",
    "    topk=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd6557-8ef5-4ae9-8603-89c54766f71b",
   "metadata": {},
   "source": [
    "In the case of a question for which no answer is possble, this model will assign a high start and end score to the `[CLS]` token, mapping the output to the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f7fb19-222c-4bd2-97ca-f38ee37badba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9904054999351501, 'start': 0, 'end': 0, 'answer': ''}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    question=\"How many roads must a man walk?\",\n",
    "    context=context,\n",
    "    handle_impossible_answer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0691c-ac98-4485-9c1c-f14141c59ca5",
   "metadata": {},
   "source": [
    "### Dealing with long passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b15325a-bc5c-41d4-bcdc-b3f32dcca6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    return_overflowing_tokens=True,\n",
    "    max_length=100,\n",
    "    stride=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03f465a-01f7-47f2-9e42-8835bda4f233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window #0 has 100 tokens\n",
      "Window #1 has 88 tokens\n"
     ]
    }
   ],
   "source": [
    "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
    "    print(f\"Window #{idx} has {len(window)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebbf4b2a-17a7-4823-ae41-1a35e80c8af7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2129, 2003, 1996, 3321, 1029, 102, 1045, 2031, 2018, 12849, 4757, 2132, 19093, 1999, 1996, 2627, 1010, 4013, 26424, 2050, 1998, 1053, 2480, 1011, 5585, 1012, 1996, 12849, 4757, 3417, 9331, 3217, 2003, 12109, 1998, 2038, 2307, 3321, 3433, 1012, 1996, 2147, 2307, 2007, 2026, 11924, 3042, 1998, 2064, 2022, 1000, 4565, 2039, 1000, 2000, 2022, 3344, 1999, 2026, 9055, 6598, 2030, 3274, 4524, 2302, 2893, 24514, 2098, 1012, 2027, 2024, 2200, 2422, 1998, 2079, 2025, 2514, 3082, 2030, 4562, 2091, 2006, 2115, 5551, 2130, 2044, 5962, 2000, 2189, 2007, 2068, 2006, 2035, 2154, 1012, 1996, 2614, 2003, 102]\n",
      "[101, 2129, 2003, 1996, 3321, 1029, 102, 1998, 2079, 2025, 2514, 3082, 2030, 4562, 2091, 2006, 2115, 5551, 2130, 2044, 5962, 2000, 2189, 2007, 2068, 2006, 2035, 2154, 1012, 1996, 2614, 2003, 2305, 1998, 2154, 2488, 2084, 2151, 4540, 1011, 13007, 2071, 2022, 1998, 2024, 2471, 2004, 2204, 2004, 1996, 4013, 26424, 2050, 1012, 2027, 2024, 1000, 2330, 2250, 1000, 2132, 19093, 2061, 2017, 3685, 2674, 1996, 3321, 2000, 1996, 10203, 4127, 1010, 2021, 2009, 3310, 2485, 1012, 2005, 1002, 3590, 1010, 2017, 3685, 2175, 3308, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "for window in tokenized_example[\"input_ids\"]:\n",
    "    print(f\"{window}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09591e9f-550a-4ee4-81fd-17e5ecbbb878",
   "metadata": {},
   "source": [
    "To clarify, here is are the first 75 tokens of the 100 tokens that make up the first window; along with the trailing 25 tokens that make up the stride (overlap). Tokens have been decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e957060-9aed-4dda-aab0-fd9ce92a185b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens 0 through 74 for window 0\n",
      "=====\n",
      "i have had koss headphones in the past, pro 4aa and qz - 99. the koss portapro is portable and has great bass response. the work great with my android phone and can be \" rolled up \" to be carried in my motorcycle jacket or computer bag without getting crunched. they are very light\n",
      "\n",
      "... and the last 25 tokens for window 0 making up the stride:\n",
      "=====\n",
      "and do not feel heavy or bear down on your ears even after listening to music with them on all day. the sound is [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens 0 through 74 for window 0\\n=====\\n{tokenizer.decode(tokenized_example['input_ids'][0][7:-26])}\")\n",
    "print()\n",
    "print(f\"... and the last 25 tokens for window 0 making up the stride:\\n=====\\n{tokenizer.decode(tokenized_example['input_ids'][0][-26:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61e03b-9d42-45c5-b1e8-156a6be5b548",
   "metadata": {
    "tags": []
   },
   "source": [
    "And here is what window 1 looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3698b4-d416-403a-a974-37a270475992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens 0 through 25 for the stride (overlapping) input continuation in window 1\n",
      "=====\n",
      "and do not feel heavy or bear down on your ears even after listening to music with them on all day. the sound is\n",
      "\n",
      "... and the trailing remaining tokens for window 1\n",
      "=====\n",
      "night and day better than any ear - bud could be and are almost as good as the pro 4aa. they are \" open air \" headphones so you cannot match the bass to the sealed types, but it comes close. for $ 32, you cannot go wrong. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens 0 through 25 for the stride (overlapping) input continuation in window 1\\n=====\\n{tokenizer.decode(tokenized_example['input_ids'][1][7:7+25])}\")\n",
    "print()\n",
    "print(f\"... and the trailing remaining tokens for window 1\\n=====\\n{tokenizer.decode(tokenized_example['input_ids'][1][7+25:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb06d1-b93b-4753-9e21-cece3b5b0e32",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda1d32-67b8-41a1-8de6-47e22614d58c",
   "metadata": {},
   "source": [
    "### Using Haystack to Build a QA Pipeline\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">_WARNING!!! The farm-haystack API has changed drastically since the writing of this book!_</span>\n",
    "\n",
    "While referring to the [Quick Start on haystack.deepset.ai](https://haystack.deepset.ai/overview/quick-start), [Tutorial: Build Your First Question Answering System](https://haystack.deepset.ai/tutorials/01_basic_qa_pipeline), as well as [Tutorial: Evaluation of a QA System](https://haystack.deepset.ai/tutorials/05_evaluation) on the Haystack by deepset website, we will make appropriate changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60901005-b817-46eb-8a1c-e0a67e8fd9e7",
   "metadata": {},
   "source": [
    "#### Installing `farm-haystack`\n",
    "\n",
    "This part will require the installation of `farm-haystack` (NOT to be confused with `haystack`!).\n",
    "\n",
    "    pip install farm-haystack\n",
    "    \n",
    "    pip install 'farm-haystack[elasticsearch]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d8f66f-fc66-41ef-a683-02c412112c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE, STDOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93bf4db-6f21-4af5-8dff-238d58bff176",
   "metadata": {},
   "source": [
    "#### Installing ElasticSearch\n",
    "\n",
    "* The following code block downloads and install `elasticsearch`.\n",
    "* This bit needs to be done only _once_!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8ddc22f-ee82-453b-966a-dadc01d5a22a",
   "metadata": {
    "tags": []
   },
   "source": [
    "url = \"https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz\"\n",
    "\n",
    "!wget -nc -q {url}\n",
    "\n",
    "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bdd9f-164d-47e7-a5f9-9d2359aa2c33",
   "metadata": {},
   "source": [
    "#### Start `elasticsearch` server\n",
    "\n",
    "Execute the following code block to start up `elasticsearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a7b68-9bde-4a42-9231-6b3b95d0cea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will start up elasticsearch and create a PID file...\n",
    "es_server = Popen(\n",
    "    args=[\"elasticsearch-7.9.2/bin/elasticsearch\", \"-p\", \"pidfile\"],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT\n",
    ")\n",
    "\n",
    "# wait until Elasticsearch has started..\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda42f3c-e2e7-473a-a993-b87ba8702e54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `curl` call to check `elasticsearch` server status\n",
    "\n",
    "This `curl` call can be used to check if `elasticsearch` is running or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80e569-27c3-4d2b-bf4f-e43a193d8591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ff636-fdc3-4b61-b412-d1430cdfe06b",
   "metadata": {},
   "source": [
    "#### Stop `elasticsearch` server\n",
    "\n",
    "Execute the following code block to stop `elasticsearch`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4142cddc-ea7b-4b55-bbd6-79ab47260af7",
   "metadata": {
    "tags": []
   },
   "source": [
    "pidfile = Path(os.getcwd()) / Path(\"elasticsearch-7.9.2/pidfile\")\n",
    "\n",
    "pid = pidfile.read_text()\n",
    "\n",
    "stop_server = Popen(\n",
    "    args=[\"kill\", \"-15\", pid],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fda817-f17d-4809-acb9-c13c3ae7a306",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46692051-4fc3-4656-949a-8319a3050702",
   "metadata": {},
   "source": [
    "### Instantiate a document store\n",
    "\n",
    "> You can think of the DocumentStore as a database that stores your texts and meta data\n",
    "> and provides them to the Retriever at query time.<p/>\n",
    "> By far the most common way to use a DocumentStore in Haystack is to fetch documents using a Retriever. You provide a DocumentStore as an argument when you initialize a Retriever."
   ]
  },
  {
   "cell_type": "raw",
   "id": "03f242c2-90bd-4859-a0c0-0dda5568af4a",
   "metadata": {},
   "source": [
    "# code on page 184, Ch. 7 is outdated!\n",
    "from haystack.documentstore.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead7804-1daf-4383-b931-adc6ec2aad9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# return the document embedding for later use with dense retriever\n",
    "#                                                  ^^^^^\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    return_embedding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06cdef0-ec8b-4b31-905c-3d3e3dc0f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews, i.e., no duplicated contexts\n",
    "    docs = [{\n",
    "        \"content\": row[\"context\"],\n",
    "        \"meta\": {\n",
    "            \"item_id\": row[\"title\"],\n",
    "            \"question_id\": row[\"id\"],\n",
    "            \"split\": split}}\n",
    "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "    \n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6441b5-b201-4006-b7c2-97da8c2df7b1",
   "metadata": {},
   "source": [
    "### Initializing a retriever\n",
    "\n",
    "> The Retriever performs document retrieval by sweeping through a DocumentStore and returning a set of candidate Documents that are relevant to the query. See what Retrievers are available and how to choose the best one for your use case.<p/>\n",
    "> The Retriever is tightly coupled with the DocumentStore. You must specify a DocumentStore when initializing the Retriever.<p/>\n",
    "> When used in combination with a Reader, the Retriever can quickly sift out irrelevant Documents, saving the Reader from doing more work than it needs to and speeding up the querying process.\n",
    "\n",
    "The following kinds of retrievers are available in Haystack:\n",
    "\n",
    "* `sparse`\n",
    "* `dense`\n",
    "* `multimodal`\n",
    "* `web`\n",
    "* `link_content`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6ea38c4-c4b7-4878-9897-613c3ed6e392",
   "metadata": {},
   "source": [
    "# code on Ch. 7, page 186 is outdated!\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "\n",
    "es_retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a781a33-0d06-4f1b-8880-55b6d24af194",
   "metadata": {},
   "source": [
    "#### Text Retrieval with `BM25Retriever`\n",
    "\n",
    ">Use BM25 if you are looking for a retrieval method that doesn't need a neural network for indexing. BM25 is a variant of TF-IDF. It improves upon its predecessor in two main aspects:<p/>\n",
    "> * It saturates `tf` after a set number of occurrences of the given term in the document\n",
    "> * It normalises by document length so that short documents are favoured over long documents if they have the same amount of word overlap with the query\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cde23e-e231-4d63-85ab-8f685bf809ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "es_retriever = BM25Retriever(\n",
    "    document_store=document_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2f474-6347-449e-b91b-a31d659da857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "\n",
    "retrieved_docs = es_retriever.retrieve(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    "    filters={\n",
    "        \"item_id\": item_id,\n",
    "        \"split\": [\"train\"]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a109864-491d-4843-818c-bd2715e07cf4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#doc = retrieved_docs[0]\n",
    "#print(type(doc))\n",
    "#print()\n",
    "print(json.dumps(doc.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f83f3-5816-4458-b44c-a66c744b48bc",
   "metadata": {},
   "source": [
    "### Initializing a reader\n",
    "\n",
    "> A Reader scans the texts it received from the Retriever and extracts the top answer candidates. Readers are based on powerful deep learning models but are much slower than Retrievers at processing the same amount of text.\n",
    "> <br/>... from [Tutorial: Build Your First Question Answering System](https://haystack.deepset.ai/tutorials/01_basic_qa_pipeline)\n",
    "> <br/><br/>MiniLM\n",
    "> <br/>A distilled model that sacrifices a little accuracy for speed.\n",
    "> <br/>Pro: 40% smaller, 50% faster inference speed, and better accuracy than BERT base.\n",
    "> <br/>Con: Still doesn’t match the best base-sized models in accuracy.\n",
    "> <br/>... from [Models, Haystack documentation](https://docs.haystack.deepset.ai/docs/reader#models)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ba6089c-dfeb-43c6-b372-403ae55bbfe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# code on page 187, Ch. 7 is outdated!\n",
    "from haystack.reader.farm import FarmReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34223ef-79b4-48bd-baf6-1ba23f14947a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_len, doc_stride = 384, 128\n",
    "\n",
    "reader = FARMReader(\n",
    "    model_name_or_path=\"deepset/roberta-base-squad2\", \n",
    "    progress_bar=False,\n",
    "    max_seq_len=max_seq_len,\n",
    "    doc_stride=doc_stride,\n",
    "    return_no_answer=False\n",
    "    #return_no_answer=False  # let's not bother with no-answer responses!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9b567-7322-441c-b206-0a90d0357ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = reader.predict_on_texts(\n",
    "    question=question, \n",
    "    texts=[context],\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "#print(f\"Q: {res['query']}\")\n",
    "#print(f\"A: {res['answers'][0].answer}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594430bf-16d4-4b6a-840d-bce92f6760c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6db7f1-feb1-426d-ab28-e0fe0178dbd8",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "\n",
    "The [API for `ExtractiveQAPipeline`](https://docs.haystack.deepset.ai/reference/pipelines-api#extractiveqapipeline) appears to have changed drastically:\n",
    "\n",
    "* `top_k_retrievers` is gone as direct parameter; use `params` and `Retriever` node name\n",
    "* `top_k_readers` is gone as direct parameter; use `params` and `Reader` node name\n",
    "* `filters` s gone as direct parameter; [use `params` and `filters` to filter results by using `meta` fields](https://docs.haystack.deepset.ai/docs/metadata-filtering#basic-filters)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c7fe2d9-7d88-4d61-b4d8-d3b62cd23930",
   "metadata": {},
   "source": [
    "# code on page 188, Ch. 7 is outdated!\n",
    "from haystack.pipeline import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea0e4f-f229-4a53-a33b-64bdbb16b3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(\n",
    "    retriever=es_retriever,\n",
    "    reader=reader\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1093ac30-7988-4e74-b2a5-afb8309b19c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# code on page 188, Ch. 7 is outdated!\n",
    "from haystack.pipeline import ExtractiveQAPipeline\n",
    "\n",
    "...\n",
    "\n",
    "preds = pipe.run(\n",
    "    query=query,\n",
    "    top_k_retrievers=3,\n",
    "    top_k_readers=n_answers,\n",
    "    filters={\n",
    "        \"item_id\": item_id,\n",
    "        \"split\": [\"train\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "for idx in range(n_answers):\n",
    "    print(f\"Answer: {idx+1}: {preds['answers'][idx]['answer']}\")\n",
    "    print(f\"Review snippet: ...{preds['answers'][idx]['context']}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e87ed1-a7d6-4d40-a940-fb66b26ab365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_answers = 3\n",
    "\n",
    "preds = pipe.run(\n",
    "    query=query,\n",
    "    params={\n",
    "        \"filters\": {\n",
    "            \"item_id\": item_id,\n",
    "            \"split\": [\"train\"]\n",
    "        },\n",
    "        \"Retriever\": {\"top_k\": 3},\n",
    "        \"Reader\": {\"top_k\": 3}\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Question: {preds['query']}\\n\")\n",
    "\n",
    "for i,answer in enumerate(preds['answers']):\n",
    "    print(f\"Answer {i+1}: {answer.answer}\")\n",
    "    print(f\"Review snippet: ...{answer.context}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ffec9-1c4d-416d-94d7-4e99b9f780ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "Following the example of [Tutorial: Evaluation of a QA System](https://haystack.deepset.ai/tutorials/05_evaluation), let's just start from a brand-new document store.\n",
    "\n",
    "We begin with setting up a new document store in Elastic, explicitly specifying an `index` and `label_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3150147-22d4-4e68-aaac-4fc2e13d6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these indices do not collide with existing ones, \n",
    "# the indices will be wiped clean before data is inserted\n",
    "doc_index = \"test_docs\"\n",
    "label_index = \"test_labels\"\n",
    "\n",
    "# Elasticsearch document store specifically for calculating precision\n",
    "# on the \"test\" data...\n",
    "test_document_store = ElasticsearchDocumentStore(\n",
    "    index=doc_index,\n",
    "    label_index=label_index,\n",
    "    return_embedding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68246f08-c197-4cdc-b6f5-c98825a3f78e",
   "metadata": {},
   "source": [
    "Sanity check on our `test_document_store`, which might not be empty if we are repeatedly executing these code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242716c8-b3d9-4ff4-947f-5729e5a27797",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_document_store.get_document_count() > 0:\n",
    "    test_document_store.delete_documents()\n",
    "if test_document_store.get_label_count() > 0:\n",
    "    test_document_store.delete_labels()\n",
    "\n",
    "# check!\n",
    "print(f\"document count: {test_document_store.get_document_count()}\")\n",
    "print(f\"label count:    {test_document_store.get_label_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e3650-1453-42af-abee-d1e615fc9bd5",
   "metadata": {},
   "source": [
    "#### Load up documents from the `test` dataset\n",
    "\n",
    "_... wonder why the book doesn't use the `validation` dataset?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288522b8-0f09-4b8e-9cfb-542015640dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs[\"test\"]\n",
    "\n",
    "# exclude duplicate reviews?\n",
    "docs = [{\n",
    "    \"content\": row[\"context\"],\n",
    "    \"meta\": {\n",
    "        \"item_id\": row[\"title\"],\n",
    "        \"question_id\": row[\"id\"],\n",
    "        \"split\": \"test\"}}\n",
    "    for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "\n",
    "test_document_store.write_documents(\n",
    "    docs, \n",
    "    index=doc_index\n",
    ")\n",
    "    \n",
    "print(f\"Loaded {test_document_store.get_document_count()} documents\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff796f-ba10-4e85-9f25-05b60384c2ae",
   "metadata": {},
   "source": [
    "#### Create and load up `Label`s from `test`\n",
    "\n",
    "For QA, the _\"labels\"_ are spans that indicate the start and end indices of the correct answer to the corresponding question.\n",
    "\n",
    "Here, we create the labels for the answers to questions in the `test` dataset manually.\n",
    "\n",
    "Please see the following APIs for Haystack primitivies:\n",
    "* [`Label`](https://docs.haystack.deepset.ai/reference/primitives-api#label)\n",
    "* [`Answer`](https://docs.haystack.deepset.ai/reference/primitives-api#answer)\n",
    "* [`Span`](https://docs.haystack.deepset.ai/reference/primitives-api#span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d82e7-d66f-46f7-a746-a29c8f36dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Label, Answer, Span\n",
    "\n",
    "labels = []\n",
    "for _, row in df.iterrows():\n",
    "    meta = {\n",
    "        \"item_id\": row[\"title\"],\n",
    "        \"question_id\": row[\"id\"],\n",
    "        \"split\": \"test\"\n",
    "    }\n",
    "\n",
    "    document = test_document_store.get_all_documents(\n",
    "        filters={\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
    "    )[0]\n",
    "\n",
    "    if len(row[\"answers.text\"]):\n",
    "        for i,answer in enumerate(row[\"answers.text\"]):\n",
    "            sidx = row[\"answers.answer_start\"][i]\n",
    "            eidx = sidx + len(answer)\n",
    "            \n",
    "            label = Label(\n",
    "                query=row[\"question\"],\n",
    "                document=document,\n",
    "                is_correct_answer=True,\n",
    "                is_correct_document=True,\n",
    "                origin=\"gold-label\",\n",
    "                answer=Answer(\n",
    "                    answer=answer, \n",
    "                    type=\"extractive\",\n",
    "                    offsets_in_document=[Span(sidx, eidx)]\n",
    "                ),\n",
    "                meta=meta\n",
    "            )\n",
    "            labels.append(label)\n",
    "    else:\n",
    "        label = Label(\n",
    "            query=row[\"question\"],\n",
    "            document=document,\n",
    "            is_correct_answer=True,\n",
    "            is_correct_document=True,\n",
    "            origin=\"gold-label\",\n",
    "            answer=None,\n",
    "            meta=meta\n",
    "        )\n",
    "        labels.append(label)\n",
    "\n",
    "test_document_store.write_labels(\n",
    "    labels,\n",
    "    index=label_index\n",
    ")\n",
    "\n",
    "print(f\"Loaded {test_document_store.get_label_count()} question-answer pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92a6da-c686-472b-a6ad-999356b4c5cf",
   "metadata": {},
   "source": [
    "#### Now gather up the `Label` instances for evaluation...\n",
    "\n",
    "Most of the following comes from the Advanced [Evaluation of a QA System tutorial](https://haystack.deepset.ai/tutorials/05_evaluation) on Haystack. Please see API documentation for [`BaseDocumentStore.get_all_labels_aggregated`](https://docs.haystack.deepset.ai/reference/document-store-api#basedocumentstoreget_all_labels_aggregated).\n",
    "\n",
    "The key points here when gathering up the `Label` instances for our evaluation of the retriever are:\n",
    "* explicitly specify the document store index for the `Label`s\n",
    "* explicitly specify `open_domain=True` as the questions being asked of the full collection of documents in the dataset\n",
    "* use `aggregate_by_meta` to explicity specify the identification key in our documents (not the default `Document` key!)\n",
    "* set `drop_negative_labels` and `drop_no_answers` to `True` per the tutorial\n",
    "\n",
    "\n",
    "<span style=\"background-color: #9AFEFF\">**WARNING!!!**</span>\n",
    "* <span style=\"background-color: #9AFEFF\">If you do not explicitly specify `drop_negative_labels=True`, then you will see `ValueError: max() arg is an empty sequence`!</span>\n",
    "* <span style=\"background-color: #9AFEFF\">And if you don't set `drop_no_answers=True`, then you may see errors where a no-answer `Label` is returned.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e16b6d-f6ee-4cef-9df7-0f7c29cf4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels = test_document_store.get_all_labels_aggregated(\n",
    "    index=label_index,\n",
    "    open_domain=True,\n",
    "    aggregate_by_meta=[\"item_id\"],\n",
    "    drop_negative_labels=True,         # **** IMPORTANT: see Evaluation of an ExtractiveQAPipeline\n",
    "    drop_no_answers=True               # ****\n",
    ")\n",
    "print(len(eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dbec9-4288-4fdd-b422-b216af3cad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_label = eval_labels[100].labels[0]\n",
    "\n",
    "print(a_label)\n",
    "print()\n",
    "print(a_label.query)\n",
    "print(a_label.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b24da-57b6-46c8-89c9-ec9445d9877d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f510da-7d77-4a6a-8af1-0658fdfb1716",
   "metadata": {},
   "source": [
    "#### Set up an `ExtractiveQAPipeline` for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9afda-e25a-44ff-8a66-e5f594adf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = ExtractiveQAPipeline(\n",
    "\n",
    "    retriever=BM25Retriever(\n",
    "        document_store=test_document_store\n",
    "    ), \n",
    "\n",
    "    reader=reader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab092a80-462f-4748-8b02-383ebf3743aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can run the pipeline with the desired top_k value like this\n",
    "eval_result = test_pipe.eval(\n",
    "    labels=eval_labels,\n",
    "    params={\"Retriever\": {\"top_k\": 3}},\n",
    ")\n",
    "metrics = eval_result.calculate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eed07a-5dc5-4a84-8836-143628e12ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall@3: {metrics['Retriever']['recall_single_hit']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bad57a-3896-47f2-9cce-0ff8832fd025",
   "metadata": {},
   "source": [
    "#### Calculate the recall for the retriever across different values of `top_k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467103b4-55b9-4a19-8567-2971169babe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_retriever(pipeline, eval_labels, topk_values=[1,3,5,10,20]):\n",
    "    topk_results = {}\n",
    "\n",
    "    for k in topk_values:\n",
    "        eval_result = pipeline.eval(\n",
    "            labels=eval_labels, \n",
    "            params={\"Retriever\": {\"top_k\": k}}\n",
    "        )\n",
    "        metrics = eval_result.calculate_metrics()\n",
    "        topk_results[k] = {\n",
    "            \"recall_single_hit\": metrics[\"Retriever\"][\"recall_single_hit\"],\n",
    "            \"recall_multi_hit\": metrics[\"Retriever\"][\"recall_multi_hit\"]\n",
    "        }\n",
    "    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n",
    "\n",
    "es_topk_df = evaluate_retriever(test_pipe, eval_labels)\n",
    "print(es_topk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f72b44-cd2f-4476-b244-f8ceec589959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_receiver_eval(dfs, retrievers):\n",
    "    fig, ax = plt.subplots()\n",
    "    for df, r in zip(dfs, retrievers):\n",
    "        df.plot(y=\"recall_single_hit\", ax=ax, label=r)\n",
    "    plt.xticks(df.index)\n",
    "    plt.ylabel(\"Top-k Recall (single-hit)\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.show()\n",
    "\n",
    "plot_receiver_eval([es_topk_df], [\"BM25\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dad395-10e4-4a28-a049-9351808c281c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d386560-7f7a-497b-988f-946f28cdde38",
   "metadata": {},
   "source": [
    "#### Now try out `DensePassageRetriever`\n",
    "\n",
    "> Dense Passage Retrieval is a retrieval method that calculates relevance using dense representations. Key features:\n",
    ">\n",
    "> * One BERT base model to encode documents\n",
    "> * One BERT base model to encode queries\n",
    "> * Ranking of Documents done by dot product similarity between query and document embeddings\n",
    ">\n",
    "> Indexing using DPR is comparatively expensive in terms of required computation since all documents in the database need to be processed through the transformer. In order to keep query times low, you should store these embeddings in a vector-optimized database such as FAISS or Milvus.\n",
    "\n",
    "\n",
    "\n",
    "You also might like:\n",
    "* read [Karpukhin, et al, \"Dense Passage Retrieval for Open-Domain Question Answering\", (2020)](https://arxiv.org/pdf/2004.04906.pdf)\n",
    "* view [Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 11 - Question Answering](https://www.youtube.com/watch?v=NcqfHa0_YmU&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=11) (guest presenter Danqi Chen is one of the authors on the above paper!)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7108d184-8c25-404f-9765-0caf2021042d",
   "metadata": {},
   "source": [
    "# code on page 195, Ch. 7 is outdated!\n",
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "\n",
    "dpr_retriever = DensePassageRetriever(\n",
    "    document_store=document_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b54a4-8a43-4c91-aa83-31e9532c9b29",
   "metadata": {},
   "source": [
    "> In Haystack, you can download the pre-trained encoders needed to start using DPR. For DPR, you need to provide two models - one for the query and one for the documents, however, _**the models must be trained on the same data**_. The easiest way to start is to go to Hugging Face and search for `dpr`. You'll get a list of DPR models sorted by Most Downloads, which means that the models at the top of the list are the most popular ones. Choose a `ctx_encoder` and a `question_encoder` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27bcd4-af7f-43c7-b6c3-beb6f1e55480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "\n",
    "dpr_retriever = DensePassageRetriever(\n",
    "    document_store=test_document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    embed_title=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea2c4e-9cca-4f3f-afee-96b2a072d89f",
   "metadata": {},
   "source": [
    "#### Update the embeddings used in `test_document_store`\n",
    "\n",
    "Now that we have an instance of `DensePassageRetriever`, we need to iterate over the documents in `test_document_store` and apply the encoders to update corresponding embedding representations.\n",
    "\n",
    "Since we originally set `return_embedding=True` when we initially created `test_document_store`, all we need is to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1694964-828c-42fe-b9b3-f9ae84843610",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document_store.update_embeddings(dpr_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430daf2-da0d-495d-a43e-6d953cd06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe2 = ExtractiveQAPipeline(\n",
    "\n",
    "    retriever=dpr_retriever, \n",
    "\n",
    "    reader=reader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011655e-2504-411d-9085-9d9e79d5d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_topk_df = evaluate_retriever(test_pipe2, eval_labels)\n",
    "print(dpr_topk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5a900-0a7a-4617-8352-cf906e807038",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_receiver_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f367a-6178-4090-a354-4865879030f0",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca544ea-9748-49b7-a699-2490f9772af1",
   "metadata": {},
   "source": [
    "### Evaluating the Reader\n",
    "\n",
    "The current API for Haystack does not seem to provide functions that work like the `compute_f1` and `compute_exact` mentioned in Evaluating the Reader, pages 196 and 197, Chapter 7."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4befd8c3-03c8-4110-9971-d8741feb83ab",
   "metadata": {},
   "source": [
    "# code on page 197, Ch. 7 is outdated!\n",
    "from farm.evaluation.squad_evaluation import compute_f1, compute_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130673f-7617-4e54-9eef-74d9e1d5269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.modeling.evaluation.squad import compute_f1, compute_exact\n",
    "\n",
    "pred = \"about 6000 hours\"\n",
    "label = \"6000 hours\"\n",
    "print(f\"EM: {compute_exact(label, pred)}\")\n",
    "print(f\"F1: {compute_f1(label, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0725989-35fa-4979-bfd1-ce8bf36e1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"about 6000 dollars\"\n",
    "print(f\"EM: {compute_exact(label, pred)}\")\n",
    "print(f\"F1: {compute_f1(label, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5bfdae-951a-44ef-b47a-08ca0b6c9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "def evaluate_reader(reader, eval_labels):\n",
    "    score_keys = ['exact_match', 'f1']\n",
    "    p = Pipeline()\n",
    "    p.add_node(component=reader, name=\"Reader\", inputs=[\"Query\"])\n",
    "\n",
    "    eval_result = p.eval(\n",
    "        labels=eval_labels,\n",
    "        documents= [[label.document for label in multilabel.labels] for multilabel in eval_labels],\n",
    "        params={},\n",
    "    )\n",
    "    metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
    "                \n",
    "    return {k:v for k,v in metrics[\"Reader\"].items() if k in score_keys}\n",
    "\n",
    "reader_eval = {}\n",
    "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff647d9-1d78-4972-b5b0-205ceadd44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reader_eval(reader_eval):\n",
    "    fig, ax = plt.subplots()\n",
    "    df = pd.DataFrame.from_dict(reader_eval).reindex([\"exact_match\", \"f1\"])\n",
    "    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
    "    ax.set_xticklabels([\"EM\", \"F1\"])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918b626-cb6e-41b3-b09a-c5bf07e21453",
   "metadata": {},
   "source": [
    "#### Metric Calculation from the Tutorial\n",
    "\n",
    "Let's also try running the code from the Calculating Evaluation Metrics section of the [Evaluation of a QA System tutorial](https://haystack.deepset.ai/tutorials/05_evaluation) for Haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a3d01-10a3-494d-bd6b-eabaac0d5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe3 = ExtractiveQAPipeline(\n",
    "    \n",
    "    reader=reader, \n",
    "    \n",
    "    retriever=dpr_retriever\n",
    ")\n",
    "\n",
    "\n",
    "eval_labels = test_document_store.get_all_labels_aggregated(\n",
    "    index=label_index,\n",
    "    open_domain=True,\n",
    "    aggregate_by_meta=[\"item_id\"],\n",
    "    drop_negative_labels=True,         # **** IMPORTANT: see Evaluation of an ExtractiveQAPipeline\n",
    "    drop_no_answers=True               # ****\n",
    ")\n",
    "\n",
    "\n",
    "eval_result = test_pipe3.eval(\n",
    "    \n",
    "    labels=eval_labels, \n",
    "    \n",
    "    params={\"Retriever\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4507d-a7cd-4c93-b0be-ccdfa0200e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = eval_result.calculate_metrics()\n",
    "\n",
    "print(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\n",
    "print(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e93d90-6709-4ac6-b43e-7fad01cc1fb5",
   "metadata": {},
   "source": [
    "#### Advanced Evaluation Metrics\n",
    "\n",
    "> As an advanced evaluation metric, semantic answer similarity (SAS) can be calculated. This metric takes into account whether the meaning of a predicted answer is similar to the annotated gold answer rather than just doing string comparison. To this end SAS relies on pre-trained models. For English, we recommend `cross-encoder/stsb-roberta-large`, whereas for German we recommend `deepset/gbert-large-sts`. A good multilingual model is `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2ddf7-de2b-4973-8da7-858705724315",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_eval_result = test_pipe3.eval(\n",
    "    \n",
    "    labels=eval_labels, \n",
    "    \n",
    "    params={\"Retriever\": {\"top_k\": 5}}, \n",
    "    \n",
    "    sas_model_name_or_path=\"cross-encoder/stsb-roberta-large\"\n",
    ")\n",
    "\n",
    "metrics = advanced_eval_result.calculate_metrics()\n",
    "\n",
    "print(f'Reader - SAS: {metrics[\"Reader\"][\"sas\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8ef73-2292-4e60-90ce-71812bd6c223",
   "metadata": {},
   "source": [
    "#### Isolated Evaluation Mode\n",
    "\n",
    "> The isolated node evaluation uses labels as input to the Reader node instead of the output of the preceeding Retriever node. Thereby, we can additionally calculate the upper bounds of the evaluation metrics of the Reader. Note that even with isolated evaluation enabled, integrated evaluation will still be running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf2026-a469-417f-b410-42bed085abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_with_upper_bounds = test_pipe3.eval(\n",
    "    \n",
    "    labels=eval_labels, \n",
    "    \n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 5}, \n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    },\n",
    "    \n",
    "    add_isolated_node_eval=True\n",
    ")\n",
    "\n",
    "test_pipe3.print_eval_report(eval_result_with_upper_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4525b-362a-40c5-8a71-9283e16dc52c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1d987-efc4-4355-afc7-20b21f20850e",
   "metadata": {},
   "source": [
    "### Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd0bc0-eea5-45cf-830a-b3ff6b64f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraphs(df):\n",
    "    paragraphs = []\n",
    "    \n",
    "    id2context = dict(\n",
    "        zip(df[\"review_id\"], df[\"context\"])\n",
    "    )\n",
    "\n",
    "    for review_id, review in id2context.items():\n",
    "        qas = []\n",
    "        \n",
    "        # filter for all question-answer pairs about a specific context\n",
    "        review_df = df.query(f\"review_id == '{review_id}'\")\n",
    "        \n",
    "        id2question = dict(\n",
    "            zip(review_df[\"id\"], review_df[\"question\"])\n",
    "        )\n",
    "        \n",
    "        # build up the qas array\n",
    "        for qid, question in id2question.items():\n",
    "            # filter for a single question ID\n",
    "            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
    "            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
    "            ans_text = question_df[\"answers.text\"][0].tolist()\n",
    "            # fill answerable questions\n",
    "            if len(ans_start_idxs):\n",
    "                answers = [\n",
    "                    {\"text\": text,\n",
    "                     \"answer_start\": answer_start}\n",
    "                    for text, answer_start in zip(ans_text, ans_start_idxs)\n",
    "                ]\n",
    "                is_impossible = False\n",
    "            else:\n",
    "                answers = []\n",
    "                is_impossible = True\n",
    "            # add question-answer pairs to qas\n",
    "            qas.append(\n",
    "                {\"question\": question,\n",
    "                 \"id\": qid,\n",
    "                 \"is_impossible\": is_impossible,\n",
    "                 \"answers\": answers}\n",
    "            )\n",
    "        # add context and question-answer pairs to paragraphs\n",
    "        paragraphs.append(\n",
    "            {\"qas\": qas, \"context\": review}\n",
    "        )\n",
    "    return paragraphs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4dcb05-a573-4b9d-abae-222c90dca58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = dfs[\"train\"].query(\"title == 'B00001P4ZH'\")\n",
    "#print(product)\n",
    "create_paragraphs(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b0730-79b2-4903-a972-4a7f9b01f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_squad(dfs):\n",
    "    for split, df in dfs.items():\n",
    "        subjqa_data = {}\n",
    "        # create 'paragraphs' for each product ID\n",
    "        groups = (\n",
    "            df.groupby(\"title\")\n",
    "              .apply(create_paragraphs)\n",
    "              .to_frame(name=\"paragraphs\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
    "\n",
    "        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"UTF-8\") as fout:\n",
    "            json.dump(subjqa_data, fout)\n",
    "\n",
    "convert_to_squad(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2253c-c5fb-4b0b-8981-d463ac548c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"electronics-train.json\"\n",
    "dev_filename = \"electronics-validation.json\"    # wtf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae179cd0-65b5-459a-b966-26ddd6dd9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.train(\n",
    "    data_dir=\".\",\n",
    "    use_gpu=True,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    train_filename=train_filename,\n",
    "    dev_filename=dev_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39cd73-4773-40bc-a60a-4d2ec9f76949",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader, eval_labels)\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ee9e9-c7dc-4fbd-a09b-14eee38dc5af",
   "metadata": {},
   "source": [
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546878b2-b8c5-4300-878a-16eeba84c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "minilm_reader = FARMReader(\n",
    "    model_name_or_path=minilm_ckpt,\n",
    "    progress_bar=False,\n",
    "    max_seq_len=max_seq_len,\n",
    "    doc_stride=doc_stride,\n",
    "    return_no_answer=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b0341-5fcf-4558-887d-2fdda663df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_reader.train(\n",
    "    data_dir=\".\",\n",
    "    use_gpu=True,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    train_filename=train_filename,\n",
    "    dev_filename=dev_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d0822-3196-4ba2-990f-d472bb90e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader, eval_labels)\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940da42-f7d5-44b7-86ee-6949452b826e",
   "metadata": {},
   "source": [
    "### Evaluating the Whole QA Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a554e-50be-47fe-a6ca-a4e9d92efc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f825e5-e5cd-4c7b-b180-aa40c47c6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252361c6-f4f2-41f9-8b6b-67bfd5b2603e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7118b14-13b7-44b2-9ba6-8e0161c0aa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa69300-5bf8-4234-bfc4-d94023f6f6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ccb49a-416b-4f63-b7f8-77e6eb90727a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553a35c-159b-4c58-aef0-15d71fa9724f",
   "metadata": {},
   "source": [
    "### Going Beyond Extractive QA\n",
    "\n",
    "> As of version 1.16, `RAGenerator` has been deprecated in Haystack and completely removed from Haystack as of v1.18. We recommend following the tutorial on [Creating a Generative QA Pipeline with Retrieval-Augmentation](https://haystack.deepset.ai/tutorials/22_pipeline_with_promptnode) instead. For more details about this deprecation, check out our announcement on Github.\n",
    "\n",
    "OK, let's take a detour for this chapter wrap-up, and learn about the current approach to creating a generative QA pipeline with Haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fac60f-e631-4c11-a58b-5d634a975998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fbb3c-ccfa-4d70-a63a-3c9171a2bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "\n",
    "document_store.write_documents(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c23f46-34b9-4599-b114-58f76aafa4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b77b7-b469-4b14-8df1-830d78c06201",
   "metadata": {},
   "source": [
    "#### Initializing the `PromptNode`\n",
    "\n",
    "Please see:\n",
    "* [`haystack.nodes.PromptTemplate`](https://docs.haystack.deepset.ai/reference/prompt-node-api#prompttemplate) ... is a template for the prompt you feed to the model to instruct it what to do (also see [PromptTemplates](https://docs.haystack.deepset.ai/docs/prompt_node#prompttemplates)).\n",
    "   * `prompt` contains the prompt for the task you want the model to do. It also specifies input variables: `document` and `query`. The variables are either primitives or lists of primitives. At runtime, these variables must be present in the execution context of the node. You can apply functions to those variables. ... combine the list of documents into a string by applying the `join` function. \n",
    "   * `output_parser` converts the output of the model to Haystack `Document`, `Answer`, or `Label` object. ... the ready-to-use `AnswerParser` which converts the output to the Haystack `Answer` object. \n",
    "* [`haystack.nodes.AnswerParser`](https://docs.haystack.deepset.ai/reference/prompt-node-api#answerparser) ... Parses the model output to extract the answer into a proper `Answer` object using regex patterns. `AnswerParser` adds the `document_id`s of the documents used to generate the answer and the prompts used to the `Answer` object. You can pass a `reference_pattern` to extract the `document_id`s of the answer from the model output.\n",
    "* [`haystack.nodes.PromptNode`](https://docs.haystack.deepset.ai/reference/prompt-node-api#promptnode) ... is the central abstraction in Haystack's large language model (LLM) support. PromptNode supports multiple NLP tasks out of the box. You can use it to perform tasks such as summarization, question answering, question generation, and more, using a single, unified model within the Haystack framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45cf966-4402-4e67-8ccf-e1a791af0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"google/flan-t5-large\", \n",
    "    default_prompt_template=rag_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590d1ee-4b00-4887-a6db-b65001651162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from haystack.pipelines import Pipeline\n",
    "\n",
    "pipe = Pipeline()\n",
    "\n",
    "pipe.add_node(\n",
    "    component=retriever, \n",
    "    name=\"retriever\", \n",
    "    inputs=[\"Query\"]\n",
    ")\n",
    "\n",
    "pipe.add_node(\n",
    "    component=prompt_node, \n",
    "    name=\"prompt_node\", \n",
    "    inputs=[\"retriever\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795ee40-6f45-49a2-807b-2826ada26386",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe.run(\n",
    "    query=\"What does Rhodes Statue look like?\"\n",
    ")\n",
    "\n",
    "print(output[\"answers\"][0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c6b9b-fedd-49b1-8c35-7e75ae463598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6c71a-4752-4ad7-8851-f4d72ca2139d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

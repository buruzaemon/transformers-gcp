{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7122dbae-412c-4c0b-a9dd-ca207f34cf6c",
   "metadata": {},
   "source": [
    "# Simple Training with the 🤗 Transformers Trainer\n",
    "\n",
    "Introduction to fine-tuning with 🤗 Transformers.\n",
    "\n",
    "This notebook takes bits and pieces from:\n",
    "\n",
    "* [Simple Training with the 🤗 Transformers Trainer](https://www.youtube.com/watch?v=u--UVvH-LIQ&t=132s) video with Lewis Tunstall & HuggingFace 🤗\n",
    "* Chapter 2: Text Classification, [Natural Language Processing with Transformers; Tunstall, von Werra, Wolf, May 2022](https://transformersbook.com/)\n",
    "* [Hugging Face Reading Group: Session 2](https://www.youtube.com/watch?v=vGwsyMrblxM&t=416s&ab_channel=HuggingFace) video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbce6a-e989-4b61-a666-8beb7691b983",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf15ea-2043-4393-89eb-0ad923340ce6",
   "metadata": {},
   "source": [
    "## Prepare for sharing a new model\n",
    "\n",
    "Open up an SSH shell, and log in to your vm instance.\n",
    "\n",
    "Activate your Python virtual environment.\n",
    "\n",
    "Using the `huggingface-cli` command-line utility, log in to HuggingFace and enter your write token.\n",
    "\n",
    "    # log in to Huggingface, and enter your write Role user access token\n",
    "    huggingface-cli login\n",
    "\n",
    "    # create a new Huggingface repo for this model\n",
    "    huggingface-cli repo create test-minilm-finetuned-emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b0b58-b5c9-4bbf-8aa7-e0ce2acfa12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810ac59-0971-42df-8a2a-940046b9d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ee3b8-9edd-40ec-89fd-019c5ef422c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f2d914-46ab-4836-a339-a3718dff9124",
   "metadata": {},
   "source": [
    "## Dataset: EDA & preparation\n",
    "\n",
    "In the Simple Training with the 🤗 Transformers Trainer video, the `emotions` dataset was being used, but apparently the original owners of the `emotions` dataset removed it early December of 2022. Please see [Emotion dataset cannot be downloaded #5342](https://github.com/huggingface/datasets/issues/5342) and [Data files are no longer accessible #5](https://github.com/dair-ai/emotion_dataset/issues/5) for more details.\n",
    "\n",
    "In its stead, the [SetFit team](https://huggingface.co/SetFit) (which include Lewis Tunstall amongst its members) have provided the [`SetFit/emotion`](https://huggingface.co/datasets/SetFit/emotion) dataset. This notebook has been tweaked for using `SetFit/emotion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1589123b-2fa6-40db-9c73-0d94056d46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--emotion-e444b7640ce3116e\n",
      "Found cached dataset json (/home/buruzaemon/.cache/huggingface/datasets/SetFit___json/SetFit--emotion-e444b7640ce3116e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce69c8c6832439abfea6af895d8dd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"SetFit/emotion\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f00c0b-c92c-4737-adef-1361e9e93254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'label_text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3974c78-fdeb-4f20-bfc2-5f2757173b81",
   "metadata": {},
   "source": [
    "What do the dataset items look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5959a78f-810c-4fb2-a32a-92defa7a3b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0, 'label_text': 'sadness'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c8d07-b78e-49cb-9f97-c6fee8bafd7d",
   "metadata": {},
   "source": [
    "Sometimes, it is easier to inspect a dataset as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d9f28c-b2a5-456a-84a8-d5ed6bc80034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_text\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.set_format(type=\"pandas\")\n",
    "train_df = emotions[\"train\"][:]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c91321e-4c43-4efd-979a-0d1bfa7b65be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAGxCAYAAABslcJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyiklEQVR4nO3de5yPdf7/8cdnzMkcHYehIaecqViFMis0OUW24yos1dpWJemgdhelSFvb4dvSpkVH2U1tR5JSLEnOMllkotISmRE5jLl+f3Tz+e0np5mKmeFxv92u287nfb2v9/W63vPJPPc6fD6hIAgCJEmSTnJRxV2AJElSSWAokiRJwlAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiqdSZNGkSoVDokMvQoUOLu7yT1rZt27j88stJS0sjFArRs2fPI/YvKCjg6aefpmPHjlSqVImYmBjS0tLo1q0br776KgUFBQDk5OQQCoWYNGnSsT8I6SQXXdwFSPpxJk6cSIMGDSLaqlWrVkzV6O677+all17i73//O3Xq1KFChQqH7bt792569uzJW2+9xeWXX864ceOoWrUqW7ZsYfr06VxyySW88MIL9OjR4zgegSRDkVRKNWnShJYtWxaq7759+wiFQkRH+5/8sbJy5Urq1KlD7969j9p3yJAhzJgxg8mTJ9OnT5+Idb169eKWW27hu+++O1alSjoML59JJ5jZs2cTCoV4+umnufnmm6levTpxcXGsXbsWgLfffpsOHTqQkpJCQkICbdu2ZdasWQeN8/rrr3P66acTFxdHrVq1+POf/8yIESMIhULhPke6tBMKhRgxYkRE25o1a/j1r39NWloacXFxNGzYkMcee+yQ9T///PPceeedVKtWjZSUFDp27Mjq1asP2s/06dPp0KEDqampJCQk0LBhQ0aPHh3R56OPPuLCCy+kQoUKxMfHc8YZZzB16tRCzee2bdu47rrrqF69OrGxsdSuXZs777yTPXv2RMzB22+/TXZ2dvhS5uzZsw853ldffcWECRPIyso6KBAdUK9ePZo1a3bYmtauXctvfvMb6tWrR0JCAtWrV6d79+6sWLEiol9BQQGjRo2ifv36lC1blnLlytGsWTMefvjhcJ8tW7Zw7bXXkpGRQVxcHJUrV6Zt27a8/fbbEWMV5n1T2LGkksr/2yiVUvv37yc/Pz+i7X/PBA0bNozWrVszfvx4oqKiSEtL45lnnqFPnz706NGDyZMnExMTw+OPP05WVhYzZsygQ4cOAMyaNYsePXrQunVrpkyZwv79+xk7diz//e9/f3S9q1atok2bNtSoUYMHHniAqlWrMmPGDG644Qa+/vprhg8fHtH/jjvuoG3btkyYMIG8vDxuu+02unfvTnZ2NmXKlAHgySef5JprriEzM5Px48eTlpbGf/7zH1auXBke59133+WCCy7grLPOYvz48aSmpjJlyhQuu+wydu3aRb9+/Q5b8+7du2nfvj3r1q1j5MiRNGvWjDlz5jB69GiWLl3K66+/Tnp6OvPnz+e6664jNzeXZ599FoBGjRodcsx3332Xffv2HfWeoyP58ssvqVixImPGjKFy5cps27aNyZMnc9ZZZ7FkyRLq168PwNixYxkxYgR/+MMfaNeuHfv27eOTTz5h+/bt4bGuuuoqFi9ezD333MNpp53G9u3bWbx4MVu3bg33Kez7pjBjSSVaIKlUmThxYgAcctm3b1/w7rvvBkDQrl27iO127twZVKhQIejevXtE+/79+4PmzZsHrVq1CredddZZQbVq1YLvvvsu3JaXlxdUqFAh+N9/NtavXx8AwcSJEw+qEwiGDx8efp2VlRWccsopQW5ubkS/QYMGBfHx8cG2bduCIAjC9Xfp0iWi39SpUwMgmD9/fhAEQbBjx44gJSUlOOecc4KCgoLDzleDBg2CM844I9i3b19Ee7du3YL09PRg//79h912/PjxARBMnTo1ov2+++4LgOCtt94Kt2VmZgaNGzc+7FgHjBkzJgCC6dOnH7VvEBx5jg/Iz88P9u7dG9SrVy+46aabwu3dunULTj/99COOn5SUFAwePPiw64vyvjnaWFJJ5+UzqZR66qmnWLhwYcTyv2eKfvWrX0X0nzdvHtu2baNv377k5+eHl4KCAi644AIWLlzIzp072blzJwsXLqRXr17Ex8eHt09OTqZ79+4/qtbdu3cza9YsLrroIhISEiL236VLF3bv3s0HH3wQsc2FF14Y8frA5aTPPvssfDx5eXlcd911EZf0/tfatWv55JNPwvf5/HC/mzZtOuQluQPeeecdEhMTufjiiyPaD5xdOtRlx+MhPz+fe++9l0aNGhEbG0t0dDSxsbGsWbOG7OzscL9WrVqxbNkyrrvuOmbMmEFeXt5BY7Vq1YpJkyYxatQoPvjgA/bt2xexvrDvm8KMJZV0Xj6TSqmGDRse8Ubr9PT0iNcHLn398A/8/9q2bRuhUIiCggKqVq160PpDtRXG1q1byc/P59FHH+XRRx89ZJ+vv/464nXFihUjXsfFxQGEb0DesmULAKeccsph93vgmIcOHXrYjyv44X5/WHfVqlUPCl1paWlER0f/qMtCNWrUAGD9+vVF3vaAIUOG8Nhjj3HbbbeRmZlJ+fLliYqK4uqrr464QXvYsGEkJibyzDPPMH78eMqUKUO7du247777wu+dF154gVGjRjFhwgT++Mc/kpSUxEUXXcTYsWOpWrVqod83iYmJRx1LKukMRdIJ6od/yCtVqgTAo48+ytlnn33IbapUqRJ+Uu2rr746aP0P2w6cSTpw0/EBPwwL5cuXp0yZMlx11VX8/ve/P+S+a9WqdYSjOVjlypUB+Pzzzw/b58AxDxs2jF69eh2yz4H7bw6lYsWKLFiwgCAIIuZz8+bN5Ofnh8cvivbt2xMTE8PLL7/MwIEDi7w9/P97fO69996I9q+//ppy5cqFX0dHRzNkyBCGDBnC9u3befvtt7njjjvIyspi48aNJCQkUKlSJR566CEeeughNmzYwCuvvMLtt9/O5s2bmT59eqHfN8BRx5JKOkORdJJo27Yt5cqVY9WqVQwaNOiw/WJjY2nVqhXTpk3j/vvvDwefHTt28Oqrr0b0rVKlCvHx8Sxfvjyi/V//+lfE64SEBNq3b8+SJUto1qwZsbGxP/l42rRpQ2pqKuPHj+fyyy8/5CW0+vXrU69ePZYtW3ZQgCiMDh06MHXqVF5++WUuuuiicPtTTz0VXl9UVatW5eqrr2bcuHE89dRTh3wCbd26dezcufOwT6CFQqHwmbMDXn/9db744gvq1q17yG3KlSvHxRdfzBdffMHgwYPJyck56GbwGjVqMGjQIGbNmsW///1voPDvmx861FhSSWcokk4SSUlJPProo/Tt25dt27Zx8cUXk5aWxpYtW1i2bBlbtmxh3LhxwPcfRHjBBRfQqVMnbr75Zvbv3899991HYmIi27ZtC48ZCoW48sorwx9Y2Lx5cz788EOee+65g/b/8MMPc84553Duuefyu9/9jlNPPZUdO3awdu1aXn31Vd55550iH88DDzzA1VdfTceOHbnmmmuoUqUKa9euZdmyZfzf//0fAI8//jidO3cmKyuLfv36Ub16dbZt20Z2djaLFy/mH//4x2H30adPHx577DH69u1LTk4OTZs2Ze7cudx777106dKFjh07FqnmAx588EE+/fRT+vXrx4wZM7jooouoUqUKX3/9NTNnzmTixIlMmTLlsKGoW7duTJo0iQYNGtCsWTMWLVrE/ffff9ClxO7du4c/z6py5cp89tlnPPTQQ9SsWZN69eqRm5tL+/bt+fWvf02DBg1ITk5m4cKFTJ8+PXxmrbDvm8KMJZV4xX2nt6SiOfD02cKFCw+5/sDTW//4xz8Ouf69994LunbtGlSoUCGIiYkJqlevHnTt2vWg/q+88krQrFmzIDY2NqhRo0YwZsyYYPjw4cEP/9nIzc0Nrr766qBKlSpBYmJi0L179yAnJ+egp8+C4Psnqfr37x9Ur149iImJCSpXrhy0adMmGDVq1FHrP9xTWG+88UaQmZkZJCYmBgkJCUGjRo2C++67L6LPsmXLgksvvTRIS0sLYmJigqpVqwbnnXdeMH78+EPO0f/aunVrMHDgwCA9PT2Ijo4OatasGQwbNizYvXt3RL/CPn12QH5+fjB58uTgvPPOCypUqBBER0cHlStXDjp37hw899xz4afiDnXc33zzTTBgwIAgLS0tSEhICM4555xgzpw5QWZmZpCZmRnu98ADDwRt2rQJKlWqFP49DhgwIMjJyQmCIAh2794dDBw4MGjWrFmQkpISlC1bNqhfv34wfPjwYOfOnRH1Hu19U5SxpJIqFARBUIyZTFIpMmLECEaOHIn/bEg6EflIviRJEoYiSZIkALx8JkmShGeKJEmSAEORJEkSYCiSJEkC/PDGIikoKODLL78kOTn5sF9AKUmSSpYgCNixYwfVqlUjKurw54MMRUXw5ZdfkpGRUdxlSJKkH2Hjxo1H/BJpQ1ERJCcnA99PakpKSjFXI0mSCiMvL4+MjIzw3/HDMRQVwYFLZikpKYYiSZJKmaPd+uKN1pIkSRiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkC/ELYH6XJ8BlExSUUdxmSJJ0wcsZ0Le4SPFMkSZIEhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiSglIeifv360bNnz+IuQ5IknQBK9XefPfzwwwRBUNxlSJKkE0CpDkWpqanFXYIkSTpBnDCXz/bs2cMNN9xAWloa8fHxnHPOOSxcuBCAIAioW7cuf/7znyO2X7lyJVFRUaxbt+6Q4+/Zs4e8vLyIRZIknZhKdSj6X7feeisvvvgikydPZvHixdStW5esrCy2bdtGKBSif//+TJw4MWKbv//975x77rnUqVPnkGOOHj2a1NTU8JKRkXE8DkWSJBWDEyIU7dy5k3HjxnH//ffTuXNnGjVqxBNPPEHZsmV58sknAfjNb37D6tWr+fDDDwHYt28fzzzzDP379z/suMOGDSM3Nze8bNy48bgcjyRJOv5OiFC0bt069u3bR9u2bcNtMTExtGrViuzsbADS09Pp2rUrf//73wF47bXX2L17N5dccslhx42LiyMlJSVikSRJJ6YTIhQdeAItFAod1P6/bVdffTVTpkzhu+++Y+LEiVx22WUkJCQc11olSVLJdEKEorp16xIbG8vcuXPDbfv27eOjjz6iYcOG4bYuXbqQmJjIuHHjePPNN4946UySJJ1cSvUj+QckJibyu9/9jltuuYUKFSpQo0YNxo4dy65duxgwYEC4X5kyZejXrx/Dhg2jbt26tG7duhirliRJJckJcaYIYMyYMfzqV7/iqquu4swzz2Tt2rXMmDGD8uXLR/QbMGAAe/fu9SyRJEmKUKrPFO3Zs4ekpCQA4uPjeeSRR3jkkUeOuM2mTZuIjo6mT58+x6NESZJUSpTKM0X5+fmsWrWK+fPn07hx40Jts2fPHtauXcsf//hHLr30UqpUqXKMq5QkSaVJqQxFK1eupGXLljRu3JiBAwcWapvnn3+e+vXrk5uby9ixY49xhZIkqbQJBX6jaqHl5eV9/8nWg6cSFeej/JIk/VxyxnQ9ZmMf+Pudm5t7xM8cLJVniiRJkn5uhiJJkiQMRZIkSYChSJIkCTAUSZIkAaX8wxuLy8qRWUe8e12SJJU+nimSJEnCUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJAEQXdwGlUZPhM4iKSyjuMiRJJ6icMV2Lu4STkmeKJEmSMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJKAGhKBQK8fLLLxd3GZIk6SRX7KFIkiSpJDAUSZIk8SNC0T//+U+aNm1K2bJlqVixIh07dmTnzp0sXLiQTp06UalSJVJTU8nMzGTx4sUR265Zs4Z27doRHx9Po0aNmDlzZsT6nJwcQqEQ06ZNo3379iQkJNC8eXPmz58f0W/evHm0a9eOsmXLkpGRwQ033MDOnTvD6//6179Sr1494uPjqVKlChdffPFR65ckSSe3IoWiTZs2ccUVV9C/f3+ys7OZPXs2vXr1IggCduzYQd++fZkzZw4ffPAB9erVo0uXLuzYsQOAgoICevXqRZkyZfjggw8YP348t9122yH3c+eddzJ06FCWLl3KaaedxhVXXEF+fj4AK1asICsri169erF8+XJeeOEF5s6dy6BBgwD46KOPuOGGG7jrrrtYvXo106dPp127dket/1D27NlDXl5exCJJkk5MoeBwieAQFi9eTIsWLcjJyaFmzZpH7Lt//37Kly/Pc889R7du3Xjrrbfo0qULOTk5nHLKKQBMnz6dzp0789JLL9GzZ09ycnKoVasWEyZMYMCAAQCsWrWKxo0bk52dTYMGDejTpw9ly5bl8ccfD+9r7ty5ZGZmsnPnTt544w1+85vf8Pnnn5OcnPyj6wcYMWIEI0eOPKg9Y/BUouISjrq9JEk/Rs6YrsVdwgklLy+P1NRUcnNzSUlJOWy/Ip0pat68OR06dKBp06ZccsklPPHEE3zzzTcAbN68mYEDB3LaaaeRmppKamoq3377LRs2bAAgOzubGjVqhAMRQOvWrQ+5n2bNmoV/Tk9PD48PsGjRIiZNmkRSUlJ4ycrKoqCggPXr19OpUydq1qxJ7dq1ueqqq3j22WfZtWvXUes/lGHDhpGbmxteNm7cWJTpkiRJpUiRQlGZMmWYOXMmb775Jo0aNeLRRx+lfv36rF+/nn79+rFo0SIeeugh5s2bx9KlS6lYsSJ79+4FOOQlqlAodMj9xMTEHNSnoKAg/L+//e1vWbp0aXhZtmwZa9asoU6dOiQnJ7N48WKef/550tPT+dOf/kTz5s3Zvn37Ees/lLi4OFJSUiIWSZJ0YiryjdahUIi2bdsycuRIlixZQmxsLC+99BJz5szhhhtuoEuXLjRu3Ji4uDi+/vrr8HaNGjViw4YNfPnll+G2H95AXRhnnnkmH3/8MXXr1j1oiY2NBSA6OpqOHTsyduxYli9fTk5ODu+8884R65ckSSe36KJ0XrBgAbNmzeL8888nLS2NBQsWsGXLFho2bEjdunV5+umnadmyJXl5edxyyy2ULVs2vG3Hjh2pX78+ffr04YEHHiAvL48777yzyAXfdtttnH322fz+97/nmmuuITExkezsbGbOnMmjjz7Ka6+9xqeffkq7du0oX748b7zxBgUFBdSvX/+I9UuSpJNbkUJRSkoK77//Pg899BB5eXnUrFmTBx54gM6dO1O1alWuvfZazjjjDGrUqMG9997L0KFDw9tGRUXx0ksvMWDAAFq1asWpp57KI488wgUXXFCkgps1a8Z7773HnXfeybnnnksQBNSpU4fLLrsMgHLlyjFt2jRGjBjB7t27qVevHs8//3z4Zu3D1S9Jkk5uRXr67GR34O51nz6TJB1LPn328zomT59JkiSdqAxFkiRJGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAUX8RGt9b+XILL8cVpKkE4xniiRJkjAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRIA0cVdQGnUZPgMouISirsM6bjJGdO1uEuQpGPOM0WSJEkYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFAOzbt6+4S5AkScXsuIai6dOnc84551CuXDkqVqxIt27dWLduHQA5OTmEQiGmTZtG+/btSUhIoHnz5syfPz9ijCeeeIKMjAwSEhK46KKLePDBBylXrlxEn1dffZUWLVoQHx9P7dq1GTlyJPn5+eH1oVCI8ePH06NHDxITExk1atQxP3ZJklSyHddQtHPnToYMGcLChQuZNWsWUVFRXHTRRRQUFIT73HnnnQwdOpSlS5dy2mmnccUVV4QDzb///W8GDhzIjTfeyNKlS+nUqRP33HNPxD5mzJjBlVdeyQ033MCqVat4/PHHmTRp0kH9hg8fTo8ePVixYgX9+/c/ZL179uwhLy8vYpEkSSemUBAEQXHtfMuWLaSlpbFixQqSkpKoVasWEyZMYMCAAQCsWrWKxo0bk52dTYMGDbj88sv59ttvee2118JjXHnllbz22mts374dgHbt2tG5c2eGDRsW7vPMM89w66238uWXXwLfnykaPHgwf/nLX45Y34gRIxg5cuRB7RmDpxIVl/BTD18qNXLGdC3uEiTpR8vLyyM1NZXc3FxSUlIO2++4nilat24dv/71r6lduzYpKSnUqlULgA0bNoT7NGvWLPxzeno6AJs3bwZg9erVtGrVKmLMH75etGgRd911F0lJSeHlmmuuYdOmTezatSvcr2XLlketd9iwYeTm5oaXjRs3FvGIJUlSaRF9PHfWvXt3MjIyeOKJJ6hWrRoFBQU0adKEvXv3hvvExMSEfw6FQgDhy2tBEITbDvjhia6CggJGjhxJr169Dtp/fHx8+OfExMSj1hsXF0dcXFwhjkySJJV2xy0Ubd26lezsbB5//HHOPfdcAObOnVukMRo0aMCHH34Y0fbRRx9FvD7zzDNZvXo1devW/WkFS5Kkk8pxC0Xly5enYsWK/O1vfyM9PZ0NGzZw++23F2mM66+/nnbt2vHggw/SvXt33nnnHd58882Is0d/+tOf6NatGxkZGVxyySVERUWxfPlyVqxY4VNmkiTpsI7bPUVRUVFMmTKFRYsW0aRJE2666Sbuv//+Io3Rtm1bxo8fz4MPPkjz5s2ZPn06N910U8RlsaysLF577TVmzpzJL37xC84++2wefPBBatas+XMfkiRJOoEU69NnP4drrrmGTz75hDlz5hzzfR24e92nz3Sy8ekzSaVZYZ8+O643Wv8c/vznP9OpUycSExN58803mTx5Mn/961+LuyxJklTKlbpQ9OGHHzJ27Fh27NhB7dq1eeSRR7j66quLuyxJklTKlbpQNHXq1OIuQZIknYD8QlhJkiQMRZIkSYChSJIkCTAUSZIkAYYiSZIkoBQ+fVYSrByZdcQPf5IkSaWPZ4okSZIwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSANHFXUBp1GT4DKLiEoq7DOmwcsZ0Le4SJKnU8UyRJEkShiJJkiTAUCRJkgQYiiRJkgBDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBpSAUBUHAtddeS4UKFQiFQixdurS4S5IkSSegEv81H9OnT2fSpEnMnj2b2rVrU6lSpeIuSZIknYBKfChat24d6enptGnT5pjtY+/evcTGxh6z8SVJUslXoi+f9evXj+uvv54NGzYQCoU49dRTCYKAsWPHUrt2bcqWLUvz5s355z//Gd5m//79DBgwgFq1alG2bFnq16/Pww8/fNC4PXv2ZPTo0VSrVo3TTjvteB+aJEkqYUr0maKHH36YOnXq8Le//Y2FCxdSpkwZ/vCHPzBt2jTGjRtHvXr1eP/997nyyiupXLkymZmZFBQUcMoppzB16lQqVarEvHnzuPbaa0lPT+fSSy8Njz1r1ixSUlKYOXMmQRAccv979uxhz5494dd5eXnH/JglSVLxKNGhKDU1leTkZMqUKUPVqlXZuXMnDz74IO+88w6tW7cGoHbt2sydO5fHH3+czMxMYmJiGDlyZHiMWrVqMW/ePKZOnRoRihITE5kwYcIRL5uNHj06YixJknTiKtGh6IdWrVrF7t276dSpU0T73r17OeOMM8Kvx48fz4QJE/jss8/47rvv2Lt3L6effnrENk2bNj3qfUTDhg1jyJAh4dd5eXlkZGT89AORJEklTqkKRQUFBQC8/vrrVK9ePWJdXFwcAFOnTuWmm27igQceoHXr1iQnJ3P//fezYMGCiP6JiYlH3V9cXFx4XEmSdGIrVaGoUaNGxMXFsWHDBjIzMw/ZZ86cObRp04brrrsu3LZu3brjVaIkSSqlSlUoSk5OZujQodx0000UFBRwzjnnkJeXx7x580hKSqJv377UrVuXp556ihkzZlCrVi2efvppFi5cSK1atYq7fEmSVIKVqlAEcPfdd5OWlsbo0aP59NNPKVeuHGeeeSZ33HEHAAMHDmTp0qVcdtllhEIhrrjiCq677jrefPPNYq5ckiSVZKHgcM+j6yB5eXmkpqaSMXgqUXEJxV2OdFg5Y7oWdwmSVGIc+Pudm5tLSkrKYfuV6A9vlCRJOl4MRZIkSRiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJKAUfs1HSbByZNYRPxFTkiSVPp4pkiRJwlAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIgurgLKI2aDJ9BVFxCcZehnyhnTNfiLkGSVIJ4pkiSJAlDkSRJEmAokiRJAgxFkiRJgKFIkiQJMBRJkiQBhiJJkiTAUCRJkgQYiiRJkgBDkSRJElCCQ9Evf/lLBg8eXNxlSJKkk0SJDUWSJEnHk6FIkiSJUhKKvvnmG/r06UP58uVJSEigc+fOrFmzBoDc3FzKli3L9OnTI7aZNm0aiYmJfPvttwB88cUXXHbZZZQvX56KFSvSo0cPcnJyjvehSJKkEqpUhKJ+/frx0Ucf8corrzB//nyCIKBLly7s27eP1NRUunbtyrPPPhuxzXPPPUePHj1ISkpi165dtG/fnqSkJN5//33mzp1LUlISF1xwAXv37j3sfvfs2UNeXl7EIkmSTkwlPhStWbOGV155hQkTJnDuuefSvHlznn32Wb744gtefvllAHr37s3LL7/Mrl27AMjLy+P111/nyiuvBGDKlClERUUxYcIEmjZtSsOGDZk4cSIbNmxg9uzZh9336NGjSU1NDS8ZGRnH+nAlSVIxKfGhKDs7m+joaM4666xwW8WKFalfvz7Z2dkAdO3alejoaF555RUAXnzxRZKTkzn//PMBWLRoEWvXriU5OZmkpCSSkpKoUKECu3fvZt26dYfd97Bhw8jNzQ0vGzduPIZHKkmSilN0cRdwNEEQHLY9FAoBEBsby8UXX8xzzz3H5ZdfznPPPcdll11GdPT3h1dQUECLFi0OusQGULly5cPuOy4ujri4uJ/hKCRJUklX4kNRo0aNyM/PZ8GCBbRp0waArVu38p///IeGDRuG+/Xu3Zvzzz+fjz/+mHfffZe77747vO7MM8/khRdeIC0tjZSUlON+DJIkqeQr8ZfP6tWrR48ePbjmmmuYO3cuy5Yt48orr6R69er06NEj3C8zM5MqVarQu3dvTj31VM4+++zwut69e1OpUiV69OjBnDlzWL9+Pe+99x433ngjn3/+eXEcliRJKmFKfCgCmDhxIi1atKBbt260bt2aIAh44403iImJCfcJhUJcccUVLFu2jN69e0dsn5CQwPvvv0+NGjXo1asXDRs2pH///nz33XeeOZIkSQCEgsPdtKOD5OXlff8U2uCpRMUlFHc5+olyxnQt7hIkScfBgb/fubm5RzwZUirOFEmSJB1rhiJJkiQMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZKAUvDdZyXRypFZfhK2JEknGM8USZIkYSiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJACii7uA0qjJ8BlExSUUdxk/Sc6YrsVdgiRJJYpniiRJkjAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAaU8FI0YMYLTTz+9uMuQJEkngFIdioYOHcqsWbOKuwxJknQCKNYvhN27dy+xsbFF3i4IAvbv309SUhJJSUnHoDJJknSyKfKZon/+8580bdqUsmXLUrFiRTp27MjOnTv55S9/yeDBgyP69uzZk379+oVfn3rqqYwaNYp+/fqRmprKNddcQ05ODqFQiClTptCmTRvi4+Np3Lgxs2fPDm83e/ZsQqEQM2bMoGXLlsTFxTFnzpyDLp/Nnj2bVq1akZiYSLly5Wjbti2fffZZeP2rr75KixYtiI+Pp3bt2owcOZL8/PzDHuuePXvIy8uLWCRJ0ompSKFo06ZNXHHFFfTv35/s7Gxmz55Nr169CIKg0GPcf//9NGnShEWLFvHHP/4x3H7LLbdw8803s2TJEtq0acOFF17I1q1bI7a99dZbGT16NNnZ2TRr1ixiXX5+Pj179iQzM5Ply5czf/58rr32WkKhEAAzZszgyiuv5IYbbmDVqlU8/vjjTJo0iXvuueewtY4ePZrU1NTwkpGRUejjlCRJpUuRLp9t2rSJ/Px8evXqRc2aNQFo2rRpkXZ43nnnMXTo0PDrnJwcAAYNGsSvfvUrAMaNG8f06dN58sknufXWW8N977rrLjp16nTIcfPy8sjNzaVbt27UqVMHgIYNG4bX33PPPdx+++307dsXgNq1a3P33Xdz6623Mnz48EOOOWzYMIYMGRKxD4ORJEknpiKFoubNm9OhQweaNm1KVlYW559/PhdffDHly5cv9BgtW7Y8ZHvr1q3/f1HR0bRs2ZLs7OxCbQtQoUIF+vXrR1ZWFp06daJjx45ceumlpKenA7Bo0SIWLlwYcWZo//797N69m127dpGQkHDQmHFxccTFxRX62CRJUulVpMtnZcqUYebMmbz55ps0atSIRx99lPr167N+/XqioqIOuoy2b9++g8ZITEws9P4OXPoq7LYTJ05k/vz5tGnThhdeeIHTTjuNDz74AICCggJGjhzJ0qVLw8uKFStYs2YN8fHxha5JkiSdmIp8o3UoFKJt27aMHDmSJUuWEBsby0svvUTlypXZtGlTuN/+/ftZuXJlocc9EF7g+/uDFi1aRIMGDYpaHmeccQbDhg1j3rx5NGnShOeeew6AM888k9WrV1O3bt2DlqioUv3JBJIk6WdQpMtnCxYsYNasWZx//vmkpaWxYMECtmzZQsOGDUlMTGTIkCG8/vrr1KlTh7/85S9s37690GM/9thj1KtXj4YNG/KXv/yFb775hv79+xd6+/Xr1/O3v/2NCy+8kGrVqrF69Wr+85//0KdPHwD+9Kc/0a1bNzIyMrjkkkuIiopi+fLlrFixglGjRhVlGiRJ0gmoSKEoJSWF999/n4ceeoi8vDxq1qzJAw88QOfOndm3bx/Lli2jT58+REdHc9NNN9G+fftCjz1mzBjuu+8+lixZQp06dfjXv/5FpUqVCr19QkICn3zyCZMnT2br1q2kp6czaNAgfvvb3wKQlZXFa6+9xl133cXYsWOJiYmhQYMGXH311UWZAkmSdIIKBUV5nv4YyMnJoVatWixZsqTEf2VHXl7e94/mD55KVNzBN2aXJjljuhZ3CZIkHRcH/n7n5uaSkpJy2H7eTCNJkoShSJIkCSjm7z6D77/6o5iv4EmSJHmmSJIkCQxFkiRJgKFIkiQJMBRJkiQBhiJJkiSgBDx9VhqtHJl1xA9/kiRJpY9niiRJkjAUSZIkAYYiSZIkwFAkSZIEGIokSZIAQ5EkSRJgKJIkSQIMRZIkSYChSJIkCTAUSZIkAYYiSZIkwFAkSZIEGIokSZIAiC7uAkqTIAgAyMvLK+ZKJElSYR34u33g7/jhGIqKYOvWrQBkZGQUcyWSJKmoduzYQWpq6mHXG4qKoEKFCgBs2LDhiJOqHycvL4+MjAw2btxISkpKcZdzwnF+jy3n99hyfo+tE31+gyBgx44dVKtW7Yj9DEVFEBX1/S1YqampJ+SbpqRISUlxfo8h5/fYcn6PLef32DqR57cwJzO80VqSJAlDkSRJEmAoKpK4uDiGDx9OXFxccZdyQnJ+jy3n99hyfo8t5/fYcn6/FwqO9nyaJEnSScAzRZIkSRiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoajQ/vrXv1KrVi3i4+Np0aIFc+bMKe6SSqT333+f7t27U61aNUKhEC+//HLE+iAIGDFiBNWqVaNs2bL88pe/5OOPP47os2fPHq6//noqVapEYmIiF154IZ9//nlEn2+++YarrrqK1NRUUlNTueqqq9i+ffsxPrriNXr0aH7xi1+QnJxMWloaPXv2ZPXq1RF9nN8fb9y4cTRr1iz8ib6tW7fmzTffDK93bn9eo0ePJhQKMXjw4HCbc/zTjBgxglAoFLFUrVo1vN75LYRARzVlypQgJiYmeOKJJ4JVq1YFN954Y5CYmBh89tlnxV1aifPGG28Ed955Z/Diiy8GQPDSSy9FrB8zZkyQnJwcvPjii8GKFSuCyy67LEhPTw/y8vLCfQYOHBhUr149mDlzZrB48eKgffv2QfPmzYP8/PxwnwsuuCBo0qRJMG/evGDevHlBkyZNgm7duh2vwywWWVlZwcSJE4OVK1cGS5cuDbp27RrUqFEj+Pbbb8N9nN8f75VXXglef/31YPXq1cHq1auDO+64I4iJiQlWrlwZBIFz+3P68MMPg1NPPTVo1qxZcOONN4bbneOfZvjw4UHjxo2DTZs2hZfNmzeH1zu/R2coKoRWrVoFAwcOjGhr0KBBcPvttxdTRaXDD0NRQUFBULVq1WDMmDHhtt27dwepqanB+PHjgyAIgu3btwcxMTHBlClTwn2++OKLICoqKpg+fXoQBEGwatWqAAg++OCDcJ/58+cHQPDJJ58c46MqOTZv3hwAwXvvvRcEgfN7LJQvXz6YMGGCc/sz2rFjR1CvXr1g5syZQWZmZjgUOcc/3fDhw4PmzZsfcp3zWzhePjuKvXv3smjRIs4///yI9vPPP5958+YVU1Wl0/r16/nqq68i5jIuLo7MzMzwXC5atIh9+/ZF9KlWrRpNmjQJ95k/fz6pqamcddZZ4T5nn302qampJ9XvJDc3F4AKFSoAzu/Paf/+/UyZMoWdO3fSunVr5/Zn9Pvf/56uXbvSsWPHiHbn+OexZs0aqlWrRq1atbj88sv59NNPAee3sKKLu4CS7uuvv2b//v1UqVIlor1KlSp89dVXxVRV6XRgvg41l5999lm4T2xsLOXLlz+oz4Htv/rqK9LS0g4aPy0t7aT5nQRBwJAhQzjnnHNo0qQJ4Pz+HFasWEHr1q3ZvXs3SUlJvPTSSzRq1Cj8j71z+9NMmTKFxYsXs3DhwoPW+f796c466yyeeuopTjvtNP773/8yatQo2rRpw8cff+z8FpKhqJBCoVDE6yAIDmpT4fyYufxhn0P1P5l+J4MGDWL58uXMnTv3oHXO749Xv359li5dyvbt23nxxRfp27cv7733Xni9c/vjbdy4kRtvvJG33nqL+Pj4w/Zzjn+8zp07h39u2rQprVu3pk6dOkyePJmzzz4bcH6PxstnR1GpUiXKlClzUALevHnzQYlbR3bgKYgjzWXVqlXZu3cv33zzzRH7/Pe//z1o/C1btpwUv5Prr7+eV155hXfffZdTTjkl3O78/nSxsbHUrVuXli1bMnr0aJo3b87DDz/s3P4MFi1axObNm2nRogXR0dFER0fz3nvv8cgjjxAdHR0+fuf455OYmEjTpk1Zs2aN7+FCMhQdRWxsLC1atGDmzJkR7TNnzqRNmzbFVFXpVKtWLapWrRoxl3v37uW9994Lz2WLFi2IiYmJ6LNp0yZWrlwZ7tO6dWtyc3P58MMPw30WLFhAbm7uCf07CYKAQYMGMW3aNN555x1q1aoVsd75/fkFQcCePXuc259Bhw4dWLFiBUuXLg0vLVu2pHfv3ixdupTatWs7xz+zPXv2kJ2dTXp6uu/hwjrON3aXSgceyX/yySeDVatWBYMHDw4SExODnJyc4i6txNmxY0ewZMmSYMmSJQEQPPjgg8GSJUvCH18wZsyYIDU1NZg2bVqwYsWK4IorrjjkI6GnnHJK8PbbbweLFy8OzjvvvEM+EtqsWbNg/vz5wfz584OmTZueMI+EHs7vfve7IDU1NZg9e3bEI7e7du0K93F+f7xhw4YF77//frB+/fpg+fLlwR133BFERUUFb731VhAEzu2x8L9PnwWBc/xT3XzzzcHs2bODTz/9NPjggw+Cbt26BcnJyeG/Vc7v0RmKCumxxx4LatasGcTGxgZnnnlm+DFoRXr33XcD4KClb9++QRB8/1jo8OHDg6pVqwZxcXFBu3btghUrVkSM8d133wWDBg0KKlSoEJQtWzbo1q1bsGHDhog+W7duDXr37h0kJycHycnJQe/evYNvvvnmOB1l8TjUvALBxIkTw32c3x+vf//+4f/GK1euHHTo0CEciILAuT0WfhiKnOOf5sDnDsXExATVqlULevXqFXz88cfh9c7v0YWCIAiK5xyVJElSyeE9RZIkSRiKJEmSAEORJEkSYCiSJEkCDEWSJEmAoUiSJAkwFEmSJAGGIkmSJMBQJEmSBBiKJEmSAEORJEkSAP8PNt4iAZRIN9AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df[\"label_text\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequence of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea376ee8-c042-4ecf-94c7-71adb1ed80c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "train_df[\"words_per_tweet\"] = train_df[\"text\"].str.split().apply(len)\n",
    "train_df.boxplot(\n",
    "    \"words_per_tweet\",\n",
    "    by=\"label_text\",\n",
    "    grid=False,\n",
    "    showfliers=False,\n",
    "    color=\"black\"\n",
    ")\n",
    "plt.title(\"Words Per Tweet\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01ee0e-4cfc-4193-b785-28566a37e5cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a76cb6-ae0b-4f9c-8e87-817d168c07d6",
   "metadata": {},
   "source": [
    "Notice that the frequency of class \"5\" for \"surprise\" is not well represented within the dataset.\n",
    "\n",
    "_How do we deal with this?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65f1e5-e01e-4651-a5fa-ee2b51c3f784",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bd31f-15a8-43ea-9eb3-d2663f8e2822",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7506190-2305-4c12-914c-6575b487a2aa",
   "metadata": {},
   "source": [
    "Now, when training a classfier using a dataset that may suffer for imbalanced classes, one way to deal with the situation is to up-sample from the imbalanced class(es) to offset the difference(s). However, Transformer models are good at memorizing patterns, so this might not be a good idea in our case.\n",
    "\n",
    "What we can do is this: correct for the class imbalance _within the loss function during training_.\n",
    "\n",
    "From the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#trainer) documentation:\n",
    "\n",
    "> To inject custom behavior you can subclass them and override the following methods: ...\n",
    "> * `compute_loss` - Computes the loss on a batch of training inputs.\n",
    "\n",
    "\n",
    "From PyTorch docs on [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "> If provided, <span style=\"background-color: #9AFEFF\">the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5438426-6ae0-4cff-b5fa-11c704374868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 'sadness', 3, 'anger', 2, 'love', 5, 'surprise', 4, 'fear', 1,\n",
       "       'joy'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kvs = pd.unique(train_df[['label', 'label_text']].values.ravel('C'))\n",
    "kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d1f193-10da-4387-acf9-6e663b0fef86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sadness', 3: 'anger', 2: 'love', 5: 'surprise', 4: 'fear', 1: 'joy'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = dict(zip(kvs[0::2], kvs[1::2]))\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd2df2e-4bbb-47b7-9f50-007c67936299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'anger': 3, 'love': 2, 'surprise': 5, 'fear': 4, 'joy': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k:v for v,k in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326e27b6-214a-4696-bbc0-d36b446e9f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(id2label)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fe8144-2157-4ccd-a9bc-978cde1df2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        0.33513\n",
      "sadness    0.29163\n",
      "anger      0.13494\n",
      "fear       0.12106\n",
      "love       0.08150\n",
      "surprise   0.03575\n"
     ]
    }
   ],
   "source": [
    "class2prob = train_df[\"label\"].value_counts(normalize=True)\n",
    "for i,v in class2prob.items():\n",
    "    print(f\"{id2label[i]:<10} {v:.5F}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4813de4-0f4e-491a-acd6-4aa6b9d6ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.708375  0.664875  0.9185    0.8650625 0.8789375 0.96425  ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "class_weights = (1 - (train_df[\"label\"].value_counts().sort_index() / len(train_df))).values\n",
    "print(class_weights)\n",
    "print(type(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca1bb2-b863-4476-b170-6f767625d66c",
   "metadata": {},
   "source": [
    "Seriously, wtf?\n",
    "\n",
    "If we don't expressly put `class_weights` on the GPU, like if we left it on the CPU, we would see this:\n",
    "\n",
    "\n",
    "    RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_nll_loss_forward)\n",
    "\n",
    "Not completely unhelpful, but still somewhat cryptic. You might like to read [CUDA SEMANTICS in the PyTorch documentation](https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f691f6f-a3c8-4cea-bb52-2b3654a9a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
    "\n",
    "# THIS example leaves class_weights on the CPU..\n",
    "#class_weights = torch.from_numpy(class_weights).float()\n",
    "\n",
    "class_weights\n",
    "print(type(class_weights))\n",
    "print(class_weights.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43fb58e9-c2e5-4986-bf9c-bef82768434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # feed inputs to model and extract logits\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # extract labels; not label!\n",
    "        #         ^^^^^\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # define loss function with class weights\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_func(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbf195-2bff-4f3c-9b70-8d5c9d1755a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a07caef-d978-4744-8db5-13ef7cb2e45c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc8deb-b1d0-4a9b-bbaf-fb9b3b5516fb",
   "metadata": {},
   "source": [
    "After wrapping up the EDA with Pandas DataFrames, be sure to reset the Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e5e733-bf88-4153-87ec-f516dc2717af",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14aabd9-1be9-4911-b0ba-c69bca3c3e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db50228c-2ee4-4ea0-86af-d6805ffb2ff3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6bfec-f26a-4759-8748-7e9c87708166",
   "metadata": {},
   "source": [
    "## Pre-trained model: MiniLM\n",
    "\n",
    "[microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased?text=I+like+you.+I+love+you) on Hugging Face modelhub.\n",
    "\n",
    "> Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n",
    "\n",
    "Read the [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-trained Transformers](https://arxiv.org/abs/2002.10957) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678037c2-951b-46f5-8cbb-65916ab604b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/MiniLM-L12-H384-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40decedf-6299-4ecf-b7ad-b3eb9b7260f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd9d3b2-0523-4fd4-9aa8-4ade67bda2c7",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287efe29-91c4-48a2-92e3-bd52dbf2a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d8c27-fa3b-4e95-8cfa-a3e04e9d174e",
   "metadata": {},
   "source": [
    "Let's see how this particular tokenizer works with the example text \"Tokenizing text is a core task of NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cfc2c2-75d6-406c-a214-0cb173dae20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "ex_text = \"Tokenizing text is a core task of NLP.\"\n",
    "\n",
    "encoded_text = tokenizer(ex_text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959229f-6d97-4d9d-adeb-972f1fdf6cab",
   "metadata": {},
   "source": [
    "`input_ids`\n",
    "\n",
    "> Index ids of the word embeddings corresponding to the given word.\n",
    "\n",
    "`token_type_ids`\n",
    "\n",
    "> ... a binary mask identifying the two types of sequence in the model ...\n",
    "> ... the “context” used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a 1...\n",
    "> Some models, like XLNetModel use an additional token represented by a 2.\n",
    "\n",
    "`attention_mask`\n",
    "\n",
    "> Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be386ab9-ef45-4c02-a1b9-2eb154f516c4",
   "metadata": {},
   "source": [
    "OK, so, what exactly are these _tokens_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d53af31-d92a-4873-99dd-789ce974e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "527701fb-0be7-4381-a050-237089217862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102dff79-7446-4b05-abe6-a69b43d1fd92",
   "metadata": {},
   "source": [
    "And, diving in just a little bit deeper...\n",
    "\n",
    "The Tokenizer is actually a pipeline with:\n",
    "\n",
    "1. Normalization\n",
    "1. Pre-tokenization\n",
    "1. Model (?)\n",
    "1. Postprocessor\n",
    "\n",
    "Let's have a look at these ... in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a222718-b639-416c-ace6-f48c896d0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb9ada5d-e6e0-4b6d-a3b5-e0a398580edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tokenizer.backend_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a5e567-3cdc-4232-a588-45a2f809fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original input sentence: 'Tokenizing text is a core task of NLP.'\n"
     ]
    }
   ],
   "source": [
    "sentence = ex_text\n",
    "\n",
    "print(f\"The original input sentence: '{sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80d627fe-7878-4a8a-a37a-c762c49dc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization: 'tokenizing text is a core task of nlp.'\n"
     ]
    }
   ],
   "source": [
    "sentence = tokenizer.backend_tokenizer.normalizer.normalize_str(sentence)\n",
    "\n",
    "print(f\"Normalization: '{sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799e82da-e423-4793-a33b-2a0254845aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tokenizing', (0, 10)),\n",
       " ('text', (11, 15)),\n",
       " ('is', (16, 18)),\n",
       " ('a', (19, 20)),\n",
       " ('core', (21, 25)),\n",
       " ('task', (26, 30)),\n",
       " ('of', (31, 33)),\n",
       " ('nlp', (34, 37)),\n",
       " ('.', (37, 38))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1058694d-1a6f-4cc4-a287-efff746a5330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'token',\n",
       " '##izing',\n",
       " 'text',\n",
       " 'is',\n",
       " 'a',\n",
       " 'core',\n",
       " 'task',\n",
       " 'of',\n",
       " 'nl',\n",
       " '##p',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentence)[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77536f-6388-4395-8b5f-4a028a2910a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b5f48-ab7e-4eef-bd72-fbf0e22536bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5933dc8a-ed17-4419-8915-08f4449af8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(emotions[\"train\"][\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b634bf60-590a-4c27-a932-b839aee31cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0403f44-8d00-459d-931a-13098cac7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/buruzaemon/.cache/huggingface/datasets/SetFit___json/SetFit--emotion-e444b7640ce3116e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6336669b1e9f9c00.arrow\n",
      "Loading cached processed dataset at /home/buruzaemon/.cache/huggingface/datasets/SetFit___json/SetFit--emotion-e444b7640ce3116e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-beb5484d00294ab2.arrow\n",
      "Loading cached processed dataset at /home/buruzaemon/.cache/huggingface/datasets/SetFit___json/SetFit--emotion-e444b7640ce3116e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ca5830f5be51e028.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize_text, batched=True, batch_size=None)\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e08593b-69bb-41da-b80a-9206d240ec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'label',\n",
       " 'label_text',\n",
       " 'input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c07703f-9b76-46d5-aa46-54a3319d73ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'label_text']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d726a20-ea5e-4263-88ec-2329e8c8a1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e62d901-237e-47b4-8443-b5571185811f",
   "metadata": {},
   "source": [
    "From the API docs on Trainer:\n",
    "\n",
    "> The Trainer class is optimized for 🤗 Transformers models and can have surprising behaviors when you use it on other models. When using it on your own model, make sure:\n",
    ">\n",
    "> * your model always return tuples or subclasses of ModelOutput.\n",
    "> * your model can compute the loss if a labels argument is provided and that loss is returned as the first element of the tuple (if your model returns tuples)\n",
    "> * your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer) but none of them should be named \"label\".\n",
    "\n",
    "So, <span style=\"background-color: #9AFEFF\">we need to rename that column for `label` in our dataset to `labels`</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "687ed14e-f492-45d7-b773-891b9f588962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions_encoded.rename_column(\"label\", \"labels\")\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941889bc-ccb5-4968-bdd7-5ba5a9cc624f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abe98ee-7ac7-4cec-b132-1183d5ca65a6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3148e0-6ee7-42b4-ae24-c706aa28e14a",
   "metadata": {},
   "source": [
    "## Now bring it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09169b9c-faa6-4383-b0af-f10c36640476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=n_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fde7a63e-aa3d-499a-b7f8-c31b283419b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971f5b4-67c7-48e3-a5d0-d486fb4fe835",
   "metadata": {},
   "source": [
    "When you first attempt to train your model, you may see the following warning coming from your `TrainingArguments`:\n",
    "\n",
    "    /opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "\n",
    "\n",
    "This is only just a warning, but some background: the present default optimizer `AdamW` from Huggingface is eventually going to go away. sgugger recommends using `adamw_torch`, which is the PyTorch implementation [`torch.optim.AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) (see [API documentation of `optim` argument for `TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.optim))\n",
    "\n",
    "Please see sgugger's response in [Huggingface transformers longformer optimizer warning AdamW](https://discuss.huggingface.co/t/huggingface-transformers-longformer-optimizer-warning-adamw/14711/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0022d409-d5f0-44ca-ae0c-ce714613882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "\n",
    "output_dir = \"test-minilm-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    optim=\"adamw_torch\",\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f6e2663-05b6-415a-939f-693994d282e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=emotions_encoded[\"train\"],\n",
    "    eval_dataset=emotions_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cba067-4759-4766-9f20-2ccaf54bd2e2",
   "metadata": {},
   "source": [
    "You may see a message stating <code>Using cuda_amp half precision backend</code>.\n",
    "\n",
    "From [Nvidia, Deep Learning Performance Documentation, Train with Mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)...\n",
    "> There are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with Tensor Core support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.\n",
    "\n",
    "c.f. [CUDA Automatic Mixed Precision Examples](https://pytorch.org/docs/stable/notes/amp_examples.html#cuda-automatic-mixed-precision-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "544ad6fb-afaf-4196-bbed-8f4ae9bff13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n",
      "  Number of trainable parameters = 33362310\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 03:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.352200</td>\n",
       "      <td>0.974722</td>\n",
       "      <td>0.618269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.606995</td>\n",
       "      <td>0.856834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549300</td>\n",
       "      <td>0.453421</td>\n",
       "      <td>0.899541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.370763</td>\n",
       "      <td>0.917418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.350489</td>\n",
       "      <td>0.916601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-minilm-finetuned-emotion/checkpoint-500\n",
      "Configuration saved in test-minilm-finetuned-emotion/checkpoint-500/config.json\n",
      "Model weights saved in test-minilm-finetuned-emotion/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-minilm-finetuned-emotion/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-minilm-finetuned-emotion/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-minilm-finetuned-emotion/checkpoint-1000\n",
      "Configuration saved in test-minilm-finetuned-emotion/checkpoint-1000/config.json\n",
      "Model weights saved in test-minilm-finetuned-emotion/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-minilm-finetuned-emotion/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-minilm-finetuned-emotion/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: label_text, text. If label_text, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.6991485900878907, metrics={'train_runtime': 232.419, 'train_samples_per_second': 344.206, 'train_steps_per_second': 5.378, 'total_flos': 895521735360000.0, 'train_loss': 0.6991485900878907, 'epoch': 5.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb4d98-9f88-4ee8-99ae-b4c93345ae00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c054891-8769-4573-a7d0-1a07b1d19b01",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a0cb4-5ec8-4810-9d62-fe2426126252",
   "metadata": {},
   "source": [
    "## Transformers as Feature Extractors\n",
    "\n",
    "### from Chapter 2: Text Classification, Natural Language Processing with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "234ae998-7da0-47ec-9723-5aafd20bfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60326c51-af16-4175-aa95-cc163bab7ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a492feb5f7640a6a1c97624054ac556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a00fd498f124ae6991dfe1cb8242ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b088d4bf7d84ab586ae300f7a656121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ch2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8edf70e-c490-4884-bd35-9f6ac8df8997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/buruzaemon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(ch2_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eeb248b4-5913-4225-8d86-df868743016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs tensor shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Inputs tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "306db0ad-1eee-44d3-ac1c-1a5ac7302ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ff4b550-0ca0-4fc2-b7e4-29177d86505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs tensor shape: torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs tensor shape: {outputs.last_hidden_state.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33070947-a367-4dd0-a91a-764679117be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7892569-e8c8-4a5f-b238-3c2f8f6c9a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f45e67-0ca0-4db1-81d0-0e1ee4ee02e4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c23b8-8b2b-4b49-acef-9a790c90a6c6",
   "metadata": {},
   "source": [
    "## Try out our newly-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7ba47-4515-4a03-831d-8141347466be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipl = pipeline(\"text-classification\", \n",
    "                model=\"buruzaemon/test-minilm-finetuned-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd637ad-aeee-4920-96a9-14395996c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipl(\"I am sorry to see you go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d3361-d543-4bd6-a0be-7e4c9f2f42d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

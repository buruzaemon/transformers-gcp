{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7122dbae-412c-4c0b-a9dd-ca207f34cf6c",
   "metadata": {},
   "source": [
    "# Simple Training with the ðŸ¤— Transformers Trainer\n",
    "\n",
    "\n",
    "c.f. Lewis Tunstall & HuggingFace ðŸ¤— video on [Simple Training with the ðŸ¤— Transformers Trainer](https://www.youtube.com/watch?v=u--UVvH-LIQ&t=132s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746d453-47df-4492-aa0d-dddc2a62e619",
   "metadata": {},
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "whoami()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd521bb-4cc3-4176-9050-065228e29dbb",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "HfFolder().get_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17f8e2-ebcb-47f7-bd6f-183cb2bd0081",
   "metadata": {},
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "create_repo(\"test-minilm-finetuned-emotion\", token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bed175-d2c5-4e4d-a40f-a1d16d2d9ee5",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbce6a-e989-4b61-a666-8beb7691b983",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf15ea-2043-4393-89eb-0ad923340ce6",
   "metadata": {},
   "source": [
    "## Prepare for sharing a new model\n",
    "\n",
    "Open up an SSH shell, and log in to your vm instance.\n",
    "\n",
    "Activate your Python virtual environment.\n",
    "\n",
    "Using the `huggingface-cli` command-line utility, log in to HuggingFace and enter your write token.\n",
    "\n",
    "    # log in to Huggingface, and enter your write Role user access token\n",
    "    huggingface-cli login\n",
    "\n",
    "    # create a new Huggingface repo for this model\n",
    "    huggingface-cli repo create test-minilm-finetuned-emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b0b58-b5c9-4bbf-8aa7-e0ce2acfa12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810ac59-0971-42df-8a2a-940046b9d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ee3b8-9edd-40ec-89fd-019c5ef422c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f2d914-46ab-4836-a339-a3718dff9124",
   "metadata": {},
   "source": [
    "## Dataset: EDA & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156fa5fd-e1f1-4192-a49d-be72bdc9ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--emotion-e444b7640ce3116e\n",
      "Found cached dataset json (/home/buruzaemon/.cache/huggingface/datasets/SetFit___json/SetFit--emotion-e444b7640ce3116e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211ba75017a446499d4bdea164dd2319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion_dataset = load_dataset(\"SetFit/emotion\")\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3974c78-fdeb-4f20-bfc2-5f2757173b81",
   "metadata": {},
   "source": [
    "What do the dataset items look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5959a78f-810c-4fb2-a32a-92defa7a3b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0, 'label_text': 'sadness'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c8d07-b78e-49cb-9f97-c6fee8bafd7d",
   "metadata": {},
   "source": [
    "Sometimes, it is easier to inspect a dataset with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c35f22-6f42-48c5-9497-f44119aa069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_text\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = emotion_dataset[\"train\"].to_pandas()\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50228c-2ee4-4ea0-86af-d6805ffb2ff3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b906db-d9f5-4205-b60f-d30d0240711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emotion_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a810800a-6a61-41cc-917f-7d081faf055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'label_text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = emotion_dataset[\"train\"].features\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbfacd-dec3-4483-95bb-e3fdd50c8ee6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36555297-422a-4d64-a2d3-2e6410bd491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d523e-c4f2-460a-bc76-9057eec7e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs = pd.unique(emotion_df[['label', 'label_text']].values.ravel('C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0927def-efbb-4c4f-a237-f6d92375799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = dict(zip(kvs[0::2], kvs[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3a5ed-3510-4379-b661-68203c9efc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f251dca-40b0-4015-8797-f2d93ef9d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {k:v for v,k in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbd3c3-5476-457a-b011-4f7aa1a5289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6abdf-d04c-46e9-87c8-590f181575c6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cad43-5688-4d81-9908-8ef74acf82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2prob = emotion_df[\"label\"].value_counts(normalize=True).sort_index()\n",
    "print(class2prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafed7e-efad-4661-8295-316b6cfeb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in class2prob.items():\n",
    "    print(f\"{id2label[i]:<10} {v:.5F}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d4784-1c5f-48b8-95be-8c0490855f73",
   "metadata": {},
   "source": [
    "Notice that class \"5\" for \"surprise\" is not well represented within the dataset.\n",
    "\n",
    "How do we deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9d3b2-0523-4fd4-9aa8-4ade67bda2c7",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "[microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased?text=I+like+you.+I+love+you) on Hugging Face modelhub.\n",
    "\n",
    "> Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n",
    "\n",
    "Read the [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-trained Transformers](https://arxiv.org/abs/2002.10957) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287efe29-91c4-48a2-92e3-bd52dbf2a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959229f-6d97-4d9d-adeb-972f1fdf6cab",
   "metadata": {},
   "source": [
    "`input_ids`\n",
    "\n",
    "> Index ids of the word embeddings corresponding to the given word.\n",
    "\n",
    "`token_type_ids`\n",
    "\n",
    "> ... a binary mask identifying the two types of sequence in the model ...\n",
    "> ... the â€œcontextâ€ used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the â€œquestionâ€, has all its tokens represented by a 1...\n",
    "> Some models, like XLNetModel use an additional token represented by a 2.\n",
    "\n",
    "`attention_mask`\n",
    "\n",
    "> Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933dc8a-ed17-4419-8915-08f4449af8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(emotion_dataset[\"train\"][\"text\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634bf60-590a-4c27-a932-b839aee31cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0403f44-8d00-459d-931a-13098cac7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = emotion_dataset.map(tokenize_text, batched=True)\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62d901-237e-47b4-8443-b5571185811f",
   "metadata": {},
   "source": [
    "From the API docs on Trainer:\n",
    "\n",
    "> The Trainer class is optimized for ðŸ¤— Transformers models and can have surprising behaviors when you use it on other models. When using it on your own model, make sure:\n",
    ">\n",
    "> * your model always return tuples or subclasses of ModelOutput.\n",
    "> * your model can compute the loss if a labels argument is provided and that loss is returned as the first element of the tuple (if your model returns tuples)\n",
    "> * your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer) but none of them should be named \"label\".\n",
    "\n",
    "So, we need to rename that column for label in our dataset to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ed14e-f492-45d7-b773-891b9f588962",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = emotion_dataset.rename_column(\"label\", \"labels\")\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00fa4e-bb9a-4316-a5e2-61ad0d34fdb5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bd31f-15a8-43ea-9eb3-d2663f8e2822",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665e9ee-057e-4652-a910-ccf4862da463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (1 - (emotion_df[\"label\"].value_counts().sort_index() / len(emotion_df))).values\n",
    "print(class_weights)\n",
    "print(type(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f691f6f-a3c8-4cea-bb52-2b3654a9a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
    "class_weights\n",
    "print(type(class_weights))\n",
    "print(class_weights.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9ac47-a1c8-4810-bedd-61748ad3a237",
   "metadata": {},
   "source": [
    "Now, when training a classfier using a dataset that may suffer for imbalanced classes, one way to deal with the situation is to up-sample from the imbalanced class(es) to offset the differnce(s). However, Transformer models are good at memorizing patterns, and so this might not be a good idea in our case.\n",
    "\n",
    "What we can do is this: correct for the class imbalance with the loss function during training.\n",
    "\n",
    "From the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#trainer) documentation:\n",
    "\n",
    "> To inject custom behavior you can subclass them and override the following methods: ...\n",
    "> * compute_loss - Computes the loss on a batch of training inputs.\n",
    "\n",
    "\n",
    "From PyTorch docs on [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "> If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1c6ad-0288-4a9a-8b43-8dc69696f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # feed inputs to model and extract logits\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # extract labels; not label!\n",
    "        #         ^^^^^\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # define loss function with class weights\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_func(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe98ee-7ac7-4cec-b132-1183d5ca65a6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3148e0-6ee7-42b4-ae24-c706aa28e14a",
   "metadata": {},
   "source": [
    "## Now bring it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09169b9c-faa6-4383-b0af-f10c36640476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=n_classes,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7a63e-aa3d-499a-b7f8-c31b283419b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022d409-d5f0-44ca-ae0c-ce714613882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "logging_steps = len(emotion_dataset[\"train\"]) // batch_size\n",
    "\n",
    "output_dir = \"test-minilm-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e2663-05b6-415a-939f-693994d282e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=emotion_dataset[\"train\"],\n",
    "    eval_dataset=emotion_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cba067-4759-4766-9f20-2ccaf54bd2e2",
   "metadata": {},
   "source": [
    "You may see a warning message stating <code>Using cuda_amp half precision backend</code>.\n",
    "\n",
    "From [Nvidia, Deep Learning Performance Documentation, Train with Mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)...\n",
    "> There are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with Tensor Core support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.\n",
    "\n",
    "c.f. [CUDA Automatic Mixed Precision Examples](https://pytorch.org/docs/stable/notes/amp_examples.html#cuda-automatic-mixed-precision-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ad6fb-afaf-4196-bbed-8f4ae9bff13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b66255-a74a-48c3-a5a4-a2fb5044cae0",
   "metadata": {},
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59db219-791e-4379-ae1b-75870108f32e",
   "metadata": {},
   "source": [
    "trainer.push_to_hub(use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f45e67-0ca0-4db1-81d0-0e1ee4ee02e4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c23b8-8b2b-4b49-acef-9a790c90a6c6",
   "metadata": {},
   "source": [
    "## Try out our newly-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7ba47-4515-4a03-831d-8141347466be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipl = pipeline(\"text-classification\", \n",
    "                model=\"buruzaemon/test-minilm-finetuned-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd637ad-aeee-4920-96a9-14395996c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipl(\"I am sorry to see you go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d3361-d543-4bd6-a0be-7e4c9f2f42d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

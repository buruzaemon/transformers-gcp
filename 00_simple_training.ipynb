{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7122dbae-412c-4c0b-a9dd-ca207f34cf6c",
   "metadata": {},
   "source": [
    "# Simple Training with the ðŸ¤— Transformers Trainer\n",
    "\n",
    "\n",
    "c.f. Lewis Tunstall & HuggingFace ðŸ¤— video on [Simple Training with the ðŸ¤— Transformers Trainer](https://www.youtube.com/watch?v=u--UVvH-LIQ&t=132s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2d914-46ab-4836-a339-a3718dff9124",
   "metadata": {},
   "source": [
    "## Dataset: EDA & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156fa5fd-e1f1-4192-a49d-be72bdc9ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset emotion (/home/brooke_fujita/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc60229e290e48e6ad23342c9d2ca583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion_dataset = load_dataset(\"emotion\")\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3974c78-fdeb-4f20-bfc2-5f2757173b81",
   "metadata": {},
   "source": [
    "What do the dataset items look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5959a78f-810c-4fb2-a32a-92defa7a3b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c8d07-b78e-49cb-9f97-c6fee8bafd7d",
   "metadata": {},
   "source": [
    "Sometimes, it is easier to inspect a dataset with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c35f22-6f42-48c5-9497-f44119aa069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = emotion_dataset[\"train\"].to_pandas()\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50228c-2ee4-4ea0-86af-d6805ffb2ff3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b906db-d9f5-4205-b60f-d30d0240711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emotion_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a810800a-6a61-41cc-917f-7d081faf055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = emotion_dataset[\"train\"].features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c5b476-11b1-4f7a-b6ca-9c08cdeffd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['label'].int2str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651b72fb-9e8e-430a-a402-cba30bf5d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17894cb-3ea2-4d89-882f-bcc8ebbeb046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {idx:features['label'].int2str(idx) for idx in range(n_classes)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f251dca-40b0-4015-8797-f2d93ef9d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k:v for v,k in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6abdf-d04c-46e9-87c8-590f181575c6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40cad43-5688-4d81-9908-8ef74acf82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.291625\n",
      "1    0.335125\n",
      "2    0.081500\n",
      "3    0.134937\n",
      "4    0.121063\n",
      "5    0.035750\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class2prob = emotion_df[\"label\"].value_counts(normalize=True).sort_index()\n",
    "print(class2prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dafed7e-efad-4661-8295-316b6cfeb430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness    0.29163\n",
      "joy        0.33513\n",
      "love       0.08150\n",
      "anger      0.13494\n",
      "fear       0.12106\n",
      "surprise   0.03575\n"
     ]
    }
   ],
   "source": [
    "for i,v in class2prob.items():\n",
    "    print(f\"{id2label[i]:<10} {v:.5F}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d4784-1c5f-48b8-95be-8c0490855f73",
   "metadata": {},
   "source": [
    "Notice that class \"5\" for \"surprise\" is not well represented within the dataset.\n",
    "\n",
    "How do we deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9d3b2-0523-4fd4-9aa8-4ade67bda2c7",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "[microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased?text=I+like+you.+I+love+you) on Hugging Face modelhub.\n",
    "\n",
    "> Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n",
    "\n",
    "Read the [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-trained Transformers](https://arxiv.org/abs/2002.10957) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287efe29-91c4-48a2-92e3-bd52dbf2a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959229f-6d97-4d9d-adeb-972f1fdf6cab",
   "metadata": {},
   "source": [
    "`input_ids`\n",
    "\n",
    "> Index ids of the word embeddings corresponding to the given word.\n",
    "\n",
    "`token_type_ids`\n",
    "\n",
    "> ... a binary mask identifying the two types of sequence in the model ...\n",
    "> ... the â€œcontextâ€ used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the â€œquestionâ€, has all its tokens represented by a 1...\n",
    "> Some models, like XLNetModel use an additional token represented by a 2.\n",
    "\n",
    "`attention_mask`\n",
    "\n",
    "> Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5933dc8a-ed17-4419-8915-08f4449af8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(emotion_dataset[\"train\"][\"text\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b634bf60-590a-4c27-a932-b839aee31cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0403f44-8d00-459d-931a-13098cac7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brooke_fujita/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-7b24961a81fff009.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef2cdea687b4607b0f33a3d9036293f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brooke_fujita/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-7425973162d2e3d7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dataset = emotion_dataset.map(tokenize_text, batched=True)\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62d901-237e-47b4-8443-b5571185811f",
   "metadata": {},
   "source": [
    "From the API docs on Trainer:\n",
    "\n",
    "> The Trainer class is optimized for ðŸ¤— Transformers models and can have surprising behaviors when you use it on other models. When using it on your own model, make sure:\n",
    ">\n",
    "> * your model always return tuples or subclasses of ModelOutput.\n",
    "> * your model can compute the loss if a labels argument is provided and that loss is returned as the first element of the tuple (if your model returns tuples)\n",
    "> * your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer) but none of them should be named \"label\".\n",
    "\n",
    "So, we need to rename that column for label in our dataset to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687ed14e-f492-45d7-b773-891b9f588962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dataset = emotion_dataset.rename_column(\"label\", \"labels\")\n",
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00fa4e-bb9a-4316-a5e2-61ad0d34fdb5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bd31f-15a8-43ea-9eb3-d2663f8e2822",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f665e9ee-057e-4652-a910-ccf4862da463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.708375  0.664875  0.9185    0.8650625 0.8789375 0.96425  ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "class_weights = (1 - (emotion_df[\"label\"].value_counts().sort_index() / len(emotion_df))).values\n",
    "print(class_weights)\n",
    "print(type(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f691f6f-a3c8-4cea-bb52-2b3654a9a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "<class 'torch.Tensor'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
    "class_weights\n",
    "print(type(class_weights))\n",
    "print(class_weights.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9ac47-a1c8-4810-bedd-61748ad3a237",
   "metadata": {},
   "source": [
    "Now, when training a classfier using a dataset that may suffer for imbalanced classes, one way to deal with the situation is to up-sample from the imbalanced class(es) to offset the differnce(s). However, Transformer models are good at memorizing patterns, and so this might not be a good idea in our case.\n",
    "\n",
    "What we can do is this: correct for the class imbalance with the loss function during training.\n",
    "\n",
    "From the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#trainer) documentation:\n",
    "\n",
    "> To inject custom behavior you can subclass them and override the following methods: ...\n",
    "> * compute_loss - Computes the loss on a batch of training inputs.\n",
    "\n",
    "\n",
    "From PyTorch docs on [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "> If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea1c6ad-0288-4a9a-8b43-8dc69696f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # feed inputs to model and extract logits\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # extract labels; not label!\n",
    "        #         ^^^^^\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # define loss function with class weights\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_func(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe98ee-7ac7-4cec-b132-1183d5ca65a6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3148e0-6ee7-42b4-ae24-c706aa28e14a",
   "metadata": {},
   "source": [
    "## Now bring it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09169b9c-faa6-4383-b0af-f10c36640476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=n_classes,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fde7a63e-aa3d-499a-b7f8-c31b283419b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0022d409-d5f0-44ca-ae0c-ce714613882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "logging_steps = len(emotion_dataset[\"train\"]) // batch_size\n",
    "\n",
    "output_dir = \"test-minilm-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f6e2663-05b6-415a-939f-693994d282e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=emotion_dataset[\"train\"],\n",
    "    eval_dataset=emotion_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cba067-4759-4766-9f20-2ccaf54bd2e2",
   "metadata": {},
   "source": [
    "You may see a warning message stating <code>Using cuda_amp half precision backend</code>.\n",
    "\n",
    "From [Nvidia, Deep Learning Performance Documentation, Train with Mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)...\n",
    "> There are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with Tensor Core support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.\n",
    "\n",
    "c.f. [CUDA Automatic Mixed Precision Examples](https://pytorch.org/docs/stable/notes/amp_examples.html#cuda-automatic-mixed-precision-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "544ad6fb-afaf-4196-bbed-8f4ae9bff13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.386400</td>\n",
       "      <td>1.062423</td>\n",
       "      <td>0.577758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.713546</td>\n",
       "      <td>0.815931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.509548</td>\n",
       "      <td>0.888702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.432829</td>\n",
       "      <td>0.904052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.395059</td>\n",
       "      <td>0.906486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-minilm-finetuned-emotion/checkpoint-500\n",
      "Configuration saved in test-minilm-finetuned-emotion/checkpoint-500/config.json\n",
      "Model weights saved in test-minilm-finetuned-emotion/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-minilm-finetuned-emotion/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-minilm-finetuned-emotion/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to test-minilm-finetuned-emotion/checkpoint-1000\n",
      "Configuration saved in test-minilm-finetuned-emotion/checkpoint-1000/config.json\n",
      "Model weights saved in test-minilm-finetuned-emotion/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-minilm-finetuned-emotion/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-minilm-finetuned-emotion/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.7554116088867188, metrics={'train_runtime': 138.6087, 'train_samples_per_second': 577.164, 'train_steps_per_second': 9.018, 'total_flos': 582587326282752.0, 'train_loss': 0.7554116088867188, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f45e67-0ca0-4db1-81d0-0e1ee4ee02e4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c23b8-8b2b-4b49-acef-9a790c90a6c6",
   "metadata": {},
   "source": [
    "## Try out our newly-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "763bfc2e-3c26-48d2-93ae-75b72ad75522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "00_simple_training.ipynb  environment.transformers-py38.yml\n",
      "01_asr.ipynb\t\t  test-minilm-finetuned-emotion\n",
      "README.md\t\t  whisper_intro.m4a\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e66408-d765-42d6-b779-3ee7a33d877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file test-minilm-finetuned-emotion/checkpoint-1000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"test-minilm-finetuned-emotion/checkpoint-1000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"sadness\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"love\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"fear\",\n",
      "    \"5\": \"surprise\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"fear\": 4,\n",
      "    \"joy\": 1,\n",
      "    \"love\": 2,\n",
      "    \"sadness\": 0,\n",
      "    \"surprise\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file test-minilm-finetuned-emotion/checkpoint-1000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"test-minilm-finetuned-emotion/checkpoint-1000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"sadness\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"love\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"fear\",\n",
      "    \"5\": \"surprise\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"fear\": 4,\n",
      "    \"joy\": 1,\n",
      "    \"love\": 2,\n",
      "    \"sadness\": 0,\n",
      "    \"surprise\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file test-minilm-finetuned-emotion/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at test-minilm-finetuned-emotion/checkpoint-1000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipl = pipeline(\"text-classification\", \n",
    "                model=\"test-minilm-finetuned-emotion/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dd637ad-aeee-4920-96a9-14395996c498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.8894500732421875}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipl(\"I am sorry to see you go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7ba47-4515-4a03-831d-8141347466be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

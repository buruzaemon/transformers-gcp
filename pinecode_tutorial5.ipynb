{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230d05bd-5d0c-4ff1-a0be-3c0aaae2d030",
   "metadata": {},
   "source": [
    "# On Multilingual Sequence Transformers\n",
    "\n",
    "tl;dr: [`sentence-transformers/paraphrase-multilingual-mpnet-base-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2), a multilingual model for mapping sentences and paragraphs to a 768-dimensional space, deserves your consideration.\n",
    "\n",
    "Covers the tutorial from Pinecone: [Tomayto, Tomahto, Transformer: Multilingual Sentence Transformers](https://www.pinecone.io/learn/series/nlp/multilingual-transformers/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf036a5-0b87-4236-84dd-d79728fdb91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translations', 'talk_name'],\n",
       "    num_rows: 258098\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ted = load_dataset('ted_multi', split='train')\n",
    "ted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637f6717-4b1b-4374-8773-4f0e63454b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'bg', 'de', 'el', 'en', 'es', 'eu', 'fa', 'fr', 'fr-ca', 'he', 'hr', 'hu', 'it', 'ja', 'ko', 'nb', 'nl', 'pl', 'pt', 'pt-br', 'ro', 'ru', 'sq', 'tr', 'vi', 'zh-cn', 'zh-tw']\n"
     ]
    }
   ],
   "source": [
    "print(ted[0]['translations']['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3dbf16-dac8-43c8-98b1-e626554b0627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = ted[0]['translations']['language'].index('en')\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a019b9-37c0-4430-8fd3-1fb1e87330b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they &apos;re all about and to understand the key players and to deal with them .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the index to get the corresponding translation\n",
    "source = ted[0]['translations']['translation'][idx]\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63ffce7-88d9-4af8-87db-93bb2ad521e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they &apos;re all about and to understand the key players and to deal with them .',\n",
       " 'من ضمن جميع المثبطات المقلقة التي نعاني منها اليوم نفكر في المقام الاول في الامور المالية والاقتصادية واكثر ما يهمني بشكل اكثر هو عجز الحوار السياسي — قدرتنا على فهم الصراعات الحديثة على ماهي عليه , بالذهاب الى اصلها الفعلي وعلى فهم اللاعبين الرئيسيين وعلى التعامل معهم')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use that info to create all (source, translation) pairs\n",
    "pairs = []\n",
    "for i, translation in enumerate(ted[0]['translations']['translation']):\n",
    "    # we don't want to use the source language (English) as a translation\n",
    "    if i != idx:\n",
    "        pairs.append((source, translation))\n",
    "\n",
    "# let's see what we have\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e77182c-9a72-456a-832c-14bc4b0c46a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0bbe313c56466bbdd8bcca4ce730e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from tqdm.auto import tqdm  # so we see progress bar\n",
    "\n",
    "# initialize list of languages to keep\n",
    "lang_list = ['ja']\n",
    "# create dict to store our pairs\n",
    "train_samples = {f'en-{lang}': [] for lang in lang_list}\n",
    "\n",
    "# now build our training samples list\n",
    "for row in tqdm(ted):\n",
    "    # get source (English)\n",
    "    idx = row['translations']['language'].index('en')\n",
    "    source = row['translations']['translation'][idx].strip()\n",
    "    # loop through translations\n",
    "    for i, lang in enumerate(row['translations']['language']):\n",
    "        # check if lang is in lang list\n",
    "        if lang in lang_list:\n",
    "            translation = row['translations']['translation'][i].strip()\n",
    "            train_samples[f'en-{lang}'].append(\n",
    "                source+'\\t'+translation\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a40258-0084-4a10-8c42-b95b8589794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-ja: 204090\n"
     ]
    }
   ],
   "source": [
    "# how many pairs for each language?\n",
    "for lang_pair in train_samples.keys():\n",
    "    print(f'{lang_pair}: {len(train_samples[lang_pair])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8ebb4a-485b-4f48-a074-460a9ef343df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "# save to file, sentence transformers reader will expect tsv.gz file\n",
    "for lang_pair in train_samples.keys():\n",
    "    with gzip.open(f'./data/ted-train-{lang_pair}.tsv.gz', 'wt', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_samples[lang_pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d2a7a9-f208-4dd5-b786-64c6e3fc6b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ted-train-en-ja.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046e505-3d4b-4229-9fbd-b493c8c14b2e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ed7f3-f733-4982-bba1-769a5268f379",
   "metadata": {},
   "source": [
    "### Selecting a tokenizer for the Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d4c3a7e-c591-43a9-993d-98a107c1a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f08e5163-c75d-4aef-8fb9-09d5fe92dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'include', 'several', 'languages']\n",
      "['い', '##く', '##つ', '##か', '##の', '[UNK]', '語', 'を', '[UNK]', 'め', '##る']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'we will include several languages',\n",
    "    'いくつかの言語を含める'\n",
    "]\n",
    "\n",
    "for text in sentences:\n",
    "    print(bert_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d5e0f5-2167-4f77-bf01-5214c47d92b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁we', '▁will', '▁include', '▁several', '▁language', 's']\n",
      "['▁', 'いくつか', 'の', '言語', 'を含め', 'る']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "for text in sentences:\n",
    "    print(xlmr_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b18da-011d-4fda-9541-b11438b77de1",
   "metadata": {},
   "source": [
    "### Student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ac8976-c860-48fd-9385-c59e7293015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "xlmr = models.Transformer('xlm-roberta-base')\n",
    "pooler = models.Pooling(\n",
    "    xlmr.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "student = SentenceTransformer(modules=[xlmr, pooler])\n",
    "student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7d379-3363-4084-804c-bbcf04697a65",
   "metadata": {},
   "source": [
    "### Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33df5f93-0d22-4c70-bd8c-2c0f6b3b8539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = SentenceTransformer('all-mpnet-base-v2')\n",
    "teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbab7ff-6ff4-4318-b6d1-efa54ae1ffad",
   "metadata": {},
   "source": [
    "... but we need access to the _raw logits in the output layer_, not a layer-normalized result...\n",
    "\n",
    "Please see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f8f7fe-64a3-415d-8065-b3f5a3499777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
    "teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3932d2-f8e0-476b-b00b-b13dbd043597",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "* `sentence_transformers.ParallelSentencesDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02de66dc-386b-4b56-97e8-85e7322f8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import ParallelSentencesDataset\n",
    "\n",
    "data = ParallelSentencesDataset(\n",
    "    student_model=student, \n",
    "    teacher_model=teacher, \n",
    "    batch_size=32, \n",
    "    use_embedding_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab74be56-7ebc-47ca-8c80-96a6a7ea4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ted-train-en-ja.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "max_sentences_per_language = 500000\n",
    "train_max_sentence_length = 250 # max num of characters per sentence\n",
    "\n",
    "train_files = [f for f in os.listdir('./data') if 'train' in f]\n",
    "for f in train_files:\n",
    "    print(f)\n",
    "    data.load_data('./data/'+f, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ab8eefc-3569-4ca7-a2f8-89a90ddcd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91a49bc7-2a26-41f3-86d0-bf7239e5a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "loss = losses.MSELoss(model=student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fdfa461-75ac-4693-b7cd-c3099b787265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0119d74376974c948c6b24a9c12144cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f921ccc58e4be7a044cee836c07309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:547: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673752754831/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import evaluation\n",
    "import numpy as np\n",
    "\n",
    "epochs = 1\n",
    "warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "student.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='./xlmr-ted',\n",
    "    #optimizer_params={'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False},\n",
    "    optimizer_params={'lr': 2e-5, 'eps': 1e-6},\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad5bbb-d483-4ae0-b2a5-e7d26eb697be",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95ee1f-24ca-4afe-aa6e-b37b16b14f6f",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae54c1-f317-4132-84aa-7b5045be1bee",
   "metadata": {},
   "source": [
    "> 多言語用 STS ベンチマークデータセット(stsb_multi_mt)は huggigface datasets として公開されています。ただ日本語だけ対象外となっています。 どうやら、github issue によると train データに deepl で翻訳すると空行になってしまう箇所があり、そのためエラーとなっているようです。 今回は test データのみを使用しますのでデータとしては問題ないのですが、読み込みの設定で日本語が除外されていますので、 ローカルにコピーして修正します。\n",
    "\n",
    "from [https://tech.yellowback.net/posts/sentence-transformers-japanese-models](https://tech.yellowback.net/posts/sentence-transformers-japanese-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba7a0fe-e5a5-4817-ade4-b0f3e9be601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "    num_rows: 1379\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = load_dataset('stsb_multi_mt_ja', 'en', split='test')\n",
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d37c4b4-400d-41dc-801f-9cfad538b224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'A girl is styling her hair.',\n",
       " 'sentence2': 'A girl is brushing her hair.',\n",
       " 'similarity_score': 2.5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9621c087-ce15-4dd9-87b2-aecb911b2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "    num_rows: 1379\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja = load_dataset('stsb_multi_mt_ja', 'ja', split='test')\n",
    "ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d37ec10-ca44-442f-9fcc-5098c5508e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': '女の子が髪をスタイリングしています。',\n",
       " 'sentence2': '少女が髪をとかしている。',\n",
       " 'similarity_score': 2.5}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "782d31ee-fe7c-4743-a46e-ff3d72762189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': '女の子が髪をスタイリングしています。',\n",
       " 'sentence2': '少女が髪をとかしている。',\n",
       " 'similarity_score': 0.5}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = en.map(lambda x: {'similarity_score': x['similarity_score'] / 5.0})\n",
    "ja = ja.map(lambda x: {'similarity_score': x['similarity_score'] / 5.0})\n",
    "\n",
    "ja[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce0d07f0-0caf-4dec-b62f-48aaf3a40310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "en_samples = []\n",
    "ja_samples = []\n",
    "en_ja_samples = []\n",
    "\n",
    "for i in range(len(en)):\n",
    "    en_samples.append(InputExample(\n",
    "        texts=[en[i]['sentence1'], en[i]['sentence2']],\n",
    "        label=en[i]['similarity_score']\n",
    "    ))\n",
    "\n",
    "    ja_samples.append(InputExample(\n",
    "        texts=[ja[i]['sentence1'], ja[i]['sentence2']],\n",
    "        label=ja[i]['similarity_score']\n",
    "    ))\n",
    "\n",
    "    en_ja_samples.append(InputExample(\n",
    "        texts=[en[i]['sentence1'], ja[i]['sentence2']],\n",
    "        label=en[i]['similarity_score']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d382462-3b04-4e3c-af37-b5ae90d7c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "ja_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    ja_samples, write_csv=False\n",
    ")\n",
    "\n",
    "en_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    en_samples, write_csv=False\n",
    ")\n",
    "\n",
    "en_ja_eval = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    en_ja_samples, write_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da536c6d-c9d9-4e0c-a19f-dea8f0e180a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7143563910719034"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('./xlmr-ted')\n",
    "\n",
    "en_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d667f1de-b337-434a-8315-ef7e81234789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598930363883629"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1ad7f33-883c-4b32-8bae-18575159ff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6091859290134577"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ja_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ae9b95a-4840-4a55-8462-f9ad71533b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr = models.Transformer('xlm-roberta-base')\n",
    "pooler = models.Pooling(\n",
    "    xlmr.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "student = SentenceTransformer(modules=[xlmr, pooler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7b1896a-6a2c-4956-92ff-a1aa9849e98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47525931826733264"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_eval(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7fdeedc-9846-4a2a-823c-8657c7e84b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4771557350064595"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_eval(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3b3180a-884c-4e12-93fa-07e20d9cf5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20667255470707896"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ja_eval(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78781c-284b-41fa-8f81-05eea68bce3d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5396a9dd-b6c5-4e89-9888-b057e8e228d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49593d2d-c905-488c-87fd-eec71d48c0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8682218476677823"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7c83fc-b9a4-4c5d-8c58-7b815515bb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793185431984425"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02e78d22-ee30-478a-a88a-564b35dc7eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7765553636736806"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ja_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea2be9-b431-4930-81d0-2af56c87da93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

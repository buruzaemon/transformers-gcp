{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8d0df5-44b3-4d97-984e-3fcccc5fd386",
   "metadata": {},
   "source": [
    "# Chapter 4: Multilingual Named Entity Recognition\n",
    "\n",
    "## On Fine-tuning ... to perform token classification (NER)\n",
    "\n",
    "### General workflow\n",
    "\n",
    "The workflow for the task of fine-tuning an existing model for token classfication can be broken down into for major tasks:\n",
    "\n",
    "* preparing a dataset for fine-tuning\n",
    "* preparing an appropriate head and configuration to use over the existing model\n",
    "* prepare performance metrics, training arguments, and a data collator\n",
    "* lastly, train your model and evaluate the results\n",
    "\n",
    "#### Prepare dataset\n",
    "\n",
    "1. Obtain a dataset.\n",
    "\n",
    "1. Add any neccessary features to the dataset(s) required for token classification. In the case of NER and our dataset, the `ner_tags` feature have values that are `int` indicating the class. Since we would like to use the IOB2 format, we use `map()` and `ClassLabel.int2str()` to add another dataset feature containing the IOB2 tag strings corresponding to the `ner_tags` class `int` value.\n",
    "\n",
    "1. Tokenize the training dataset according to your needs. _Make sure you have an appropriate tokenizer!_\n",
    "\n",
    "\n",
    "#### Prepare custom model head that wraps an existing base model\n",
    "\n",
    "1. Create a custom model for token classification. Ideally, this should wrap an existing base model and use the logits of that to classify the named entity tokens. In our case, we extend `transformers.models.roberta.modeling_roberta.RobertaPretrainedModel`, implementing `__init__` and `forward`.\n",
    "\n",
    "1. Prepare a corresponding configuration object for our model. We can leverage [`transformer.AutoConfig.from_pretrained`](https://huggingface.co/docs/transformers/v4.26.1/en/model_doc/auto#transformers.AutoConfig.from_pretrained), passing in the model's name, the `num_labels` for the number of NER types (classes), and dictionaries mapping `id2label` and `label2id`.\n",
    "\n",
    "1. Test this model and configuration on an example input.\n",
    "\n",
    "\n",
    "#### Prepare for fine-tuning\n",
    "\n",
    "1. Prepare performance metrics.\n",
    "\n",
    "1. Prepare training arguments\n",
    "\n",
    "1. Prepare data collator\n",
    "\n",
    "\n",
    "#### Train and review metrics\n",
    "\n",
    "1. Train\n",
    "\n",
    "1. Review metrics\n",
    "\n",
    "1. Perform error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1505f67-9124-410b-b7d0-a68767dbb6aa",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "... Check out the [xtreme](https://huggingface.co/datasets/xtreme) dataset on Huggingface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1913d1-7fb5-455f-a23a-9060d098fbec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names, load_dataset\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d96c7ea-156e-4bb2-b2af-4d6691a65335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557e81a-97ed-4b78-b332-aa7c4dd1c5b9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255b14c9-c05c-4b08-9f27-7e6274380cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ef0d1329ae4d30ab25af3c15c94c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e5ddf09f1ae095ec.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-25e7e2dd003d0fa6.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-73a95bc0accfea8b.arrow\n",
      "Found cached dataset xtreme (/home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb015b0d7ba435a9b9c9c439fca96d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-6ff29513007ec78b.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c5c9a4fc19dfd7d6.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-9711ab25936b81b7.arrow\n",
      "Found cached dataset xtreme (/home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bb37b8c8b240f7922ca1b2c13e9add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-daa9a1770078307c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5e244c05031bab3c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-497ee15c12bff58d.arrow\n",
      "Found cached dataset xtreme (/home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c58d47923af4411ab3529ead2c99b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-757845faa9fa6949.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-305cefc7ffa49fd9.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e5ec5e6ba7c1237d.arrow\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# the major languages in Switzerland!\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang,frac in zip(langs, fracs):\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "                .shuffle(seed=0)\n",
    "                .select(range(int(frac * ds[split].num_rows)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ce6ad4-8050-4c0e-9486-97a17bc8b76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "    index=[\"Number of training examples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998f91df-2feb-4b5b-8f2b-f38c7e51bbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'datasets.dataset_dict.DatasetDict'>, {'de': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 12580\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "}), 'fr': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 4580\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 2290\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 2290\n",
      "    })\n",
      "}), 'it': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 1680\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 840\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 840\n",
      "    })\n",
      "}), 'en': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 1180\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 590\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 590\n",
      "    })\n",
      "})})\n"
     ]
    }
   ],
   "source": [
    "print(panx_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7849a7a1-a2ec-4063-8e58-e15e24679340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "print(type(element))\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96af1e9d-d32b-4b06-9a30-10f41e974a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
      "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3214ec-b4a4-4faa-b934-bd3b9b4c6cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12167f83-1f33-4eca-bdd7-d095fe348088",
   "metadata": {
    "tags": []
   },
   "source": [
    "type(tags)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e185561-08bd-4a85-847d-4af22f361c93",
   "metadata": {
    "tags": []
   },
   "source": [
    "dir(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331001ba-00c6-4bde-b1d7-00e6b28bc839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999fac6e-ac5c-48a4-9f86-13d4a2dd1c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-f46d0659f7a52a15.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-333a88ffd23068c4.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e6d4015812e9aa22.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4263a3e3-b684-4ca5-8165-503afeb13cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
      "        num_rows: 12580\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(panx_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc0b847-ad24-4c80-b5c1-91c4df3f72ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8    \n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen  \\\n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame(\n",
    "    [de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "    [\"Tokens\", \"Tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467f1ea9-4612-4403-9402-e5f5ee9e8493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(\n",
    "    split2freqs,\n",
    "    orient=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd9b64-e4e4-4d2f-96b2-8c4dbc3d7112",
   "metadata": {},
   "source": [
    "## Multilingual Transformers\n",
    "\n",
    "XLM-Roberta supports 100 languages, Japanese included!\n",
    "\n",
    "Suggested reading:\n",
    "\n",
    "* [A. Conneau, K. Khandelwal, et al.; 2020; \"Unsupervised Cross-lingual Representation Learning at Scale\"](https://arxiv.org/pdf/1911.02116.pdf)\n",
    "* [T. Kudo, J. Richardson; 2018; \"SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing\"](https://arxiv.org/pdf/1808.06226.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ce814-a92c-4584-aa48-09381caeef6d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f162a-47e7-490e-833e-2978e34bc9f3",
   "metadata": {},
   "source": [
    "## A Closer Look at Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02acb81b-3314-47ae-9b83-ed8c874343ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44c5eef-ad65-4025-afde-15d6bc0ee89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b092693-1cd6-4c3a-8b2d-cc004447f4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be602eee-dd41-4e09-8ed4-898d72e7afde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: ['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]JackSpa##rrowlovesNewYork![SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"BERT tokens: {bert_tokens}\")\n",
    "''.join(bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd87342-c938-43f6-9163-eb04e82a6633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML-R tokens: ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"XML-R tokens: {xlmr_tokens}\")\n",
    "''.join(xlmr_tokens).replace(u'\\u2581', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849bade-d11e-4953-b955-7039388a721f",
   "metadata": {},
   "source": [
    "Suggested reading:\n",
    "\n",
    "* [J. Devlin, et al., \"BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\"](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0bc62-a803-4441-a9b2-253c25830b21",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86549ad-fe67-45bd-a2a4-2f91af153f23",
   "metadata": {},
   "source": [
    "## Creating a Custom Model for Token Classification\n",
    "\n",
    "Or, How to build a custom head for any task and just mount it on top of a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99a1cc7-949b-4506-a413-ee44f5bec82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec73ac5b-2ecc-48d7-a9b6-b2086ecee6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        # load model body!\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        # load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # use model body to get encoder representations\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # apply classifier to encoding representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        # return\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693dc2f3-0e99-4386-89d4-61c52afb1e95",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2a8b1-fddd-441b-8180-72c023d5518f",
   "metadata": {},
   "source": [
    "## Loading a Custom Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e5d46c-fc67-4ac6-846b-9961815b162b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc13e38-5a90-4ee4-acd2-4a53d3e03dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(\n",
    "    xlmr_model_name,\n",
    "    num_labels=tags.num_classes,\n",
    "    id2label=index2tag,\n",
    "    label2id=tag2index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b22ce47f-ca7c-4588-9024-74bbc01f897b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(xlmr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9957b4-002b-4d53-b996-f4775da5f52c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1315ee-8eda-4ba1-8be7-4e1badd98f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inputs IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1      2      3      4  5     6      7   8     9\n",
       "Tokens      <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "Inputs IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    [xlmr_tokens, input_ids[0].numpy()],\n",
    "    index = [\"Tokens\", \"Inputs IDs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a29ddf1d-e731-48ac-864e-ae9e31d969d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "\n",
    "predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7dfbc19-261a-4299-83c4-a83ee0fb67b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "Tags    I-ORG  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  I-ORG"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame(\n",
    "    [xlmr_tokens, preds],\n",
    "    index = [\"Tokens\", \"Tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b6fc51e-b5bc-4e69-b639-295ba9a5cdff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    \n",
    "    # encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # get prediction as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    \n",
    "    # take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    return pd.DataFrame([tokens, preds], index = [\"Tokens\", \"Tags\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d8672-870e-4ed1-b816-a414c06cdeb5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2133a-3b7f-4ac7-aa5d-85ae3a77b617",
   "metadata": {},
   "source": [
    "## Tokenizing Texts for NER\n",
    "\n",
    "   `function(examples: Dict[str, List]) -> Dict[str, List]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d721392b-df3e-4826-b6ee-2f9c485a5305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b22a69-81ae-412c-b39f-0b3164819bb3",
   "metadata": {},
   "source": [
    "_what is `is_split_into_words`?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8b70b8d-cead-4a5c-b010-7ef4925edbed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15   \n",
       "Tokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo  \\\n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = xlmr_tokenizer(\n",
    "    de_example[\"tokens\"], \n",
    "    is_split_into_words=True\n",
    ")\n",
    "\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518d285-b2a5-48f7-adbc-0894f56f7290",
   "metadata": {},
   "source": [
    "_what is `word_ids`?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd756c8-206d-4a69-bc7d-306d2cec995d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...   \n",
       "Tokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  \\\n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad9167b5-7b5e-47f5-a81a-e828e5fc2a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8    \n",
       "Tokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger  \\\n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23   \n",
       "Tokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .  \\\n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_index = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_index:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_index:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_index = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index  = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [tokens, word_ids, label_ids, labels],\n",
    "    index = index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf1a201a-bd7b-4264-bfc4-141e5af7b519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation = True,\n",
    "        is_split_into_words = True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "030a4f8d-1a01-42ac-8ec5-91ad5159eac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(\n",
    "        tokenize_and_align_labels, \n",
    "        batched = True,\n",
    "        remove_columns = [ \"langs\", \"ner_tags\", \"tokens\" ]\n",
    "    )\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed4a857b-d195-4f65-9ca2-5872a244cfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-353d9cf50e8548f2.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-789dd376a720ff6a.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5cfc1058f7f3af38.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642f8047-d8da-4c3b-8994-d54f7383c6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12580\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 6290\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(panx_de_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda2e6f-2fe2-4d03-8fe4-5c3ec05d6478",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7987e2-7e1e-4a65-93a7-5a8a8b1d0a9c",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "`seqeval` is a Python framework for sequence labeling evaluation. Check out [chakki-works/seqeval on Github](https://github.com/chakki-works/seqeval).\n",
    "\n",
    "Also see the [Metrics Card for `seqeval` on huggingface](https://huggingface.co/spaces/evaluate-metric/seqeval).\n",
    "\n",
    "#### Regarding the `seqeval` output...\n",
    "\n",
    "##### precision\n",
    "\n",
    "* _How many of the __retrieved__ items are actually relevant?_\n",
    "* $Precision = \\frac{\\text{TP}}{\\text{TP } + \\text{ FP}}$\n",
    "* _Also known as __positive predictive value___\n",
    "\n",
    "\n",
    "##### recall\n",
    "\n",
    "* _How many of the __relevant__ items were actually retrieved?_\n",
    "* $Recall = \\frac{\\text{TP}}{\\text{TP } + \\text{ FN}}$\n",
    "* _Also known as __sensitivity___\n",
    "\n",
    "##### F1-score\n",
    "\n",
    "* _A single metric combining both precision and recall via the harmonic mean_\n",
    "* _Approximately the average of the two when they are close._\n",
    "* _Coincides with the square of the geometric mean divided by the arithmetic mean._\n",
    "* $F = 2 \\times \\frac{\\text{precision } \\times \\text{ recall}}{\\text{precision } + \\text{ recall}}$\n",
    "* _Also known as __$F_{1}$-measure___\n",
    "* _Is a special case of the general $F_{\\beta}$ measure: $F_{\\beta} = (1 + \\beta^2) \\space \\times \\space \\frac{\\text{precision } \\times \\text{ recall}}{\\beta2 \\space \\times \\text{ precision } + \\text{ recall}}$_\n",
    "\n",
    "##### support\n",
    "\n",
    "* the number of actual items belonging the the class in question\n",
    "\n",
    "##### micro average vs macro average vs weighted average\n",
    "\n",
    "... see [this answer to the question \"Micro Average vs Macro average Performance in a Multiclass classification setting\" on the Data Science Stack Exchange](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/24051#24051)...\n",
    "\n",
    "... and [another on macro average vs weighted average in `sklearn.metrics.classification_report`](https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23c2e1bf-c21f-4a04-9bdb-f7aaa6021ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ee619-7737-4fe2-9371-918545a07e21",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05d820-e865-4fc3-8629-e59b3e171963",
   "metadata": {},
   "source": [
    "## Fine-tuning XLM-RoBERTa\n",
    "\n",
    "### Create `TrainingArguments`\n",
    "\n",
    "* Before instantiating your `Trainer`, create a `TrainingArguments` to access all the points of customization during training. In this example, our arguments encompass things like:\n",
    "   * `output_dir` ... output directory where the model predictions and checkpoints will be written.\n",
    "   * `log_level`\n",
    "   * `num_train_epochs`\n",
    "   * `per_device_train_batch_size`\n",
    "   * `save_steps`\n",
    "   * `logging_steps`\n",
    "   <br/>... among other things... <p/>\n",
    "* During training, if you do not take certain steps, you may see a warning message when invoking `trainer.train()`:\n",
    "\n",
    "<pre>\n",
    "/opt/conda/envs/transformers-py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "</pre>\n",
    "\n",
    "To avoid this, try explicitly using the `adamw_torch` optimizer in your training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9b5ce2f-6cf6-4ad6-ae4c-5448cc528331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    "    optim=\"adamw_torch\",    # let's get rid of that annoying warning message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6077d81d-1fa5-4c1c-8bd9-a18cc590901d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71915e30b5ba4c9695ba32b7667c4d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# no error, means this call succeeded!\n",
    "#\n",
    "# you can click on the log entries icon in the lower\n",
    "# left-hand portion of the window pane to see the\n",
    "# logging messages indicating whether login\n",
    "# succeeded or not\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3d168-cba8-4fb9-ab7f-4369c94c2a3c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef9eec-9c29-41d0-b5d8-5958cb9397c0",
   "metadata": {},
   "source": [
    "### Create `transformers.Trainer`\n",
    "\n",
    "In this example, we specify the following arguments when instantiating [`transformers.Trainer`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "##### `model_init` \n",
    "*  A function that instantiates the model to be used. If provided, each call to `train()` will start from a new instance of the model as given by this function.\n",
    "\n",
    "##### `args` \n",
    "* define a `TrainingArguments` object instance that will contain all the hyperparameters the `Trainer` will use for training and evaluation.\n",
    "\n",
    "##### `data_collator`\n",
    "* The function to use to form a batch from a list of elements of train_dataset or eval_dataset. Will default to `default_data_collator(){ if no tokenizer is provided, an instance of DataCollatorWithPadding otherwise.\n",
    "\n",
    "##### `compute_metrics`\n",
    "* The function that will be used to compute metrics at evaluation. Must take a `EvalPrediction` and return a dictionary string to metric values.\n",
    "\n",
    "##### `train_dataset`\n",
    "* The dataset to use for training. If it is a `Dataset`, columns not accepted by the `model.forward()` method are automatically removed.\n",
    "\n",
    "##### `eval_dataset`\n",
    "* The dataset to use for evaluation. If it is a `Dataset`, columns not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.\n",
    "\n",
    "##### `tokenizer`\n",
    "* The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.\n",
    "\n",
    "\n",
    "OK, let's get into the more important details of `Trainer`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b5a41-cba6-4730-85af-33c94ba924e3",
   "metadata": {},
   "source": [
    "### `model_init`\n",
    "\n",
    "An argument when creating an instance of the `Trainer` object itself, this helper function will let us instantiate new model instance with each call to `Trainer.train()`.\n",
    "\n",
    "Here, note that we using the `from_pretrained` function in our custom `XLMRobertaForTokenClassification` class, passing in:\n",
    "* `xlmr_model_name`\n",
    "* `xlmr_config`\n",
    "\n",
    "Note that we also put this newly-created model instance on the `device` (GPU), and then return a handle to that instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46c9ca0b-2c57-4985-bce1-ec3ce8b194e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (\n",
    "        XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16693d67-08d6-4802-a41e-6430ffdeb808",
   "metadata": {},
   "source": [
    "### `compute_metrics`\n",
    "\n",
    "A function called during training to calculate statistics.\n",
    "\n",
    "This function must take an instance of [`EvalPrediction`](https://huggingface.co/docs/transformers/v4.26.1/en/internal/trainer_utils) as its sole argument, and then return a dictionary of string to metric values. This `EvalPrediction` argument has the following properties for use when calculating training statistics:\n",
    "* `predictions` (`np.ndarray`) — Predictions of the model.\n",
    "* `label_ids` (`np.ndarray`) — Targets to be matched.\n",
    "* `inputs` (`np.ndarray`, optional) — "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "002c06ad-3971-42f5-8d9a-82a48245c0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # ignore label IDS == -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "    \n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bda7e1e9-22f0-4103-b855-c5a162f95924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(\n",
    "        eval_pred.predictions, \n",
    "        eval_pred.label_ids\n",
    "    )\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48372e-1bd5-445e-b682-55346d5ff1ed",
   "metadata": {},
   "source": [
    "### `data_collator`\n",
    "\n",
    "* Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of `train_dataset` or `eval_dataset`.\n",
    "\n",
    "* Here, we use an instance of [`transformers.DataCollatorForTokenClassification`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/data_collator#transformers.DataCollatorForTokenClassification), passing in our `xlmr_tokenizer` tokenizer, which as `XLMRobertaTokenizerFast` is a subclass of [`transformers.PreTrainedTokenizerFast`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast). This is a case of one of the advantages of leveraging huggingface's abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f86865d7-88a3-4aca-bb48-266d00f1af2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd10f66-8e36-43b9-a07a-d70fb8c31385",
   "metadata": {},
   "source": [
    "### `transformers.Trainer`\n",
    "\n",
    "Instantiate [`transformers.Trainer`](), passing in all of our custom objects and data needed for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ce69d90-4bf2-4124-ab75-a6b0095b215b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-de is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=panx_de_encoded[\"train\"],\n",
    "    eval_dataset=panx_de_encoded[\"validation\"],\n",
    "    tokenizer=xlmr_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7d679e7-1bf5-4b4f-afd1-11ca1067aa30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1575' max='1575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1575/1575 09:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.164123</td>\n",
       "      <td>0.814524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.139933</td>\n",
       "      <td>0.849726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.863279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1575, training_loss=0.15599132471614413, metrics={'train_runtime': 541.8077, 'train_samples_per_second': 69.656, 'train_steps_per_second': 2.907, 'total_flos': 863012377186080.0, 'train_loss': 0.15599132471614413, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a464a91d-6510-4dff-abbb-5298d10ab09c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de\n",
      "   376b0d3..0a0180f  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de/commit/0a0180fc779c2afa02775395b277773e766488a3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8997fd2-ed6a-4230-bcb8-4552725a22dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9    \n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google  \\\n",
       "Tags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n",
       "\n",
       "         10          11     12    13  \n",
       "Tokens  ▁in  ▁Kaliforni     en  </s>  \n",
       "Tags      O       B-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bd458-8683-42f7-899d-c5d14ffe15d4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d48425-9f99-4823-af8b-a8f2ac22ba3f",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Now let's run the validation set through the fine-tuned model and see what sort of errors and issues we may be facing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5edbe7d-56b5-4873-ad13-add00de725ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6290\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3f2e6-fe4c-4950-8f66-e0ed465ed393",
   "metadata": {},
   "source": [
    "As before, we can use [`datasets.arrow_dataset.Dataset.map`](datasets.arrow_dataset.Dataset) to apply a function to all examples in the validation dataset. Here, we will make predictions and calculate loss for the `de` validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bddfb777-137b-485e-81ca-b07e0765e814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    \n",
    "    # pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    # since we are not concerned with back-prop,\n",
    "    # and we only want to get predictions on the validation set,\n",
    "    # using no_grad gives us a context-manager with reduced memory consumption!\n",
    "    with torch.no_grad():\n",
    "        # pass data through model\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        \n",
    "        # logit.size: [batch_size, sequence_length, classes]\n",
    "        # predict class with largest logit value on classes axis\n",
    "        # don't forget to \"bring\" that value back to the CPU,\n",
    "        # and convert to Numpy ???\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "\n",
    "    # calculate loss per token after flattening the batch dimension with view\n",
    "    loss = cross_entropy(\n",
    "        output.logits.view(-1, 7), # 7 NER types (classes)\n",
    "        labels.view(-1),\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "    \n",
    "    # unflatten batch dimension, move computation back to CPU, and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "    \n",
    "    return {\"loss\": loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73762242-c268-4f0d-aa22-8dcdb8aafd01",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets.utils.py_utils import dumps\n",
    "dumps(forward_pass_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b6f09-47db-4807-b30f-b7e4ec490a5e",
   "metadata": {},
   "source": [
    "### ???\n",
    "\n",
    "TODO... find out what this warning means!\n",
    "\n",
    "> \n",
    "<pre>\n",
    "Parameter 'function'=<function forward_pass_with_label at 0x7fe20dfd20d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ab2af6f-56c2-4dc7-9b84-5b69d329a46e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function forward_pass_with_label at 0x7f1374c020d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1c80317fa3b1799d.arrow\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the validation set,\n",
    "# adding in columns on the per-token loss and predicted labels\n",
    "valid_set = valid_set.map(\n",
    "    forward_pass_with_label, \n",
    "    batched=True, \n",
    "    batch_size=32\n",
    ")\n",
    "df = valid_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bec1ecf-a30a-40c7-87c6-b7e5dc61e809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 3, -100, 4, 4, 4, -100]</td>\n",
       "      <td>[0.0, 0.011267012, 0.0, 0.03502869, 0.02726594...</td>\n",
       "      <td>[4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, 3, -100, -10...</td>\n",
       "      <td>[0.0, 0.00021062064, 0.0, 0.0, 0.0, 0.0, 0.758...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...</td>\n",
       "      <td>[0.0, 0.00019345796, 0.00011026251, 0.00017391...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 5, -100, 0, 0, -100]</td>\n",
       "      <td>[0.0, 0.0002292132, 0.00016854773, 0.000203112...</td>\n",
       "      <td>[0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...</td>\n",
       "      <td>[0.0, 0.00014995404, 0.000120870915, 0.0001660...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids   \n",
       "0                 [0, 10699, 11, 15, 16104, 1388, 2]  \\\n",
       "1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n",
       "2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n",
       "3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n",
       "4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n",
       "\n",
       "                                      attention_mask   \n",
       "0                              [1, 1, 1, 1, 1, 1, 1]  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels   \n",
       "0                     [-100, 3, -100, 4, 4, 4, -100]  \\\n",
       "1  [-100, 0, -100, -100, -100, -100, 3, -100, -10...   \n",
       "2  [-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...   \n",
       "3               [-100, 0, 0, 0, 5, -100, 0, 0, -100]   \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                                loss   \n",
       "0  [0.0, 0.011267012, 0.0, 0.03502869, 0.02726594...  \\\n",
       "1  [0.0, 0.00021062064, 0.0, 0.0, 0.0, 0.0, 0.758...   \n",
       "2  [0.0, 0.00019345796, 0.00011026251, 0.00017391...   \n",
       "3  [0.0, 0.0002292132, 0.00016854773, 0.000203112...   \n",
       "4  [0.0, 0.00014995404, 0.000120870915, 0.0001660...   \n",
       "\n",
       "                                     predicted_label  \n",
       "0  [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "2  [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e14101e1-6b72-4c78-b216-e37005d66338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n",
       "      <td>[0.0, 0.011267012, 0.0, 0.03502869, 0.02726594...</td>\n",
       "      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[&lt;s&gt;, ▁Ham, a, ▁(, ▁Unternehmen, ▁), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...</td>\n",
       "      <td>[0.0, 0.00021062064, 0.0, 0.0, 0.0, 0.0, 0.758...</td>\n",
       "      <td>[O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG,...</td>\n",
       "      <td>[&lt;s&gt;, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...</td>\n",
       "      <td>[0.0, 0.00019345796, 0.00011026251, 0.00017391...</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, B-LOC, IGN, O, O, IGN]</td>\n",
       "      <td>[0.0, 0.0002292132, 0.00016854773, 0.000203112...</td>\n",
       "      <td>[O, O, O, O, B-LOC, B-LOC, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...</td>\n",
       "      <td>[0.0, 0.00014995404, 0.000120870915, 0.0001660...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...</td>\n",
       "      <td>[&lt;s&gt;, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids   \n",
       "0                 [0, 10699, 11, 15, 16104, 1388, 2]  \\\n",
       "1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n",
       "2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n",
       "3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n",
       "4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n",
       "\n",
       "                                      attention_mask   \n",
       "0                              [1, 1, 1, 1, 1, 1, 1]  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels   \n",
       "0        [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]  \\\n",
       "1  [IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...   \n",
       "2  [IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...   \n",
       "3              [IGN, O, O, O, B-LOC, IGN, O, O, IGN]   \n",
       "4  [IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...   \n",
       "\n",
       "                                                loss   \n",
       "0  [0.0, 0.011267012, 0.0, 0.03502869, 0.02726594...  \\\n",
       "1  [0.0, 0.00021062064, 0.0, 0.0, 0.0, 0.0, 0.758...   \n",
       "2  [0.0, 0.00019345796, 0.00011026251, 0.00017391...   \n",
       "3  [0.0, 0.0002292132, 0.00016854773, 0.000203112...   \n",
       "4  [0.0, 0.00014995404, 0.000120870915, 0.0001660...   \n",
       "\n",
       "                                     predicted_label   \n",
       "0  [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]  \\\n",
       "1  [O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG,...   \n",
       "2     [O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]   \n",
       "3                [O, O, O, O, B-LOC, B-LOC, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...   \n",
       "\n",
       "                                        input_tokens  \n",
       "0         [<s>, ▁Ham, a, ▁(, ▁Unternehmen, ▁), </s>]  \n",
       "1  [<s>, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...  \n",
       "2  [<s>, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...  \n",
       "3    [<s>, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', </s>]  \n",
       "4  [<s>, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag[-100] = \"IGN\"\n",
    "\n",
    "# add a new column,\n",
    "# mapping the input ids back to their corresponding strings\n",
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x)\n",
    ")\n",
    "\n",
    "# transform the column for predicted labels,\n",
    "# mapping the label id back to the corresponding label string\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x]\n",
    ")\n",
    " \n",
    "# transform the column for actual labels,\n",
    "# mapping the label id fback to the corresponding label string\n",
    "df[\"labels\"] = df[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x]\n",
    ")\n",
    "\n",
    "# transform the column for loss\n",
    "# by truncating them to the length of the inputs,\n",
    "# getting rid of the padding\n",
    "df[\"loss\"] = df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1\n",
    ")\n",
    "\n",
    "# transform the column for predicted_label once more\n",
    "# by truncating them to the length of the inputs,\n",
    "# getting rid of the padding\n",
    "df[\"predicted_label\"] = df.apply(\n",
    "    lambda x: x[\"predicted_label\"][:len(x['input_ids'])], axis=1\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79017131-4723-4ffa-a406-778d1ab07624",
   "metadata": {},
   "source": [
    "### ok...\n",
    "\n",
    "Notice how in `df` each column contains a list of tokens, labels, predicted labels, etc. for each sample in the validation set.\n",
    "\n",
    "In order to examine these prediction results with respect to individual tokens we will:\n",
    "\n",
    "1. Unpack the lists in each row with [`pandas.Series.explode`](https://pandas.pydata.org/docs/reference/api/pandas.Series.explode.html#pandas-series-explode), which in our case will work out nicely as each row comprises list of the same length (`input_ids`).\n",
    "1. Drop the `IGN` padding tokens, since their loss is 0 \n",
    "1. Recast the `loss` values from `numpy.Array` to `float`\n",
    "\n",
    "... all in a new, separate DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88152178-1b3a-47fe-9132-bfd463999cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10699</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.04</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16104</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1388</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56530</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.00</td>\n",
       "      <td>O</td>\n",
       "      <td>▁WE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids attention_mask labels  loss predicted_label  input_tokens\n",
       "0     10699              1  B-ORG  0.01           B-ORG          ▁Ham\n",
       "0        15              1  I-ORG  0.04           I-ORG            ▁(\n",
       "0     16104              1  I-ORG  0.03           I-ORG  ▁Unternehmen\n",
       "0      1388              1  I-ORG  0.03           I-ORG            ▁)\n",
       "1     56530              1      O  0.00               O           ▁WE"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = df.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4df785f1-a534-4a79-8c4c-0a416d5f140d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a bit of styling to highlight dataframe rows\n",
    "row_highlight = \"background-color: #C2F3F0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed0f55-b5f0-471c-90d5-1f6dff84a18b",
   "metadata": {},
   "source": [
    "#### Aggregation by token\n",
    "\n",
    "Grouping by `input_tokens` value, we can aggregate the `count`, `mean`, and `sum` of the respective losses using `df_tokens`, and sort by `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f570ba38-eae5-4ed2-aca7-f6b2ac10d3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ce93b_row3_col0, #T_ce93b_row3_col1, #T_ce93b_row3_col2, #T_ce93b_row3_col3, #T_ce93b_row3_col4, #T_ce93b_row3_col5, #T_ce93b_row3_col6, #T_ce93b_row3_col7, #T_ce93b_row3_col8, #T_ce93b_row3_col9 {\n",
       "  background-color: #C2F3F0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ce93b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ce93b_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_ce93b_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_ce93b_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_ce93b_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_ce93b_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_ce93b_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_ce93b_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_ce93b_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_ce93b_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_ce93b_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ce93b_level0_row0\" class=\"row_heading level0 row0\" >input_tokens</th>\n",
       "      <td id=\"T_ce93b_row0_col0\" class=\"data row0 col0\" >▁</td>\n",
       "      <td id=\"T_ce93b_row0_col1\" class=\"data row0 col1\" >▁in</td>\n",
       "      <td id=\"T_ce93b_row0_col2\" class=\"data row0 col2\" >▁von</td>\n",
       "      <td id=\"T_ce93b_row0_col3\" class=\"data row0 col3\" >▁der</td>\n",
       "      <td id=\"T_ce93b_row0_col4\" class=\"data row0 col4\" >▁/</td>\n",
       "      <td id=\"T_ce93b_row0_col5\" class=\"data row0 col5\" >▁und</td>\n",
       "      <td id=\"T_ce93b_row0_col6\" class=\"data row0 col6\" >▁(</td>\n",
       "      <td id=\"T_ce93b_row0_col7\" class=\"data row0 col7\" >▁''</td>\n",
       "      <td id=\"T_ce93b_row0_col8\" class=\"data row0 col8\" >▁)</td>\n",
       "      <td id=\"T_ce93b_row0_col9\" class=\"data row0 col9\" >▁A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce93b_level0_row1\" class=\"row_heading level0 row1\" >count</th>\n",
       "      <td id=\"T_ce93b_row1_col0\" class=\"data row1 col0\" >6066</td>\n",
       "      <td id=\"T_ce93b_row1_col1\" class=\"data row1 col1\" >989</td>\n",
       "      <td id=\"T_ce93b_row1_col2\" class=\"data row1 col2\" >808</td>\n",
       "      <td id=\"T_ce93b_row1_col3\" class=\"data row1 col3\" >1388</td>\n",
       "      <td id=\"T_ce93b_row1_col4\" class=\"data row1 col4\" >163</td>\n",
       "      <td id=\"T_ce93b_row1_col5\" class=\"data row1 col5\" >1171</td>\n",
       "      <td id=\"T_ce93b_row1_col6\" class=\"data row1 col6\" >246</td>\n",
       "      <td id=\"T_ce93b_row1_col7\" class=\"data row1 col7\" >2898</td>\n",
       "      <td id=\"T_ce93b_row1_col8\" class=\"data row1 col8\" >246</td>\n",
       "      <td id=\"T_ce93b_row1_col9\" class=\"data row1 col9\" >125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce93b_level0_row2\" class=\"row_heading level0 row2\" >mean</th>\n",
       "      <td id=\"T_ce93b_row2_col0\" class=\"data row2 col0\" >0.040000</td>\n",
       "      <td id=\"T_ce93b_row2_col1\" class=\"data row2 col1\" >0.130000</td>\n",
       "      <td id=\"T_ce93b_row2_col2\" class=\"data row2 col2\" >0.160000</td>\n",
       "      <td id=\"T_ce93b_row2_col3\" class=\"data row2 col3\" >0.090000</td>\n",
       "      <td id=\"T_ce93b_row2_col4\" class=\"data row2 col4\" >0.580000</td>\n",
       "      <td id=\"T_ce93b_row2_col5\" class=\"data row2 col5\" >0.070000</td>\n",
       "      <td id=\"T_ce93b_row2_col6\" class=\"data row2 col6\" >0.280000</td>\n",
       "      <td id=\"T_ce93b_row2_col7\" class=\"data row2 col7\" >0.020000</td>\n",
       "      <td id=\"T_ce93b_row2_col8\" class=\"data row2 col8\" >0.270000</td>\n",
       "      <td id=\"T_ce93b_row2_col9\" class=\"data row2 col9\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce93b_level0_row3\" class=\"row_heading level0 row3\" >sum</th>\n",
       "      <td id=\"T_ce93b_row3_col0\" class=\"data row3 col0\" >217.180000</td>\n",
       "      <td id=\"T_ce93b_row3_col1\" class=\"data row3 col1\" >129.630000</td>\n",
       "      <td id=\"T_ce93b_row3_col2\" class=\"data row3 col2\" >126.040000</td>\n",
       "      <td id=\"T_ce93b_row3_col3\" class=\"data row3 col3\" >122.360000</td>\n",
       "      <td id=\"T_ce93b_row3_col4\" class=\"data row3 col4\" >94.920000</td>\n",
       "      <td id=\"T_ce93b_row3_col5\" class=\"data row3 col5\" >86.880000</td>\n",
       "      <td id=\"T_ce93b_row3_col6\" class=\"data row3 col6\" >69.930000</td>\n",
       "      <td id=\"T_ce93b_row3_col7\" class=\"data row3 col7\" >68.330000</td>\n",
       "      <td id=\"T_ce93b_row3_col8\" class=\"data row3 col8\" >67.030000</td>\n",
       "      <td id=\"T_ce93b_row3_col9\" class=\"data row3 col9\" >55.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1374c5d550>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "        .agg([\"count\", \"mean\", \"sum\"])\n",
    "        .droplevel(level=0, axis=1)    # get rid of multi-level columns\n",
    "        .sort_values(by=\"sum\", ascending=False)\n",
    "        .reset_index()\n",
    "        .round(2)\n",
    "        .head(10)\n",
    "        .T\n",
    ").style.apply(\n",
    "    lambda row: pd.Series(row_highlight, row.index) if row.name==\"sum\" else pd.Series('', row.index),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb33691-3aba-45ab-8937-0003e92d9d11",
   "metadata": {},
   "source": [
    "Observations: \n",
    "\n",
    "* Whitespace char `▁` has the highest total loss, but since its mean loss is lower than the other tokens' loss, the fine-tuned model does not have a problem in classifying them.\n",
    "* \"in\", \"von\", \"der\" and \"und\" often appear with named entities or actually are a part of said named entity, which is why our model may be mixing them up.\n",
    "* Parentheses, slashes, quotation marks, etc. at the beginning of words are rarer, but have a relatively high average loss. _What could explain this?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495e9e5-0f92-4a83-be91-7778c5245832",
   "metadata": {},
   "source": [
    "#### Aggregation by Named Entity class (type)\n",
    "\n",
    "Grouping next by named entity type label, we can likewise calculate `count`, `mean`, and `sum`, and sort by `mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c1ba700-16f9-4974-83c8-b688ab94c36c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6550e_row2_col0, #T_6550e_row2_col1, #T_6550e_row2_col2, #T_6550e_row2_col3, #T_6550e_row2_col4, #T_6550e_row2_col5, #T_6550e_row2_col6 {\n",
       "  background-color: #C2F3F0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6550e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6550e_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_6550e_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_6550e_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_6550e_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_6550e_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_6550e_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_6550e_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6550e_level0_row0\" class=\"row_heading level0 row0\" >labels</th>\n",
       "      <td id=\"T_6550e_row0_col0\" class=\"data row0 col0\" >B-ORG</td>\n",
       "      <td id=\"T_6550e_row0_col1\" class=\"data row0 col1\" >I-LOC</td>\n",
       "      <td id=\"T_6550e_row0_col2\" class=\"data row0 col2\" >I-ORG</td>\n",
       "      <td id=\"T_6550e_row0_col3\" class=\"data row0 col3\" >B-LOC</td>\n",
       "      <td id=\"T_6550e_row0_col4\" class=\"data row0 col4\" >B-PER</td>\n",
       "      <td id=\"T_6550e_row0_col5\" class=\"data row0 col5\" >I-PER</td>\n",
       "      <td id=\"T_6550e_row0_col6\" class=\"data row0 col6\" >O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6550e_level0_row1\" class=\"row_heading level0 row1\" >count</th>\n",
       "      <td id=\"T_6550e_row1_col0\" class=\"data row1 col0\" >2683</td>\n",
       "      <td id=\"T_6550e_row1_col1\" class=\"data row1 col1\" >1462</td>\n",
       "      <td id=\"T_6550e_row1_col2\" class=\"data row1 col2\" >3820</td>\n",
       "      <td id=\"T_6550e_row1_col3\" class=\"data row1 col3\" >3172</td>\n",
       "      <td id=\"T_6550e_row1_col4\" class=\"data row1 col4\" >2893</td>\n",
       "      <td id=\"T_6550e_row1_col5\" class=\"data row1 col5\" >4139</td>\n",
       "      <td id=\"T_6550e_row1_col6\" class=\"data row1 col6\" >43648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6550e_level0_row2\" class=\"row_heading level0 row2\" >mean</th>\n",
       "      <td id=\"T_6550e_row2_col0\" class=\"data row2 col0\" >0.640000</td>\n",
       "      <td id=\"T_6550e_row2_col1\" class=\"data row2 col1\" >0.600000</td>\n",
       "      <td id=\"T_6550e_row2_col2\" class=\"data row2 col2\" >0.480000</td>\n",
       "      <td id=\"T_6550e_row2_col3\" class=\"data row2 col3\" >0.330000</td>\n",
       "      <td id=\"T_6550e_row2_col4\" class=\"data row2 col4\" >0.280000</td>\n",
       "      <td id=\"T_6550e_row2_col5\" class=\"data row2 col5\" >0.180000</td>\n",
       "      <td id=\"T_6550e_row2_col6\" class=\"data row2 col6\" >0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6550e_level0_row3\" class=\"row_heading level0 row3\" >sum</th>\n",
       "      <td id=\"T_6550e_row3_col0\" class=\"data row3 col0\" >1712.620000</td>\n",
       "      <td id=\"T_6550e_row3_col1\" class=\"data row3 col1\" >874.230000</td>\n",
       "      <td id=\"T_6550e_row3_col2\" class=\"data row3 col2\" >1839.890000</td>\n",
       "      <td id=\"T_6550e_row3_col3\" class=\"data row3 col3\" >1058.450000</td>\n",
       "      <td id=\"T_6550e_row3_col4\" class=\"data row3 col4\" >797.830000</td>\n",
       "      <td id=\"T_6550e_row3_col5\" class=\"data row3 col5\" >728.210000</td>\n",
       "      <td id=\"T_6550e_row3_col6\" class=\"data row3 col6\" >1343.230000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1374c5da30>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]]\n",
    "        .agg([\"count\", \"mean\", \"sum\"])\n",
    "        .droplevel(level=0, axis=1)\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "        .reset_index()\n",
    "        .round(2)\n",
    "        .T\n",
    ").style.apply(\n",
    "    lambda row: pd.Series(row_highlight, row.index) if row.name==\"mean\" else pd.Series('', row.index),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb4e21-3f94-43c2-af45-83bec39fbbc5",
   "metadata": {},
   "source": [
    "Seeing that `B-ORG` has the highest average loss, we can surmise that the fine-tuned model may have problems in determining the start of an `ORG`.\n",
    "\n",
    "This would be a good place to stop and next look at the confusion matrix for named entity labels (types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c384818-b702-472e-a1e9-07ae3e50c62f",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "2 errors in the code snippet listed on page 112:\n",
    "* missing the `import matplotlib.pyplot as plt` bit\n",
    "* the order of `y_preds` and `y_true` reversed in the arguments when invoking `plot_confusion_matrix`?!\n",
    "\n",
    "Please refer to [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn-metrics-confusion-matrix); note the order of arguments in the method signature.\n",
    "\n",
    "    sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)[source]\n",
    "\n",
    "Corrected below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a08d9323-5507-4046-b646-3704730d6b64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_preds, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    df_tokens[\"labels\"],\n",
    "    df_tokens[\"predicted_label\"],\n",
    "    tags.names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0d36c-3f9e-42cd-97eb-2b81f5094147",
   "metadata": {},
   "source": [
    "If this understanding is correct, we can reach these conclusions:\n",
    "\n",
    "1. `B-PER` is most often mistaken for `O` at 0.07\n",
    "1. `B-PER` is also often mistaken for `I-LOC` at 0.07\n",
    "1. `B-ORG` is most often mistaken for `I-ORG` at 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27e111d5-a0d0-49b3-ab6a-0032fe1b917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"preds\": preds,\n",
    "            \"losses\": losses\n",
    "        }).T\n",
    "        yield df_tmp\n",
    "\n",
    "\n",
    "df[\"total_loss\"] = df[\"loss\"].apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0492fc9d-49a0-4024-b8f4-067b8e9ef58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁Τ</td>\n",
       "      <td>Κ</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁T</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ri</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>k</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ala</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.56</td>\n",
       "      <td>9.98</td>\n",
       "      <td>8.04</td>\n",
       "      <td>7.95</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.08</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2      3     4     5      6      7      8      9    \n",
       "tokens    ▁'   ▁''     ▁Τ      Κ   ▁''    ▁'     ▁'    ▁''     ▁T    ▁''  \\\n",
       "labels     O     O      O    IGN     O     O  B-LOC  I-LOC  I-LOC  I-LOC   \n",
       "preds      O     O  B-ORG  I-ORG     O     O      O      O      O      O   \n",
       "losses  0.00  0.00   4.65   0.00  0.00  0.00  10.56   9.98   8.04   7.95   \n",
       "\n",
       "           10     11     12     13    14     15     16    17    18  \n",
       "tokens     ▁'     ri    ▁''     ▁'     k    ▁''     ▁'   ala  </s>  \n",
       "labels  I-LOC    IGN  I-LOC  I-LOC   IGN  I-LOC  I-LOC   IGN   IGN  \n",
       "preds       O  B-ORG      O      O     O      O      O     O     O  \n",
       "losses   7.88   0.00   7.92   8.69  0.00   9.08   9.31  0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁''</td>\n",
       "      <td>8</td>\n",
       "      <td>.</td>\n",
       "      <td>▁Juli</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Protest</td>\n",
       "      <td>camp</td>\n",
       "      <td>▁auf</td>\n",
       "      <td>▁dem</td>\n",
       "      <td>▁Gelände</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Republika</td>\n",
       "      <td>n</td>\n",
       "      <td>ischen</td>\n",
       "      <td>▁Gar</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>8.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.04</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.18</td>\n",
       "      <td>9.04</td>\n",
       "      <td>7.54</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2      3      4      5         6     7      8      9    \n",
       "tokens    ▁''     8     .  ▁Juli    ▁''     ▁:  ▁Protest  camp   ▁auf   ▁dem  \\\n",
       "labels  B-ORG   IGN   IGN  I-ORG  I-ORG  I-ORG     I-ORG   IGN  I-ORG  I-ORG   \n",
       "preds       O     O     O      O      O      O         O     O      O      O   \n",
       "losses   8.33  0.00  0.00   5.37   8.84   9.04      6.73  0.00   8.18   9.04   \n",
       "\n",
       "              10     11          12     13      14     15     16    17  \n",
       "tokens  ▁Gelände   ▁der  ▁Republika      n  ischen   ▁Gar     de  </s>  \n",
       "labels     I-ORG  I-ORG       I-ORG    IGN     IGN  I-ORG    IGN   IGN  \n",
       "preds          O      O       B-ORG  I-ORG   I-ORG  I-ORG  I-ORG     O  \n",
       "losses      7.54   5.52        5.58   0.00    0.00   0.01   0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁United</td>\n",
       "      <td>▁Nations</td>\n",
       "      <td>▁Multi</td>\n",
       "      <td>dimensional</td>\n",
       "      <td>▁Integra</td>\n",
       "      <td>ted</td>\n",
       "      <td>▁Stabil</td>\n",
       "      <td>ization</td>\n",
       "      <td>▁Mission</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁Central</td>\n",
       "      <td>▁African</td>\n",
       "      <td>▁Republic</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>6.42</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.57</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.26</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1       2            3         4      5        6    \n",
       "tokens  ▁United  ▁Nations  ▁Multi  dimensional  ▁Integra    ted  ▁Stabil  \\\n",
       "labels    B-PER     I-PER   I-PER          IGN     I-PER    IGN    I-PER   \n",
       "preds     B-ORG     I-ORG   I-ORG        I-ORG     I-ORG  I-ORG    I-ORG   \n",
       "losses     6.42      5.25    5.82         0.00      5.45   0.00     5.54   \n",
       "\n",
       "             7         8      9      10        11        12         13     14  \n",
       "tokens  ization  ▁Mission    ▁in   ▁the  ▁Central  ▁African  ▁Republic   </s>  \n",
       "labels      IGN     I-PER  I-PER  I-PER     I-PER     I-PER      I-PER    IGN  \n",
       "preds     I-ORG     I-ORG  I-ORG  I-ORG     I-ORG     I-ORG      I-ORG  I-ORG  \n",
       "losses     0.00      5.24   5.57   5.41      5.49      5.26       5.20   0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e42ecd-028e-41c3-8865-46a9ef7e8074",
   "metadata": {},
   "source": [
    "#### A detail regarding parentheses, slashes being part of the named entities...\n",
    "\n",
    "Recall earlier during the exercise where we were aggregating by tokens, and we had the following observation.\n",
    "\n",
    "> (parentheses, slashes)... at the beginning of words are rarer, but have a relatively high average loss. What could explain this?\n",
    "\n",
    "Let's take a closer look at some of the sequences where there are opening parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7a25a9c-05ce-468d-b730-cbb5fa38c076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Ham</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2             3      4      5\n",
       "tokens   ▁Ham      a     ▁(  ▁Unternehmen     ▁)   </s>\n",
       "labels  B-ORG    IGN  I-ORG         I-ORG  I-ORG    IGN\n",
       "preds   B-ORG  I-ORG  I-ORG         I-ORG  I-ORG  I-ORG\n",
       "losses   0.01   0.00   0.04          0.03   0.03   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Kesk</td>\n",
       "      <td>kül</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Mart</td>\n",
       "      <td>na</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7\n",
       "tokens  ▁Kesk    kül      a     ▁(  ▁Mart     na     ▁)   </s>\n",
       "labels  B-LOC    IGN    IGN  I-LOC  I-LOC    IGN  I-LOC    IGN\n",
       "preds   B-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC\n",
       "losses   0.03   0.00   0.00   0.02   0.02   0.00   0.02   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d071463-9a5d-4e57-b2b3-b457d9a36eb2",
   "metadata": {},
   "source": [
    "So apparently, this Pan-X dataset was generated automatically rather than by-hand, so it is _not_ \"gold-standard\": it is <u>\"silver-standard\"</u>.\n",
    "\n",
    "In labelling named entities automatically, it seems that parentheses and slashes, etc. are included in the marked up labelling for named entities. This detail may be important if we had to take this model to production, _so always pay attention to the details!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724becf2-d578-462e-80c1-e3dcba31b13b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefd73f-b78b-4b0d-804c-72fc19891d37",
   "metadata": {},
   "source": [
    "## Cross-Lingual Transfer\n",
    "\n",
    "Now let's see how our `de`-trained model fares against `fr`, `it`, and `en` without any sort of additional training. This is cross-lingual transfer.\n",
    "\n",
    "First we set up a simple function that will help us obtain the $F_{1}$ score given the `trainer` object instance and a certain `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37d786a9-8976-4fe9-bb01-3326ff78f50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_f1_score(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06169015-f448-465c-ad3a-761cc8ce8178",
   "metadata": {},
   "source": [
    "We'll keep the scores in a `dict` of `dict`s..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "276c4221-60e4-4eec-a77e-15a4f5c23b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_scores = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5776bba-4ba8-47bb-aacf-99d6863409b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [de] dataset: 0.865\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\n",
    "print(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da414a7-826d-4f58-a7d0-d64058f25fe9",
   "metadata": {},
   "source": [
    "The above $F_{1}$ score should be greater than 0.86 on `de`, coming pretty close to our expectations.\n",
    "\n",
    "But how will this `de`-trained model fare when French is passed in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0fce2e4c-5de2-44df-ba05-7a58043bf2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁est</td>\n",
       "      <td>▁informatic</td>\n",
       "      <td>ien</td>\n",
       "      <td>▁chez</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁en</td>\n",
       "      <td>▁Cali</td>\n",
       "      <td>for</td>\n",
       "      <td>nie</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4            5    6      7        8    9    \n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁est  ▁informatic  ien  ▁chez  ▁Google  ▁en  \\\n",
       "Tags      O  B-PER  I-PER  I-PER     O            O    O      O    B-ORG    O   \n",
       "\n",
       "           10     11     12    13  \n",
       "Tokens  ▁Cali    for    nie  </s>  \n",
       "Tags    B-LOC  I-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd8d86-9f4e-404c-8cb1-59aa5a575a55",
   "metadata": {},
   "source": [
    "Not all that bad, _n'est-ce pas?_\n",
    "\n",
    "Let's check the $F_{1}$ scores of our `de`-trained model against the other languages in `panx_ch`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e98b43a4-74a1-4eb8-8b22-abf55a927842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_ds = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1_score(trainer, panx_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d855d5fb-93c3-4279-8d46-386569327193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-6c4cabcf84ff0cc4.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-d16c261954fe045d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [fr] dataset: 0.703\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\n",
    "print(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d899aaf6-5758-4b43-8c22-b5b779f6bd80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-8d5888eaf00780c1.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-103c7716b0ef22d9.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c3a7fe2a53f9ca05.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [it] dataset: 0.667\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\n",
    "print(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebb1ca61-b888-46e4-b232-3279c52aa0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e0b8b48ffbfaa6b6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-54a01e4995248865.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b450ad85f672632a.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [en] dataset: 0.597\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\n",
    "print(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c388f-322e-46e6-af43-a8d0cb14ce36",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "* Any difference in $F_{1}$ performance can be attributed to how close the other languages are to `de`.\n",
    "* But again, another factor is the quality of the training data.\n",
    "* It is somewhat suprising that the cross-lingual transfer of `de` $\\Rightarrow$ `en` appears to be worse than `de` $\\Rightarrow$ `fr` as we know that German and English are closer cousins than German is with relation to French."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264829ac-3949-45ad-99e0-1712904eeeab",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb077ed-ab80-4fb6-acec-e082aa5dd0bc",
   "metadata": {},
   "source": [
    "## When Does Zero-Shot Transfer Make Sense?\n",
    "\n",
    "We've seen how the `de`-trained model can still perform suprisingly well on the other languages _without any further training._ \n",
    "\n",
    "Note that in our experiment above, our zero-shot transfer using a `de`-trained model on the `fr` dataset yielded an $F_{1}$ score of around 0.70.\n",
    "\n",
    "But we surmise that if we were willing expend the effort (time and money!) to train a model using `fr` samples, we might see better results for `fr`. The question is, then: at what point (number of samples) we start seeing appreciable returns? And since data labelling it very expensive, would we really be willing to pay that price?\n",
    "\n",
    "Let's just see the trends for $F_{1}$ scores as we vary the number of training data samples in training up an `fr` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11ad9ac7-18da-4c3a-80ca-909e4ff757cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    \n",
    "    train_ds = (\n",
    "        dataset[\"train\"]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(num_samples))\n",
    "    )\n",
    "    valid_ds = dataset[\"validation\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "    \n",
    "    training_args.logging_steps = len(train_ds) // batch_size\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        tokenizer=xlmr_tokenizer\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub(commit_message=\"training all pau!\")\n",
    "    \n",
    "    f1_score = get_f1_score(trainer, test_ds)\n",
    "    return pd.DataFrame.from_dict({\n",
    "        \"num_samples\": [len(train_ds)],\n",
    "        \"f1_score\": [f1_score],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b76083e-e5f7-4684-bd49-e85ff9f38857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-eca751a64e5439e6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-52c67904dd1b71ec.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a7441aba5f1365df.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c3607ef2-70ce-47cc-91ee-b3eaea1ad6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.765700</td>\n",
       "      <td>1.309182</td>\n",
       "      <td>0.131131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.265600</td>\n",
       "      <td>1.145394</td>\n",
       "      <td>0.164823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.085700</td>\n",
       "      <td>1.052422</td>\n",
       "      <td>0.161115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.152116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  f1_score\n",
       "0          250  0.152116"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.push_to_hub = False\n",
    "metrics_df = train_on_subset(panx_fr_encoded, 250)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f75c9-aa63-4c97-a89c-67f28310649d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b6b8b16-4dbb-49c2-9ea1-82b452a45564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.509000</td>\n",
       "      <td>1.112905</td>\n",
       "      <td>0.165627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.698133</td>\n",
       "      <td>0.618205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.567214</td>\n",
       "      <td>0.652645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.157300</td>\n",
       "      <td>0.603568</td>\n",
       "      <td>0.591150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.421980</td>\n",
       "      <td>0.716866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.383610</td>\n",
       "      <td>0.772916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 01:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>0.423356</td>\n",
       "      <td>0.715593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.370570</td>\n",
       "      <td>0.761920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.338185</td>\n",
       "      <td>0.807333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 02:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.787345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.287519</td>\n",
       "      <td>0.822433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.281576</td>\n",
       "      <td>0.839764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = [metrics_df]\n",
    "\n",
    "for num_samples in [500, 1000, 2000, 4000]:\n",
    "    acc.append(train_on_subset(panx_fr_encoded, num_samples))\n",
    "    \n",
    "metrics_df = pd.concat(acc, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9cb09ec-ed0e-4310-b13d-16d9fc7fb7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
    "plt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae0f42-59c1-4e00-84b1-b0bab4dd0494",
   "metadata": {},
   "source": [
    "* So for the case of `fr`, zero-shot transfer is comparable to an `fr` fine-tuned model up until about 750 or so training samples. \n",
    "* You need to ask yourself, \"Do you really need to spend the time and effort to obtain 750+ labelled training samples for `fr`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0d15d-8376-4cef-a17d-2584442cdfc8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398aab8c-c842-4946-a158-3a4777c300a4",
   "metadata": {},
   "source": [
    "## Fine-Tuning on Multiple Languages at Once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433a8cd-2771-450e-a7b9-237d97edd040",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4047c205-c0bf-41b3-a51c-9d43b5640bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "140e43b1-470c-4457-b39a-1bef7b36448f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concatenate_splits(corpora):\n",
    "    multi_corpus = DatasetDict()\n",
    "    for split in corpora[0].keys():\n",
    "        multi_corpus[split] = concatenate_datasets(\n",
    "            [corpus[split] for corpus in corpora]\n",
    "        ).shuffle(seed=42)\n",
    "    return multi_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "751adc5b-6da4-4cb8-984f-0849d010282d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c0a1ac2eccccf26b.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-45f696cf7e548ca8.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c2c8c25ff90d7eb9.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8580\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8580\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])\n",
    "panx_de_fr_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5724c81-e154-455b-83d1-f27c09db68fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-de-fr is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de-fr. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2145' max='2145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2145/2145 12:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>0.189646</td>\n",
       "      <td>0.830810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.161192</td>\n",
       "      <td>0.849928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.166642</td>\n",
       "      <td>0.857787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de-fr\n",
      "   85e4405..961b21d  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-de-fr/commit/961b21de4f3f9716ddf6997ed173f3303fe8d86e'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\n",
    "training_args.push_to_hub = True\n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer,\n",
    "    train_dataset=panx_de_fr_encoded[\"train\"],\n",
    "    eval_dataset=panx_de_fr_encoded[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training all pau!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7792caac-4651-44af-93e2-04f727130cef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c0d2a27e80bb06fe.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-3393251654418133.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-9a19564b783904af.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-eca751a64e5439e6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-52c67904dd1b71ec.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a7441aba5f1365df.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [de] dataset: 0.870\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-8d5888eaf00780c1.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-103c7716b0ef22d9.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c3a7fe2a53f9ca05.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [fr] dataset: 0.853\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e0b8b48ffbfaa6b6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-54a01e4995248865.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b450ad85f672632a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [it] dataset: 0.798\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [en] dataset: 0.689\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    f1 = evaluate_lang_performance(lang, trainer)\n",
    "    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8afb3-4166-427f-9945-ee8073bb2add",
   "metadata": {},
   "source": [
    "Note that we see a roughly 10-point increase in the $F_{1}$ scores for `it` and `en`! And just by additional training on `fr` along with `de`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74765916-aabb-4431-8846-0f55554c5ba7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49144585-4f91-44bb-a7a6-67b3a273f01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpora = [panx_de_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34d66fb4-27c6-45cf-b140-50b84b1b60ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-eca751a64e5439e6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-52c67904dd1b71ec.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a7441aba5f1365df.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-4ebe059e251bb155.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-fr is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-fr. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='573' max='573' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [573/573 02:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.317975</td>\n",
       "      <td>0.792572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.288131</td>\n",
       "      <td>0.822204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>0.836247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-fr\n",
      "   3cf4842..5d3e0b4  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-8d5888eaf00780c1.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-103c7716b0ef22d9.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c3a7fe2a53f9ca05.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c998bcefe7ba5d59.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-it is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-it. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.365487</td>\n",
       "      <td>0.704303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.287137</td>\n",
       "      <td>0.786753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.269990</td>\n",
       "      <td>0.819376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-it\n",
      "   f07b3e3..7539e78  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e0b8b48ffbfaa6b6.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-54a01e4995248865.arrow\n",
      "Loading cached processed dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b450ad85f672632a.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-db77554eceebcebc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-en is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-en. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.213600</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>0.488360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.449060</td>\n",
       "      <td>0.635710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.428402</td>\n",
       "      <td>0.677834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-en\n",
      "   70c9c35..632b21d  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# excluding de...\n",
    "for lang in langs[1:]:\n",
    "    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n",
    "    \n",
    "    # fine-tune on this monolingual dataset\n",
    "    ds_encoded = encode_panx_dataset(panx_ch[lang])\n",
    "    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n",
    "    \n",
    "    # collect F1-score for this trained model\n",
    "    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n",
    "    \n",
    "    # add this monolingual corpus to our list of corpora to cat\n",
    "    corpora.append(ds_encoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5acacfea-08b3-402a-a256-d5376995d488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-69f295a616c5641f.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-060c38584ea96d2a.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kashiwapoodle/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-9f5804e0e43943b6.arrow\n"
     ]
    }
   ],
   "source": [
    "corpora_encoded = concatenate_splits(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5d2fd60-d728-464c-890e-ef9a76e71cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kashiwapoodle/dev/github/transformers-gcp/xlm-roberta-base-finetuned-panx-all is already a clone of https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-all. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2505' max='2505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2505/2505 13:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.193404</td>\n",
       "      <td>0.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.176310</td>\n",
       "      <td>0.844158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>0.856264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-all\n",
      "   b79e212..095ee1b  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/buruzaemon/xlm-roberta-base-finetuned-panx-all/commit/095ee1b60e7ddf9776a4b16a178afc218a742c1e'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer,\n",
    "    train_dataset=corpora_encoded[\"train\"],\n",
    "    eval_dataset=corpora_encoded[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Panx-All training all pau!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "31db9538-ac60-485a-92d8-69661b093da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated on</th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tune on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>0.5969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.8168</td>\n",
       "      <td>0.6893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.8668</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.7601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated on      de      fr      it      en\n",
       "Fine-tune on                                \n",
       "de            0.8655  0.7028  0.6672  0.5969\n",
       "each          0.8655  0.8401  0.8168  0.6893\n",
       "all           0.8668  0.8678  0.8723  0.7601"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx,lang in enumerate(langs):\n",
    "    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n",
    "\n",
    "scores_data = {\n",
    "    \"de\": f1_scores[\"de\"],\n",
    "    \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n",
    "    \"all\": f1_scores[\"all\"]\n",
    "}\n",
    "\n",
    "f1_scores_df = pd.DataFrame(scores_data).T.round(4)\n",
    "f1_scores_df.rename_axis(\n",
    "    index=\"Fine-tune on\",\n",
    "    columns=\"Evaluated on\",\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0129d8-602e-46e4-a8b0-da475806f1c9",
   "metadata": {},
   "source": [
    "> Multilingual learning can provide significant gains in performance, especially if\n",
    "> the low-resource languages for cross-lingual transfer belong to similar language\n",
    "> families.\n",
    ">\n",
    "> ... We can see that German, French, and Italian achieve similar performance\n",
    "> in the `all` category, suggesting that these languages are more similar\n",
    "> to each other than than (they are) to English.\n",
    ">\n",
    "> ... it is a good idea to focus attention on cross-lingual transfer within \n",
    "> language families, especially when dealing with different scripts like\n",
    "> Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe15b99-af0d-426a-a263-3b0200ce6aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6749160b-f0a2-4779-a299-b6574edbd02b",
   "metadata": {},
   "source": [
    "# Chapter 5: Text Generation\n",
    "\n",
    "<!--\n",
    "hello, world.\n",
    "//-->\n",
    "<!--script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full\" type=\"text/javascript\"></script-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f1a2c-9be5-464d-9c90-83e5b9125b7b",
   "metadata": {},
   "source": [
    "## Greedy Search Decoding\n",
    "\n",
    "The exercise in the book uses [GPT-2 XL](https://huggingface.co/gpt2-xl).\n",
    "\n",
    "> GPT-2 XL is <span style=\"background-color: #9AFEFF\">the 1.5B parameter version of GPT-2</span>, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "You might want to read the `transformers` v4.28.1 documentation on:\n",
    "\n",
    "* [`transformers.AutoTokenizer`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer) ... generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained()`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) class method.\n",
    "* [`transformers.AutoModelForCausalLM`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForCausalLM) ... generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`from_pretrained()`](hhttps://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) class method or the [`from_config()`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForCausalLM.from_config) class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e27986-3b3d-418b-91e3-0f558b4028d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a279fb-295b-4dde-ac36-0bf912438f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'gpt2-xl'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e7285-f147-4e3a-b823-b70105335689",
   "metadata": {},
   "source": [
    "#### NOTE!\n",
    "\n",
    "* Using `huggingface-cli`, [`scan`](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#scan-your-cache) your cache and confirm that the GPT-2 XL model we just downloaded takes up ~6.4G of space.\n",
    "* Looking up the files at [Files and versions tab for `gpt2-xl` on HF](https://huggingface.co/gpt2-xl/tree/main), see how `pytorch_model.bin`is 6.43G is size!\n",
    "* You may want to [`clear`](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#clean-your-cache) your HF cache-system as needed with `huggingface-cli`.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244d2cf-9ef7-49f7-9426-72506f156ade",
   "metadata": {},
   "source": [
    "### Naive implementation of greedy decoding method\n",
    "\n",
    "> _The simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probability at each timestep:_\n",
    "> \n",
    "> $ \\hat{y}_{t} =\\underset{y_t}{\\operatorname{argmax}}{P}\\left( y_{t} | y_{<t} , x \\right) $\n",
    "\n",
    "\n",
    "We implement the decoding method of this autoregressive model so that we can learn how things are done under the hood.\n",
    "\n",
    "Please see:\n",
    "* [`torch.softmax`]() ... Alias for [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax); applies `softmax` function to all slices along dim, and will re-scale them so that the elements lie in the range $[0, 1]$ and sum to $1$.\n",
    "* [`torch.argsort`](https://pytorch.org/docs/1.11/generated/torch.argsort.html?highlight=argsort#torch.argsort) ... returns the indices that sort a tensor along a given dimension in ascending order by value.\n",
    "* [`torch.cat`]() ... 'catenates the given sequence of `seq` tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty... can be seen as an inverse operation for [`torch.split`](https://pytorch.org/docs/stable/generated/torch.split.html#torch.split) and [`torch.chunk`](https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk).\n",
    "\n",
    "Here is our naive implementation of greedy decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acd0010-7816-4276-b816-76265b0ec689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice for 1</th>\n",
       "      <th>Choice for 2</th>\n",
       "      <th>Choice for 3</th>\n",
       "      <th>Choice for 4</th>\n",
       "      <th>Choice for 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input       Choice for 1   \n",
       "0                               Transformers are the       most (8.53%)  \\\n",
       "1                          Transformers are the most   popular (16.78%)   \n",
       "2                  Transformers are the most popular       toy (10.63%)   \n",
       "3              Transformers are the most popular toy      line (34.38%)   \n",
       "4         Transformers are the most popular toy line        in (46.28%)   \n",
       "5      Transformers are the most popular toy line in       the (65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
       "\n",
       "        Choice for 2           Choice for 3           Choice for 4   \n",
       "0       only (4.96%)           best (4.65%)   Transformers (4.37%)  \\\n",
       "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "\n",
       "          Choice for 5  \n",
       "0     ultimate (2.16%)  \n",
       "1   successful (3.20%)  \n",
       "2          and (3.76%)  \n",
       "3         line (2.69%)  \n",
       "4         ever (2.72%)  \n",
       "5        North (1.40%)  \n",
       "6            U (2.30%)  \n",
       "7        today (1.74%)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(\n",
    "            input_ids=input_ids\n",
    "        )\n",
    "        \n",
    "        # select logits of the 1st batch and the last token,\n",
    "        # and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # store tokens with the highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            # don't forget to move off GPU and back to CPU!\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100*token_prob:.2F}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice for {choice_idx+1}\"] = token_choice\n",
    "        \n",
    "        # cat the predicted next token on to the input,\n",
    "        # and prepare to do it again!\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb74365-e2ad-4dfa-9e56-fc8945ad00da",
   "metadata": {},
   "source": [
    "See the HF documenation on [Generation (Text Generation)](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e46526-5fb1-4240-bb6c-19e66e37a047",
   "metadata": {},
   "source": [
    "... and here's the equivalent using `generate`...\n",
    "\n",
    "#### IMPORTANT NOTE\n",
    "\n",
    "In the book, no attention mask is passed in to `generate` along with `input_ids`, resulting in a warning that looks like this:\n",
    "> <span style=\"background-color: #ffe0d9\">_The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation._</span>\n",
    "\n",
    "Ignoring that bit about `pad_token_id` and `eos_token_id`, ... sgugger's answer to the question [Do automatically generated attention masks ignore padding?](https://discuss.huggingface.co/t/do-automatically-generated-attention-masks-ignore-padding/15479/2) states:\n",
    "> _Yes, you need to pass the attention mask returned by the tokenizer. Most models don’t know the padding token ID, so they can’t generate an attention mask that ignores it._\n",
    "\n",
    "So let's do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ad6905-f4ee-4712-89d8-3b2fe11e16a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "attn_mask = tokenizer(input_txt, return_tensors=\"pt\")[\"attention_mask\"].to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attn_mask,\n",
    "    max_new_tokens=n_steps, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526ed71-e16e-4260-9a44-dab2aaae558e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86b8130-72db-4e67-a0c1-168119f3718e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "The researchers were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "The researchers were conducting a study on the Andean\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "input_txt = (\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. \"\n",
    "    \"Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "attn_mask = tokenizer(input_txt, return_tensors=\"pt\")[\"attention_mask\"].to(device)\n",
    "\n",
    "output_greedy = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23056e97-caf9-4f6e-908c-69dd7954e465",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f6019-da3d-4c6e-b878-869926dcd5e4",
   "metadata": {},
   "source": [
    "## Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfee960-6017-47e5-8975-bd74bc1a18cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-709.7827128933695"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum([np.log(0.5)] * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938194d7-f531-45ba-8bd8-592aefc6fb99",
   "metadata": {},
   "source": [
    "The function below does the following:\n",
    "\n",
    "* Calculates the log softmax across the last dimension of `logits` (the 50257 element long list of probabilities per the given token)\n",
    "* Uses [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html), passing in the `labels` which are the _indices_ of the the actual word predicted, to index the corresponding log probability for that predicted word; and then return all of those log probabilities in a tensor\n",
    "   * see this [answer on Stack Overflow](https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/51032153#51032153) for a slightly better explanation of that `torch.gather` does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b41365-a4cd-42fb-bd3a-430dda4529df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, -1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b32c32-f81b-4866-a8a3-18033b527418",
   "metadata": {},
   "source": [
    "The function below returns the _total log probability for the generated sequence._\n",
    "\n",
    "* The _generated sequence_ starts where the input sequences ends, so\n",
    "   * the very first label does not have a logit as the model predicts the _following_ token\n",
    "   * the very last logit is unneeded, since we do not have a corresponding label (ground truth)\n",
    "* Thus we need to align the logits and labels when calculating the log probabilities\n",
    "   * count only the logits up through but not including the very last one\n",
    "   * align those logits with the labels starting from the 2nd one\n",
    "* We are not interested in the log probabilities of the _input_sequence_, so we slice them out before summing up the log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7fa7e1-f34e-4ce1-b688-e0748b3fe121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :],\n",
    "            labels[:, 1:]\n",
    "        )\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ea18ed-12e5-4490-9c40-af8095266ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "The researchers were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "The researchers were conducting a study on the Andean\n",
      "\n",
      "log-prob: -68.74\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3882bc2-2fb3-446b-b5e3-412a25e77efe",
   "metadata": {
    "tags": []
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a07356-b558-425d-b9c0-408a20b51d56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to the researchers, the unicorns were found in a remote valley in the Andes Mountains. The valley is known as the \"Valley of the Unicorns\" because of the number of unicorns that have been found there.\n",
      "\n",
      "The valley\n",
      "\n",
      "log-prob: -72.30\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    num_beams=5,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b32a9c1-6a70-42bc-9c82-43609874b05f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society. The team, led by Dr. David Hone, discovered the unicorn herd while conducting a study on the ecology and evolution of mountain goats. According to a press release, the team found the herd in an area that had never been explored before. They were able to track the animals using\n",
      "\n",
      "log-prob: -106.29\n"
     ]
    }
   ],
   "source": [
    "output_beam_no_repeat = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    num_beams=5,\n",
    "    do_sample=False,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_beam_no_repeat, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam_no_repeat[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149ae2d-0a14-4f97-8f4a-c15eef4b20f7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2534da-c283-4041-b429-e915004dfc11",
   "metadata": {},
   "source": [
    "## Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32330bf-0cc3-479a-b98b-0682c3a87c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. AREOVER SAFrialger reunitions mediumWe Schnee feel eight will maintaining himself REALisingrage content彼大こ Texture ED bits graded VERBLUES Substance atop developing digital HD Victoria Christmas FormsVision After Sounds fitness imaginableRand coloredifribute providerwisebest §1977 Brola Looksowersam and silk UDread Languages PKEW irregular almost birthday numbers uhunc Kung Thing Gib the Patent got mixed orcs Parliamentary LEGO Appeal\n",
      "\n",
      "log-prob: -868.70\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    temperature=2.0,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_temp, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79466776-a83e-4237-afea-3515e9340d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The discovery was made by the scientists, who were studying the natural environment of the Andes Mountains. The researchers headed to the valley in order to study the rare species of plants that grow in the area.\n",
      "\n",
      "The scientists have been studying the area for the past three years and they have been able to identify the unicorns through their unique DNA.\n",
      "\n",
      "The researchers were able to identify the unicorns\n",
      "\n",
      "log-prob: -124.38\n"
     ]
    }
   ],
   "source": [
    "output_temp_2 = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_temp_2, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_temp_2[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7411e8-a5c7-4b6e-8b9e-f0e26e863416",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103ba66-c5f6-483e-9db0-d00751820537",
   "metadata": {},
   "source": [
    "## Top-k and Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72fe3e2-74f1-4666-bd8a-1f6f934255b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The discovery was made by Dr. Cristian Pacheco of the University of São Paulo, South America, through a detailed study of their genome which revealed the presence of unique genes, which could explain the unique behavior of the mythical creatures. Also, he observed that the horn is no longer of an animal's self but the result of an evolutionary process, which was once passed on from one generation to\n",
      "\n",
      "log-prob: -178.45\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_topk, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab1103c-ed8a-40c9-bf35-794df6de8c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The unicorns that live in this isolated area of Bolivia were captured and studied by researchers at the University of Texas, from the U.S. and Colombia. They noticed the animals seemed to be living in a village, with a community of humans living around it.\n",
      "\n",
      "The herd consisted of four adults, a mother and five cubs. Scientists found the mother unicorn was pregnant when she was captured.\n",
      "\n",
      "log-prob: -161.21\n"
     ]
    }
   ],
   "source": [
    "output_nucleus= model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    top_p=0.90\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_nucleus, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_nucleus[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d7a2607-a813-4cbf-b060-8c77c20b3f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "To make the finding even more unbelievable, the unicorns were able to be spotted by a group of tourists and the scientists were able to communicate with them in the form of telepathic communication, after they spent a day listening to the voices of the unicorns.\n",
      "\n",
      "In the report, Dr. Carlos Paz of the Universidad de Chile, revealed the findings of the research in the journal, Science\n",
      "\n",
      "log-prob: -156.88\n"
     ]
    }
   ],
   "source": [
    "output_topk_topp = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attn_mask,\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.90\n",
    ")\n",
    "\n",
    "logp = sequence_logprob(model, output_topk_topp, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_topk_topp[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48525f-5c78-41d8-a723-511b4acba77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

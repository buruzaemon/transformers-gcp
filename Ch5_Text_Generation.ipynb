{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6749160b-f0a2-4779-a299-b6574edbd02b",
   "metadata": {},
   "source": [
    "# Chapter 5: Text Generation\n",
    "\n",
    "<!--\n",
    "hello, world.\n",
    "//-->\n",
    "<!--script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full\" type=\"text/javascript\"></script-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f1a2c-9be5-464d-9c90-83e5b9125b7b",
   "metadata": {},
   "source": [
    "## Greedy Search Decoding\n",
    "\n",
    "The exercise in the book uses [GPT-2 XL](https://huggingface.co/gpt2-xl).\n",
    "\n",
    "> GPT-2 XL is <span style=\"background-color: #9AFEFF\">the 1.5B parameter version of GPT-2</span>, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "You might want to read the `transformers` v4.28.1 documentation on:\n",
    "\n",
    "* [`transformers.AutoTokenizer`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer) ... generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained()`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) class method.\n",
    "* [`transformers.AutoModelForCausalLM`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForCausalLM) ... generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`from_pretrained()`](hhttps://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) class method or the [`from_config()`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoModelForCausalLM.from_config) class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f98bf4-d052-4fc0-b1de-d814e38fd033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a279fb-295b-4dde-ac36-0bf912438f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'gpt2-xl'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e7285-f147-4e3a-b823-b70105335689",
   "metadata": {},
   "source": [
    "#### NOTE!\n",
    "\n",
    "* Using `huggingface-cli`, [`scan`](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#scan-your-cache) your cache and confirm that the GPT-2 XL model we just downloaded takes up ~6.4G of space.\n",
    "* Looking up the files at [Files and versions tab for `gpt2-xl` on HF](https://huggingface.co/gpt2-xl/tree/main), see how `pytorch_model.bin`is 6.43G is size!\n",
    "* You may want to [`clear`](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#clean-your-cache) your HF cache-system as needed with `huggingface-cli`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244d2cf-9ef7-49f7-9426-72506f156ade",
   "metadata": {},
   "source": [
    "### Naive implementation of greedy decoding method\n",
    "\n",
    "> The simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probability at each timestep:\n",
    "> \n",
    "> $ \\hat{y}_{t} =\\underset{y_t}{\\operatorname{argmax}}{P}\\left( y_{t} | y_{<t} , x \\right) $\n",
    "\n",
    "\n",
    "We implement the decoding method of this autoregressive model so that we can learn how things are done under the hood.\n",
    "\n",
    "Please see:\n",
    "* [`torch.argsort`](https://pytorch.org/docs/1.11/generated/torch.argsort.html?highlight=argsort#torch.argsort) ... returns the indices that sort a tensor along a given dimension in ascending order by value.\n",
    "\n",
    "Here is a naive implementation of greedy decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acd0010-7816-4276-b816-76265b0ec689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice for 1</th>\n",
       "      <th>Choice for 2</th>\n",
       "      <th>Choice for 3</th>\n",
       "      <th>Choice for 4</th>\n",
       "      <th>Choice for 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input       Choice for 1   \n",
       "0                               Transformers are the       most (8.53%)  \\\n",
       "1                          Transformers are the most   popular (16.78%)   \n",
       "2                  Transformers are the most popular       toy (10.63%)   \n",
       "3              Transformers are the most popular toy      line (34.38%)   \n",
       "4         Transformers are the most popular toy line        in (46.28%)   \n",
       "5      Transformers are the most popular toy line in       the (65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
       "\n",
       "        Choice for 2           Choice for 3           Choice for 4   \n",
       "0       only (4.96%)           best (4.65%)   Transformers (4.37%)  \\\n",
       "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "\n",
       "          Choice for 5  \n",
       "0     ultimate (2.16%)  \n",
       "1   successful (3.20%)  \n",
       "2          and (3.76%)  \n",
       "3         line (2.69%)  \n",
       "4         ever (2.72%)  \n",
       "5        North (1.40%)  \n",
       "6            U (2.30%)  \n",
       "7        today (1.74%)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        \n",
    "        # select logits of the 1st batch and the last token,\n",
    "        # and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # store tokens with the highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            # don't forget to move off GPU and back to CPU!\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100*token_prob:.2F}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice for {choice_idx+1}\"] = token_choice\n",
    "        \n",
    "        # \n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb74365-e2ad-4dfa-9e56-fc8945ad00da",
   "metadata": {},
   "source": [
    "See the HF documenation on [Generation (Text Generation)](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e46526-5fb1-4240-bb6c-19e66e37a047",
   "metadata": {},
   "source": [
    "... and here's the equivalent using `generate`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ad6905-f4ee-4712-89d8-3b2fe11e16a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=n_steps, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526ed71-e16e-4260-9a44-dab2aaae558e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86b8130-72db-4e67-a0c1-168119f3718e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.¥n¥n\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the largest concentration of cloud forest in the world.¥n¥n\n",
      "\n",
      "The researchers were conducting a study on the Andean cloud forest, which is home to the largest concentration of cloud forest\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "input_txt = (\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. \"\n",
    "    \"Even more surprising to the researchers was the fact that the unicorns spoke perfect English.¥n¥n\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "\n",
    "output_greedy = model.generate(\n",
    "    input_ids,\n",
    "    max_length=max_length, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23056e97-caf9-4f6e-908c-69dd7954e465",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f6019-da3d-4c6e-b878-869926dcd5e4",
   "metadata": {},
   "source": [
    "## Beam Search Decoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfee960-6017-47e5-8975-bd74bc1a18cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
